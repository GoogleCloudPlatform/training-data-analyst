{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xfcSMZEIFA_l"
   },
   "source": [
    "# ML with TensorFlow Extended (TFX) -- Part 2\n",
    "The puprpose of this tutorial is to show how to do end-to-end ML with TFX libraries on Google Cloud Platform. This tutorial covers:\n",
    "1. Data analysis and schema generation with **TF Data Validation**.\n",
    "2. Data preprocessing with **TF Transform**.\n",
    "3. Model training with **TF Estimator**.\n",
    "4. Model evaluation with **TF Model Analysis**.\n",
    "\n",
    "This notebook has been tested in Jupyter on the Deep Learning VM.\n",
    "\n",
    "## Cloud environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "print('TF version: {}'.format(tf.__version__))\n",
    "print('TFT version: {}'.format(tft.__version__))\n",
    "print('TFDV version: {}'.format(tfdv.__version__))\n",
    "print('Apache Beam version: {}'.format(beam.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'cloud-training-demos'    # Replace with your PROJECT\n",
    "BUCKET = 'cloud-training-demos-ml'  # Replace with your BUCKET\n",
    "REGION = 'us-central1'              # Choose an available region for Cloud MLE\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "\n",
    "## ensure we're using python2 env\n",
    "os.environ['CLOUDSDK_PYTHON'] = 'python2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION\n",
    "\n",
    "## ensure we predict locally with our current Python environment\n",
    "gcloud config set ml_engine/local_python `which python`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img valign=\"middle\" src=\"images/tfx.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l9u699PmHJXU"
   },
   "source": [
    "### UCI Adult Dataset: https://archive.ics.uci.edu/ml/datasets/adult\n",
    "Predict whether income exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR='gs://cloud-samples-data/ml-engine/census/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ksuSTsysHfZV",
    "outputId": "87adfbf0-be77-4d81-9162-5a2f9feffd90"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "TRAIN_DATA_FILE = os.path.join(DATA_DIR, 'adult.data.csv')\n",
    "EVAL_DATA_FILE = os.path.join(DATA_DIR, 'adult.test.csv')\n",
    "!gcloud storage ls --long $TRAIN_DATA_FILE\n",
    "!gcloud storage ls --long $EVAL_DATA_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "               'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "               'capital_gain', 'capital_loss', 'hours_per_week',\n",
    "               'native_country', 'income_bracket']\n",
    "\n",
    "TARGET_FEATURE_NAME = 'income_bracket'\n",
    "TARGET_LABELS = [' <=50K', ' >50K']\n",
    "WEIGHT_COLUMN_NAME = 'fnlwgt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kzEuipT4G0JE"
   },
   "source": [
    "## 2. Data Preprocessing\n",
    "For data preprocessing and transformation, we use [TensorFlow Transform](https://www.tensorflow.org/tfx/guide/tft) to perform the following:\n",
    "1. Implement transformation logic in **preprocess_fn**\n",
    "2. **Analyze and transform** training data.\n",
    "4. **Transform** evaluation data.\n",
    "5. Save transformed **data**, transform **schema**, and trasform **logic**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uzEWJDIbRcGQ"
   },
   "source": [
    "### 2.1 Implement preprocess_fn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I6svvdw3RhG2"
   },
   "outputs": [],
   "source": [
    "def make_preprocessing_fn(raw_schema):\n",
    "\n",
    "  def preprocessing_fn(input_features):\n",
    "\n",
    "    processed_features = {}\n",
    "\n",
    "    for feature in raw_schema.feature:\n",
    "      feature_name = feature.name\n",
    "      \n",
    "      if feature_name in ['income_bracket']:\n",
    "        processed_features[feature_name] = input_features[feature_name]\n",
    "      elif feature_name in ['fnlwgt']:\n",
    "        # Scale weights to be less than 1.0\n",
    "        processed_features[feature_name + \"_scaled\"] = (\n",
    "            tf.cast(input_features[feature_name], tf.float32) / \n",
    "            tf.cast(tft.max(input_features[feature_name]), tf.float32))\n",
    "      elif feature.type == 1:\n",
    "        # Extract vocabulary and integerize categorical features.\n",
    "        processed_features[feature_name+\"_integerized\"] = (\n",
    "            tft.compute_and_apply_vocabulary(input_features[feature_name], vocab_filename=feature_name))\n",
    "      else:\n",
    "        # normalize numeric features.\n",
    "        processed_features[feature_name+\"_scaled\"] = tft.scale_to_z_score(input_features[feature_name])\n",
    "\n",
    "    # Bucketize age using quantiles. \n",
    "    quantiles = tft.quantiles(input_features[\"age\"], num_buckets=5, epsilon=0.01)\n",
    "    processed_features[\"age_bucketized\"] = tft.apply_buckets(\n",
    "      input_features[\"age\"], bucket_boundaries=quantiles)\n",
    "\n",
    "    return processed_features\n",
    "\n",
    "  return preprocessing_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4rMiQew8Z8V7"
   },
   "source": [
    "### 2.2 Implement the Beam pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YSjlcdLNZ7FX"
   },
   "outputs": [],
   "source": [
    "def run_pipeline(args):\n",
    "  import tensorflow_transform as tft\n",
    "  import tensorflow_transform.beam as tft_beam\n",
    "  import tensorflow_data_validation as tfdv\n",
    "  from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "  from tensorflow_transform.tf_metadata import dataset_schema\n",
    "  from tensorflow_transform.tf_metadata import schema_utils\n",
    "    \n",
    "  pipeline_options = beam.pipeline.PipelineOptions(flags=[], **args)\n",
    "    \n",
    "  raw_schema_location = args['raw_schema_location']\n",
    "  raw_train_data_location = args['raw_train_data_location']\n",
    "  raw_eval_data_location = args['raw_eval_data_location']\n",
    "  transformed_train_data_location = args['transformed_train_data_location']\n",
    "  transformed_eval_data_location = args['transformed_eval_data_location']\n",
    "  transform_artifact_location = args['transform_artifact_location']\n",
    "  temporary_dir = args['temporary_dir']\n",
    "  runner = args['runner']\n",
    "    \n",
    "  print (\"Raw schema location: {}\".format(raw_schema_location))\n",
    "  print (\"Raw train data location: {}\".format(raw_train_data_location))\n",
    "  print (\"Raw evaluation data location: {}\".format(raw_eval_data_location))\n",
    "  print (\"Transformed train data location: {}\".format(transformed_train_data_location))\n",
    "  print (\"Transformed evaluation data location: {}\".format(transformed_eval_data_location))\n",
    "  print (\"Transform artifact location: {}\".format(transform_artifact_location))\n",
    "  print (\"Temporary directory: {}\".format(temporary_dir))\n",
    "  print (\"Runner: {}\".format(runner))\n",
    "  print (\"\")\n",
    "\n",
    "  # Load TFDV schema and create tft schema from it.\n",
    "  source_raw_schema = tfdv.load_schema_text(raw_schema_location)\n",
    "  raw_feature_spec = schema_utils.schema_as_feature_spec(source_raw_schema).feature_spec\n",
    "  raw_metadata = dataset_metadata.DatasetMetadata(\n",
    "    dataset_schema.from_feature_spec(raw_feature_spec))\n",
    "\n",
    "  with beam.Pipeline(runner, options=pipeline_options) as pipeline:\n",
    "    with tft_beam.Context(temporary_dir):\n",
    "      \n",
    "      converter = tft.coders.CsvCoder(column_names=HEADER, \n",
    "        schema=raw_metadata.schema)\n",
    "\n",
    "      ###### analyze & transform trainining data ###############################\n",
    "\n",
    "      # Read raw training csv data.\n",
    "      step = 'Train'\n",
    "      raw_train_data = (\n",
    "        pipeline\n",
    "          | '{} - Read Raw Data'.format(step) >> beam.io.textio.ReadFromText(raw_train_data_location)\n",
    "          | '{} - Remove Empty Rows'.format(step) >> beam.Filter(lambda line: line)\n",
    "          | '{} - Decode CSV Data'.format(step) >> beam.Map(converter.decode)\n",
    "        )\n",
    "      \n",
    "      # Create a train dataset from the data and schema.\n",
    "      raw_train_dataset = (raw_train_data, raw_metadata)\n",
    "\n",
    "      # Analyze and transform raw_train_dataset to produced transformed_train_dataset and transform_fn.\n",
    "      transformed_train_dataset, transform_fn = (\n",
    "        raw_train_dataset \n",
    "        | '{} - Analyze & Transform'.format(step) >> tft_beam.AnalyzeAndTransformDataset(\n",
    "              make_preprocessing_fn(source_raw_schema))\n",
    "      )\n",
    "  \n",
    "      # Get data and schema separately from the transformed_train_dataset.\n",
    "      transformed_train_data, transformed_metadata = transformed_train_dataset\n",
    "\n",
    "      # write transformed train data to sink.\n",
    "      print (\"Writing transformed training data...\")\n",
    "      _ = (\n",
    "        transformed_train_data \n",
    "          | '{} - Write Transformed Data'.format(step) >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "            file_path_prefix=transformed_train_data_location,\n",
    "            file_name_suffix=\".tfrecords\",\n",
    "            coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema))\n",
    "        )\n",
    "\n",
    "      ###### transform evaluation data #########################################\n",
    "\n",
    "      # Read raw training csv data.\n",
    "      step = 'Eval'\n",
    "      raw_eval_data = (\n",
    "        pipeline\n",
    "          | '{} - Read Raw Data'.format(step) >> beam.io.textio.ReadFromText(raw_eval_data_location)\n",
    "          | '{} - Remove Empty Rows'.format(step) >> beam.Filter(lambda line: line)\n",
    "          | '{} - Decode CSV Data'.format(step) >> beam.Map(converter.decode)\n",
    "        )\n",
    "      \n",
    "      # Create a eval dataset from the data and schema.\n",
    "      raw_eval_dataset = (raw_eval_data, raw_metadata)\n",
    "\n",
    "      # Transform eval data based on produced transform_fn.\n",
    "      transformed_eval_dataset = (\n",
    "        (raw_eval_dataset, transform_fn) \n",
    "          | '{} - Transform'.format(step) >> tft_beam.TransformDataset()\n",
    "      )\n",
    "\n",
    "      # Get data from the transformed_eval_dataset.\n",
    "      transformed_eval_data, _ = transformed_eval_dataset\n",
    "\n",
    "      # Write transformed eval data to sink.\n",
    "      _ = (\n",
    "          transformed_eval_data \n",
    "          | '{} - Write Transformed Data'.format(step) >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "              file_path_prefix=transformed_eval_data_location,\n",
    "              file_name_suffix=\".tfrecords\",\n",
    "              coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema))\n",
    "        )\n",
    "\n",
    "      ###### write transformation metadata #######################################################\n",
    "\n",
    "      # Write transform_fn.\n",
    "      print (\"Writing transform artifacts...\")\n",
    "      _ = (\n",
    "          transform_fn \n",
    "          | 'Write Transform Artifacts' >> tft_beam.WriteTransformFn(\n",
    "              transform_artifact_location)\n",
    "      )\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2QQzYPXGhqwP"
   },
   "source": [
    "### 1.4 Run data tranformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip freeze | grep tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile setup.py\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(name='tfxdemo',\n",
    "      version='1.0',\n",
    "      packages=find_packages(),\n",
    "      install_requires=['tensorflow-transform==0.13.0', \n",
    "                        'tensorflow-data-validation==0.13.1'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Snd1a6_yhq9H"
   },
   "outputs": [],
   "source": [
    "#runner = 'DirectRunner'; OUTPUT_DIR = 'output'   # on-prem\n",
    "#runner = 'DirectRunner'; OUTPUT_DIR = 'gs://{}/census/tfx'.format(BUCKET)  # hybrid\n",
    "runner = 'DataflowRunner'; OUTPUT_DIR = 'gs://{}/census/tfx'.format(BUCKET)  # on GCP\n",
    "job_name = 'tft-census' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "\n",
    "RAW_SCHEMA_LOCATION = 'raw_schema.pbtxt'\n",
    "TRANSFORM_ARTIFACTS_DIR = os.path.join(OUTPUT_DIR,'transform')\n",
    "TRANSFORMED_DATA_DIR = os.path.join(OUTPUT_DIR,'transformed')\n",
    "TEMP_DIR = os.path.join(OUTPUT_DIR, 'tmp')\n",
    "\n",
    "args = {\n",
    "    \n",
    "    'runner': runner,\n",
    "    'job_name': job_name,\n",
    "\n",
    "    'raw_schema_location': RAW_SCHEMA_LOCATION,\n",
    "\n",
    "    'raw_train_data_location': TRAIN_DATA_FILE,\n",
    "    'raw_eval_data_location': EVAL_DATA_FILE,\n",
    "\n",
    "    'transformed_train_data_location':  os.path.join(TRANSFORMED_DATA_DIR, \"train\"),\n",
    "    'transformed_eval_data_location':  os.path.join(TRANSFORMED_DATA_DIR, \"eval\"),\n",
    "    'transform_artifact_location':  TRANSFORM_ARTIFACTS_DIR,\n",
    "    \n",
    "    'temporary_dir': TEMP_DIR,\n",
    "    'project': PROJECT,\n",
    "    'temp_location': TEMP_DIR,\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'staging'),\n",
    "    'max_num_workers': 8,\n",
    "    'save_main_session': False,\n",
    "    'setup_file': './setup.py'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "qNyztdP3jjaz",
    "outputId": "2c75be8e-cfe5-4834-d0ae-2f07bfecf373"
   },
   "outputs": [],
   "source": [
    "if tf.gfile.Exists(OUTPUT_DIR):\n",
    "  print(\"Removing {} contents...\".format(OUTPUT_DIR))\n",
    "  tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "print(\"Running TF Transform pipeline...\")\n",
    "print(\"Runner: {}\".format(runner))\n",
    "if runner == \"DataflowRunner\":\n",
    "    print(\"Launching Dataflow job {} ...\".format(job_name))\n",
    "    print(\"Waitting until the Dataflow job finishes...\")\n",
    "print()\n",
    "run_pipeline(args)\n",
    "print()\n",
    "print(\"Pipeline is done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PfQO1IjmsZQb"
   },
   "source": [
    "### Check the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "SmBSS38GsZbx",
    "outputId": "8876bb26-57ab-40dd-aac9-35210344e09b"
   },
   "outputs": [],
   "source": [
    "!gcloud storage ls $OUTPUT_DIR/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "sCexlD9EtV2I",
    "outputId": "5713ef76-ccc6-4578-9b8f-72df71ff62a8"
   },
   "outputs": [],
   "source": [
    "!gcloud storage ls $OUTPUT_DIR/transform/transform_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lYsjUQt2tbEV"
   },
   "outputs": [],
   "source": [
    "!gcloud storage cat $OUTPUT_DIR/transform/transformed_metadata/schema.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gsi_Hsh89Cl7"
   },
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0fOWx1yI9Dyn"
   },
   "source": [
    "Copyright 2019 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0.\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "---\n",
    "This is not an official Google product. The sample code provided for educational purposes only.\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "02-tfx_end_to_end",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
