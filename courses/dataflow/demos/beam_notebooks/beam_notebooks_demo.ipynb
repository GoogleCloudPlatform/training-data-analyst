{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87420f48",
   "metadata": {},
   "source": [
    "# Beam Notebooks and Dataframes Demo\n",
    "\n",
    "This example demonstrates how to set up an Apache Beam pipeline that reads from a\n",
    "[Google Cloud Storage](https://cloud.google.com/storage) file containing text from Shakespeare's work *King Lear*, \n",
    "tokenizes the text lines into individual words, and performs a frequency count on each of those words. \n",
    "\n",
    "We will perform the aggregation operations using the Beam Dataframes API, which allows us to use Pandas-like syntax to write your transformations. We will see how we can easily translate from using Pandas locally to using Dataframes in Apache Beam (which could then be run on Dataflow\n",
    "\n",
    "For details about the Apache Beam Dataframe API, see the [Documentation](https://beam.apache.org/documentation/dsls/dataframes/overview/).\n",
    "\n",
    "We first start with the necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9f352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python's regular expression library\n",
    "import re\n",
    "\n",
    "# Beam and interactive Beam imports\n",
    "import apache_beam as beam\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "\n",
    "# Dataframe API imports\n",
    "from apache_beam.dataframe.convert import to_dataframe\n",
    "from apache_beam.dataframe.convert import to_pcollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb53e3a",
   "metadata": {},
   "source": [
    "We will be using the `re` library to parse our lines of text. We will import the `InteractiveRunner` class for executing out pipeline in the notebook environment and the `interactive_beam` module for exploring the PCollections. Finally we will import two functions from the Dataframe API, `to_dataframe` and `to_pcollection`. `to_dataframe` converts your (schema-aware) PCollection into a dataframe and `to_pcollection` goes back in the other direction to a `PCollection` of type `beam.Row`.\n",
    "\n",
    "We will first create a composite PTransform `ReadWordsFromText` to read in a file pattern (`file_pattern`), use the `ReadFromText` source to read in the files, and then `FlatMap` with a lambda to parse the line into individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cc4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadWordsFromText(beam.PTransform):\n",
    "    \n",
    "    def __init__(self, file_pattern):\n",
    "        self._file_pattern = file_pattern\n",
    "    \n",
    "    def expand(self, pcoll):\n",
    "        return (pcoll.pipeline\n",
    "                | beam.io.ReadFromText(self._file_pattern)\n",
    "                | beam.FlatMap(lambda line: re.findall(r'[\\w\\']+', line.strip(), re.UNICODE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7de319c",
   "metadata": {},
   "source": [
    "To be able to process our data in the notebook environment and explore the PCollections, we will use the interactive runner. We create this pipeline object in the same manner as usually, but passing in `InteractiveRunner()` as the runner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa57ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(InteractiveRunner())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1fdcdb",
   "metadata": {},
   "source": [
    "Now we're ready to start processing our data! We first apply our `ReadWordsFromText` transform to read in the lines of text from Google Cloud Storage and parse into individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d67e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = p | 'ReadWordsFromText' >> ReadWordsFromText('gs://apache-beam-samples/shakespeare/kinglear.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62cb595",
   "metadata": {},
   "source": [
    "Now we will see some capabilities of the interactive runner. First we can use `ib.show` to view the contents of a specific `PCollection` from any point of our pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9286d157",
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be2093a",
   "metadata": {},
   "source": [
    "Great! We see that we have 28,001 words in our PCollection and we can view the words in our PCollection. \n",
    "\n",
    "We can also view the current DAG for our graph by using the `ib.show_graph()` method. Note that here we pass in the pipeline object rather than a PCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c18ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show_graph(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3c0802",
   "metadata": {},
   "source": [
    "In the above graph, the rectanglar boxes correspond to PTransforms and the circles correspond to PCollections. \n",
    "\n",
    "Next we will add a simple schema to our PCollection and convert the PCollection into a dataframe using the `to_dataframe` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bf1d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_rows = words | 'ToRows' >> beam.Map(lambda word: beam.Row(word=word))\n",
    "\n",
    "df = to_dataframe(word_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3694d5d",
   "metadata": {},
   "source": [
    "We can now explore our PCollection as a Pandas-like dataframe! One of the first things many data scientists do as soon as they load data into a dataframe is explore the first few rows of data using the `head` method. Let's see what happens here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d963e261",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ba08e0",
   "metadata": {},
   "source": [
    "Notice that we got a very specific type of error! The `WontImplementError` is for Pandas methods that will not be implemented for Beam dataframes. These are methods that violate the Beam model for one reason or another. For example, in this case the `head` method depends on the order of the dataframe. However, this is in conflict with the Beam model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a04cc1b",
   "metadata": {},
   "source": [
    "Our goal however is to count the number of times each word appears in the ingested text. First we will add a new column in our dataframe named `count` with a value of `1` for all rows. After that, we will group by the value of the `word` column and apply the `sum` method for the `count` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c8822",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count'] = 1\n",
    "counted = df.groupby('word').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea4ccdf",
   "metadata": {},
   "source": [
    "That's it! It looks exactly like the code one would write when using Pandas. However, what does this look like in the DAG for the pipeline? We can see this by executing `ib.show_graph(p)` as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79fbd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show_graph(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf3c807",
   "metadata": {},
   "source": [
    "We can see that the dataframe manipulations added a new PTransform to our pipeline. Let us convert the dataframe back to a PCollection so we can use `ib.show` to view the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee7367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = to_pcollection(counted, include_indexes=True)\n",
    "ib.show(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd928f",
   "metadata": {},
   "source": [
    "Great! We can now see that the words have been successfully counted. Finally let us build in a sink into the pipeline. We can do this in two ways. If we wish to write to a CSV file, then we can use the dataframe's `to_csv` method. We can also use the `WriteToText` transform after converting back to a PCollection. Let's do both and explore the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f5f821",
   "metadata": {},
   "outputs": [],
   "source": [
    "counted.to_csv('from_df.csv')\n",
    "_ = word_counts | beam.io.WriteToText('from_pcoll.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e41e3ba",
   "metadata": {},
   "source": [
    "Before saving the outputs to the sinks, let's take a peek at our finished pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cb1509",
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show_graph(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22b0b07",
   "metadata": {},
   "source": [
    "Note that we can see the branching with two different sinks, also we can see where the dataframe is converted back to a PCollection. We can run our entire pipeline by using `p.run()` as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e365786",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601272b9",
   "metadata": {},
   "source": [
    "Let us now look at the beginning of the CSV files using the bash line magic with the `head` command to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0407fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head from_df*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ae2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head from_pcoll*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cd1e97",
   "metadata": {},
   "source": [
    "We (functionally) end up with the same information as expected! The big difference is in how the results are presented. In the case of the output from the `WriteToText` connector, we did not convert our PCollection from objects of type `Row`. We could write a simple intermediate transform to pull out the properties of the `Row` object into a comma-seperated representation. For example:\n",
    "\n",
    "```\n",
    "def row_to_csv(element):\n",
    "    output = f\"{element.word},{element.count}\"\n",
    "    return output\n",
    "```\n",
    "\n",
    "The we could replace the code `_ = word_counts | beam.io.WriteToText('from_pcoll.csv')` with\n",
    "\n",
    "```\n",
    "_ = word_counts | beam.Map(row_to_csv)\n",
    "                | beam.io.WriteToText('from_pcoll.csv')\n",
    "```\n",
    "\n",
    "However, note that the `to_csv` method for the dataframe took care of this conversion for us."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Beam 2.29.0 for Python 3",
   "language": "python",
   "name": "apache-beam-2.29.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
