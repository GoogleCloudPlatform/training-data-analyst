{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNdWfPXCjTjY"
   },
   "source": [
    "# LAB 03:  Basic Feature Engineering in Keras \n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "\n",
    "1. Create an input pipeline using tf.data\n",
    "2. Engineer features to create categorical, crossed, and numerical feature columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Introduction \n",
    "In this lab, we utilize feature engineering to improve the prediction of housing prices using a Keras Sequential Model.  \n",
    "\n",
    "Each learning objective will correspond to a __#TODO__ in the notebook where you will complete the notebook cell's code before running. Refer to the [solution](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive2/feature_engineering/solutions/3_keras_basic_feat_eng.ipynb) for reference. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9dEreb4QKizj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow.keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import feature_column as fc\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import plot_model\n",
    "\n",
    "print(\"TensorFlow version: \",tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the Google Machine Learning Courses Programming Exercises use the  [California Housing Dataset](https://developers.google.com/machine-learning/crash-course/california-housing-data-description\n",
    "), which contains data drawn from the 1990 U.S. Census.  Our lab dataset has been pre-processed so that there are no missing values.\n",
    "\n",
    "First, let's download the raw .csv data by copying the data from a cloud storage bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"../data\"):\n",
    "    os.makedirs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp gs://cloud-training-demos/feat_eng/housing/housing_pre-proc.csv ../data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l ../data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lM6-n6xntv3t"
   },
   "source": [
    "Now, let's read in the dataset just copied from the cloud storage bucket and create a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "colab_type": "code",
    "id": "REZ57BXCLdfG",
    "outputId": "a6ef2eda-c7eb-4e2d-92e4-e7fcaa20b0af"
   },
   "outputs": [],
   "source": [
    "housing_df = pd.read_csv('../data/housing_pre-proc.csv', error_bad_lines=False)\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use .describe() to see some summary statistics for the numeric fields in our dataframe. Note, for example, the count row and corresponding columns. The count shows 20433.000000 for all feature columns. Thus, there are no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0zhLtQqMPem"
   },
   "source": [
    "####  Split the dataset for ML\n",
    "\n",
    "The dataset we loaded was a single CSV file. We will split this into train, validation, and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "YEOpw7LhMYsI",
    "outputId": "6161a660-7133-465a-d754-d7acae2b68c8"
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(housing_df, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dz9kfjOMBX9U"
   },
   "source": [
    "Now, we need to output the split files.  We will specifically need the test.csv later for testing.  You should see the files appear in the home directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "colab_type": "code",
    "id": "ADX23QUu_Wiu",
    "outputId": "e97fa59e-4ed4-48a3-8fba-c95f293944ee"
   },
   "outputs": [],
   "source": [
    "train.to_csv('../data/housing-train.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.to_csv('../data/housing-val.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "colab_type": "code",
    "id": "CU1FgmKEAmWh",
    "outputId": "2cce91e1-2c4a-4fe8-a6c3-3da52cb9458f"
   },
   "outputs": [],
   "source": [
    "test.to_csv('../data/housing-test.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../data/housing*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aj35eYy_lutI"
   },
   "source": [
    "## Lab Task 1: Create an input pipeline using tf.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84ef46LXMfvu"
   },
   "source": [
    "Next, we will wrap the dataframes with [tf.data](https://www.tensorflow.org/guide/datasets). This will enable us  to use feature columns as a bridge to map from the columns in the Pandas dataframe to features used to train the model. \n",
    "\n",
    "Here, we create an input pipeline using tf.data.  This function is missing two lines.  Correct and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    dataframe = dataframe.copy()\n",
    "    \n",
    "   # TODO 1a -- Your code here\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we initialize the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_ds = df_to_dataset(train)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRLGSMDzM-dl"
   },
   "source": [
    "Now that we have created the input pipeline, let's call it to see the format of the data it returns. We have used a small batch size to keep the output readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "colab_type": "code",
    "id": "CSBo3dUVNFc9",
    "outputId": "d1be2646-b1e5-4110-dbba-5bc49d9b30f6"
   },
   "outputs": [],
   "source": [
    "# TODO 1b -- Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OT5N6Se-NQsC"
   },
   "source": [
    "We can see that the dataset returns a dictionary of column names (from the dataframe) that map to column values from rows in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YEGEAqaziwfC"
   },
   "source": [
    "#### Numeric columns\n",
    "The output of a feature column becomes the input to the model. A numeric is the simplest type of column. It is used to represent real valued features. When using this column, your model will receive the column value from the dataframe unchanged.\n",
    "\n",
    "In the California housing prices dataset, most columns from the dataframe are numeric.  Let' create a variable called **numeric_cols** to hold only the numerical feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1c -- Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EwMEcH_52JT8"
   },
   "source": [
    "#### Scaler function\n",
    "It is very important for numerical variables to get scaled before they are \"fed\" into the neural network. Here we use min-max scaling. Here we are creating a function named 'get_scal' which takes a list of numerical features and returns a 'minmax' function, which will be used in tf.feature_column.numeric_column() as normalizer_fn in parameters. 'Minmax' function itself takes a 'numerical' number from a particular feature and return scaled value of that number. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ig1k5ovWBnN8"
   },
   "source": [
    "Next, we scale the numerical feature columns that we assigned to the variable \"numeric cols\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar def get_scal(feature):\n",
    "# TODO 1d -- Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y8IUfcuVaS_g"
   },
   "outputs": [],
   "source": [
    "# TODO 1e -- Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8v9XoD7WCKRM"
   },
   "source": [
    "Next, we should validate the total number of feature columns.  Compare this number to the number of numeric features you input earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4jgPFThi50sS",
    "outputId": "23ede6f5-a62a-4767-b3a6-fe8a3b89a212"
   },
   "outputs": [],
   "source": [
    "print('Total number of feature coLumns: ', len(feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Ug3hB8Sl0jO"
   },
   "source": [
    "### Using the Keras Sequential Model\n",
    "\n",
    "Next, we will run this cell to compile and fit the Keras Sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_YJPPb3xTPeZ",
    "outputId": "2d445722-1d43-4a27-a6c0-c6ce813ab450"
   },
   "outputs": [],
   "source": [
    "# Model create\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns, dtype='float64')\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  feature_layer,\n",
    "  layers.Dense(12, input_dim=8, activation='relu'),\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(1, activation='linear',  name='median_house_value')\n",
    "])\n",
    "\n",
    "# Model compile\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mse'])\n",
    "\n",
    "# Model Fit\n",
    "history = model.fit(train_ds,\n",
    "                    validation_data=val_ds,\n",
    "                    epochs=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we show loss as  Mean Square Error (MSE).  Remember that MSE is the most commonly used regression loss function. MSE is the sum of squared distances between our target variable (e.g. housing median age) and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "vo7hhkPqm6Jx",
    "outputId": "938907f6-b6c8-497c-a8f6-0f1cdbf336c9"
   },
   "outputs": [],
   "source": [
    "loss, mse = model.evaluate(train_ds)\n",
    "print(\"Mean Squared Error\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "252EPxGp7-FJ"
   },
   "source": [
    "#### Visualize the model loss curve\n",
    "\n",
    "Next, we will use matplotlib to draw the model's loss curves for training and validation.  A line plot is also created showing the mean squared error loss over the training epochs for both the train (blue) and test (orange) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(history, metrics):\n",
    "    nrows = 1\n",
    "    ncols = 2\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    for idx, key in enumerate(metrics):  \n",
    "        ax = fig.add_subplot(nrows, ncols, idx+1)\n",
    "        plt.plot(history.history[key])\n",
    "        plt.plot(history.history['val_{}'.format(key)])\n",
    "        plt.title('model {}'.format(key))\n",
    "        plt.ylabel(key)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc='upper left');  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(history, ['loss', 'mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wqkozY268xi7"
   },
   "source": [
    "### Load test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uf4TyVJ_Dzxe"
   },
   "source": [
    "Next, we read in the test.csv file and validate that there are no null values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can use .describe() to see some summary statistics for the numeric fields in our dataframe.  The count shows 4087.000000 for all feature columns. Thus, there are no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "colab_type": "code",
    "id": "b4C4BmhV8ch9",
    "outputId": "82bcc9d3-4432-4068-ab82-6a6abbe4a024"
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/housing-test.csv')\n",
    "test_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nY2Yrt8fC7RW"
   },
   "source": [
    "Now that we have created an input pipeline using tf.data and compiled a Keras Sequential Model, we now create the input function for the test data and to initialize the test_predict variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8rMdDeGDCwpT"
   },
   "outputs": [],
   "source": [
    "# TODO 1f -- Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = test_input_fn(dict(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H5SkINtbDIdr"
   },
   "source": [
    "#### Prediction:  Linear Regression\n",
    "\n",
    "Before we begin to feature engineer our feature columns, we should predict the median house value.  By predicting the median house value now, we can then compare it with the median house value after feature engineeing.\n",
    "\n",
    "To predict with Keras, you simply call [model.predict()](https://keras.io/models/model/#predict) and pass in the housing features you want to predict the median_house_value for. Note:  We are predicting the model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uNc6TSoJDL7-"
   },
   "outputs": [],
   "source": [
    "predicted_median_house_value = model.predict(test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HFXK1SKPDYgD"
   },
   "source": [
    "Next, we run two predictions in separate cells - one where ocean_proximity=INLAND and one where ocean_proximity= NEAR OCEAN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xepss0vhoHge",
    "outputId": "46842a26-eacd-4801-857b-18c6a8f2005c"
   },
   "outputs": [],
   "source": [
    "# Ocean_proximity is INLAND\n",
    "model.predict({\n",
    "    'longitude': tf.convert_to_tensor([-121.86]),\n",
    "    'latitude': tf.convert_to_tensor([39.78]),\n",
    "    'housing_median_age': tf.convert_to_tensor([12.0]),\n",
    "    'total_rooms': tf.convert_to_tensor([7653.0]),\n",
    "    'total_bedrooms': tf.convert_to_tensor([1578.0]),\n",
    "    'population': tf.convert_to_tensor([3628.0]),\n",
    "    'households': tf.convert_to_tensor([1494.0]),\n",
    "    'median_income': tf.convert_to_tensor([3.0905]),\n",
    "    'ocean_proximity': tf.convert_to_tensor(['INLAND'])\n",
    "}, steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qPssm8p4EZHh",
    "outputId": "2a55d427-7857-401c-f60d-edbb36be19ec"
   },
   "outputs": [],
   "source": [
    "# Ocean_proximity is NEAR OCEAN\n",
    "model.predict({\n",
    "    'longitude': tf.convert_to_tensor([-122.43]),\n",
    "    'latitude': tf.convert_to_tensor([37.63]),\n",
    "    'housing_median_age': tf.convert_to_tensor([34.0]),\n",
    "    'total_rooms': tf.convert_to_tensor([4135.0]),\n",
    "    'total_bedrooms': tf.convert_to_tensor([687.0]),\n",
    "    'population': tf.convert_to_tensor([2154.0]),\n",
    "    'households': tf.convert_to_tensor([742.0]),\n",
    "    'median_income': tf.convert_to_tensor([4.9732]),\n",
    "    'ocean_proximity': tf.convert_to_tensor(['NEAR OCEAN'])\n",
    "}, steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Txl-MRuLFE_8"
   },
   "source": [
    "The arrays returns a predicted value.  What do these numbers mean?  Let's compare this value to the test set.  \n",
    "\n",
    "Go to the test.csv you read in a few cells up.  Locate the first line and find the median_house_value - which should be 249,000 dollars near the ocean. What value did your model predicted for the median_house_value? Was it a solid model performance? Let's see if we can improve this a bit with feature engineering!  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Task 2: Engineer features to create categorical and numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "78F1XH1Qwvbt"
   },
   "source": [
    "Now we create a cell that indicates which features will be used in the model.  \n",
    "Note:  Be sure to bucketize 'housing_median_age' and ensure that 'ocean_proximity' is one-hot encoded.  And, don't forget your numeric values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZxSatLUxUmvI"
   },
   "outputs": [],
   "source": [
    "# TODO 2a -- Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HbypkYHxxwt"
   },
   "source": [
    "Next, we scale the numerical, bucktized, and categorical feature columns that we assigned to the variables in the precding cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ExX5Akz0UnE-"
   },
   "outputs": [],
   "source": [
    "# Scalar def get_scal(feature):\n",
    "def get_scal(feature):\n",
    "    def minmax(x):\n",
    "        mini = train[feature].min()\n",
    "        maxi = train[feature].max()\n",
    "        return (x - mini)/(maxi-mini)\n",
    "        return(minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wzqcddUQUnKn"
   },
   "outputs": [],
   "source": [
    "# All numerical features - scaling\n",
    "feature_columns = []\n",
    "for header in numeric_cols:\n",
    "    scal_input_fn = get_scal(header)\n",
    "    feature_columns.append(fc.numeric_column(header,\n",
    "                                             normalizer_fn=scal_input_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYUpUZvgwrPe"
   },
   "source": [
    "### Categorical Feature\n",
    "In this dataset, 'ocean_proximity' is represented as a string.  We cannot feed strings directly to a model. Instead, we must first map them to numeric values. The categorical vocabulary columns provide a way to represent strings as a one-hot vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sZnlnFZkyEbe"
   },
   "source": [
    "Next, we create a categorical feature using 'ocean_proximity'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Cf6SoFTUnc6"
   },
   "outputs": [],
   "source": [
    "# TODO 2b -- Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qnGyWaijzShj"
   },
   "source": [
    "### Bucketized Feature\n",
    "\n",
    "Often, you don't want to feed a number directly into the model, but instead split its value into different categories based on numerical ranges. Consider our raw data that represents a homes' age. Instead of representing the house age as a numeric column, we could split the home age into several buckets using a [bucketized column](https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column). Notice the one-hot values below describe which age range each row matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ZRlFyP7fOw-"
   },
   "source": [
    "Next we create a bucketized column using 'housing_median_age'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xB-yiVLmUnXp"
   },
   "outputs": [],
   "source": [
    "# TODO 2c -- Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ri4_wssOg943"
   },
   "source": [
    "### Feature Cross\n",
    "\n",
    "Combining features into a single feature, better known as [feature crosses](https://developers.google.com/machine-learning/glossary/#feature_cross), enables a model to learn separate weights for each combination of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6HHJl3J0j0T"
   },
   "source": [
    "Next, we create a feature cross of 'housing_median_age' and 'ocean_proximity'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JVLnG0WbUnkl"
   },
   "outputs": [],
   "source": [
    "# TODO 2d -- Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hiz6HCWg1CXO"
   },
   "source": [
    "Next, we should validate the total number of feature columns.  Compare this number to the number of numeric features you input earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6P3Ewc3_Unsv",
    "outputId": "42c1c4a6-89f8-4685-b2d0-e76a90cdf9ee"
   },
   "outputs": [],
   "source": [
    "print('Total number of feature coumns: ', len(feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lNr00mP41sJp"
   },
   "source": [
    "Next, we will run this cell to compile and fit the Keras Sequential model.  This is the same model we ran earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4Dwal3oxUoCe",
    "outputId": "1ae08747-7dbe-47a5-b3e7-87581e460b1b"
   },
   "outputs": [],
   "source": [
    "# Model create\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns,\n",
    "                                              dtype='float64')\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  feature_layer,\n",
    "  layers.Dense(12, input_dim=8, activation='relu'),\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(1, activation='linear',  name='median_house_value')\n",
    "])\n",
    "\n",
    "# Model compile\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mse'])\n",
    "\n",
    "# Model Fit\n",
    "history = model.fit(train_ds,\n",
    "                    validation_data=val_ds,\n",
    "                    epochs=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3LdUQszM16Oj"
   },
   "source": [
    "Next, we show loss and mean squared error then plot the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "ZtFSpkd9UoAW",
    "outputId": "bac4836e-c4f1-4b29-876d-91fe1b51a5a7"
   },
   "outputs": [],
   "source": [
    "loss, mse = model.evaluate(train_ds)\n",
    "print(\"Mean Squared Error\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "O8kWMa6xUn-M",
    "outputId": "05ed9323-1102-4245-a40b-88543f11b0f3"
   },
   "outputs": [],
   "source": [
    "plot_curves(history, ['loss', 'mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C4tWwOQt2e-P"
   },
   "source": [
    "Next we create a prediction model.  Note:  You may use the same values from the previous prediciton.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2e -- Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rcbdA3arXkej"
   },
   "source": [
    "### Analysis \n",
    "\n",
    "The array returns a predicted value.  Compare this value to the test set you ran earlier. Your predicted value may be a bit better.\n",
    "\n",
    "Now that you have your \"feature engineering template\" setup, you can experiment by creating additional features.  For example, you can create derived features, such as households per population, and see how they impact the model.  You can also experiment with replacing the features you used to create the feature cross.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Google Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Basic Feature Engineering in Keras.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-1.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
