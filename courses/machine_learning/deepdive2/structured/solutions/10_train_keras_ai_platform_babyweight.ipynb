{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 10:  Training Keras model on Cloud AI Platform.\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "1. Setup up the environment\n",
    "1. Create trainer module's task.py to hold hyperparameter argparsing code\n",
    "1. Create trainer module's model.py to hold Keras model code\n",
    "1. Run trainer module package locally\n",
    "1. Submit training job to Cloud AI Platform\n",
    "1. Submit hyperparameter tuning job to Cloud AI Platform\n",
    "\n",
    "\n",
    "## Introduction\n",
    "After having testing our training pipeline both locally and in the cloud on a susbset of the data, we can submit another (much larger) training job to the cloud. It is also a good idea to run a hyperparameter tuning job to make sure we have optimized the hyperparameters of our model. \n",
    "\n",
    "In this notebook we'll be training our Keras model at scale using Cloud AI Platform.\n",
    "\n",
    "In this lab we will set up the environment, create the trainer module's task.py to hold hyperparameter argparsing code, create the trainer module's model.py to hold Keras model code, run the trainer module package locally, submit a training job to Cloud AI Platform, and submit a hyperparameter tuning job to Cloud AI Platform.\n",
    "\n",
    "Each learning objective will correspond to a __#TODO__ in this student lab notebook -- try to complete this notebook first and then review the [solution notebook](../solutions/10_train_keras_ai_platform_babyweight.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJ7ByvoXzpVI"
   },
   "source": [
    "## Set up environment variables and load necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment variables.\n",
    "\n",
    "Set environment variables so that we can use them throughout the entire lab. We will be using our project name for our bucket, so you only need to change your project and region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PROJECT=$(gcloud config list project --format \"value(core.project)\")\n",
    "echo \"Your current GCP Project Name is: \"$PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these to try this notebook out\n",
    "PROJECT = \"cloud-training-demos\"  # Replace with your PROJECT\n",
    "BUCKET = PROJECT  # defaults to PROJECT\n",
    "REGION = \"us-central1\"  # Replace with your REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = \"2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}; then\n",
    "    gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/babyweight/data/*000000000000.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the [Keras wide-and-deep code](../solutions/9_keras_wide_and_deep_babyweight.ipynb) working on a subset of the data, we can package the TensorFlow code up as a Python module and train it on Cloud AI Platform.\n",
    "<p>\n",
    "<h2> Train on Cloud AI Platform</h2>\n",
    "<p>\n",
    "Training on Cloud AI Platform requires:\n",
    "<ol>\n",
    "<li> Making the code a Python package\n",
    "<li> Using gcloud to submit the training code to Cloud AI Platform\n",
    "</ol>\n",
    "\n",
    "Ensure that the AI Platform API is enabled by going to this [link](https://console.developers.google.com/apis/library/ml.googleapis.com).\n",
    "\n",
    "### Move code into a Python package\n",
    "\n",
    "A Python package is simply a collection of one or more `.py` files along with an `__init__.py` file to identify the containing directory as a package. The `__init__.py` sometimes contains initialization code but for our purposes an empty file suffices.\n",
    "\n",
    "The bash command `touch` creates an empty file in the specified location, the directory `babyweight` should already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p babyweight/trainer\n",
    "touch babyweight/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the `%%writefile` magic to write the contents of the cell below to a file called `task.py` in the `babyweight/trainer` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create trainer module's task.py to hold hyperparameter argparsing code.\n",
    "\n",
    "The cell below writes the file `babyweight/trainer/task.py` which sets up our training job. Here is where we determine which parameters of our model to pass as flags during training using the `parser` module. Look at how `batch_size` is passed to the model in the code below. Use this as an example to parse arguements for the following variables\n",
    "- `nnsize` which represents the hidden layer sizes to use for DNN feature columns\n",
    "- `nembeds` which represents the embedding size of a cross of n key real-valued parameters\n",
    "- `train_examples` which represents the number of examples (in thousands) to run the training job\n",
    "- `eval_steps` which represents the positive number of steps for which to evaluate model\n",
    "\n",
    "Be sure to include a default value for the parsed arguments above and specfy the `type` if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile babyweight/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from babyweight.trainer import model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help=\"this model ignores this field, but it is required by gcloud\",\n",
    "        default=\"junk\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help=\"GCS location of training data\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help=\"GCS location of evaluation data\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help=\"Number of examples to compute gradient over.\",\n",
    "        type=int,\n",
    "        default=512\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nnsize\",\n",
    "        help=\"Hidden layer sizes for DNN -- provide space-separated layers\",\n",
    "        nargs=\"+\",\n",
    "        type=int,\n",
    "        default=[128, 32, 4]\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nembeds\",\n",
    "        help=\"Embedding size of a cross of n key real-valued parameters\",\n",
    "        type=int,\n",
    "        default=3\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_epochs\",\n",
    "        help=\"Number of epochs to train the model.\",\n",
    "        type=int,\n",
    "        default=10\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_examples\",\n",
    "        help=\"\"\"Number of examples (in thousands) to run the training job over.\n",
    "        If this is more than actual # of examples available, it cycles through\n",
    "        them. So specifying 1000 here when you have only 100k examples makes\n",
    "        this 10 epochs.\"\"\",\n",
    "        type=int,\n",
    "        default=5000\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_steps\",\n",
    "        help=\"\"\"Positive number of steps for which to evaluate model. Default\n",
    "        to None, which means to evaluate until input_fn raises an end-of-input\n",
    "        exception\"\"\",\n",
    "        type=int,\n",
    "        default=None\n",
    "    )\n",
    "\n",
    "    # Parse all arguments\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # Unused args provided by service\n",
    "    arguments.pop(\"job_dir\", None)\n",
    "    arguments.pop(\"job-dir\", None)\n",
    "\n",
    "    # Modify some arguments\n",
    "    arguments[\"train_examples\"] *= 1000\n",
    "\n",
    "    # Append trial_id to path if we are doing hptuning\n",
    "    # This code can be removed if you are not using hyperparameter tuning\n",
    "    arguments[\"output_dir\"] = os.path.join(\n",
    "        arguments[\"output_dir\"],\n",
    "        json.loads(\n",
    "            os.environ.get(\"TF_CONFIG\", \"{}\")\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    )\n",
    "\n",
    "    # Run the training job\n",
    "    model.train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way we can write to the file `model.py` the model that we developed in the previous notebooks. \n",
    "\n",
    "### Create trainer module's model.py to hold Keras model code.\n",
    "\n",
    "To create our `model.py`, we'll use the code we wrote for the Wide & Deep model. Look back at your [9_keras_wide_and_deep_babyweight](../solutions/9_keras_wide_and_deep_babyweight.ipynb) notebook and copy/paste the necessary code from that notebook into its place in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile babyweight/trainer/model.py\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Determine CSV, label, and key columns\n",
    "CSV_COLUMNS = [\"weight_pounds\",\n",
    "               \"is_male\",\n",
    "               \"mother_age\",\n",
    "               \"plurality\",\n",
    "               \"gestation_weeks\"]\n",
    "LABEL_COLUMN = \"weight_pounds\"\n",
    "\n",
    "# Set default values for each CSV column.\n",
    "# Treat is_male and plurality as strings.\n",
    "DEFAULTS = [[0.0], [\"null\"], [0.0], [\"null\"], [0.0]]\n",
    "\n",
    "\n",
    "def features_and_labels(row_data):\n",
    "    \"\"\"Splits features and labels from feature dictionary.\n",
    "\n",
    "    Args:\n",
    "        row_data: Dictionary of CSV column names and tensor values.\n",
    "    Returns:\n",
    "        Dictionary of feature tensors and label tensor.\n",
    "    \"\"\"\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "\n",
    "    return row_data, label  # features, label\n",
    "\n",
    "\n",
    "def load_dataset(pattern, batch_size=1, mode=tf.estimator.ModeKeys.EVAL):\n",
    "    \"\"\"Loads dataset using the tf.data API from CSV files.\n",
    "\n",
    "    Args:\n",
    "        pattern: str, file pattern to glob into list of files.\n",
    "        batch_size: int, the number of examples per batch.\n",
    "        mode: tf.estimator.ModeKeys to determine if training or evaluating.\n",
    "    Returns:\n",
    "        `Dataset` object.\n",
    "    \"\"\"\n",
    "    print(\"mode = {}\".format(mode))\n",
    "    # Make a CSV dataset\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern=pattern,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_COLUMNS,\n",
    "        column_defaults=DEFAULTS)\n",
    "\n",
    "    # Map dataset to features and label\n",
    "    dataset = dataset.map(map_func=features_and_labels)  # features, label\n",
    "\n",
    "    # Shuffle and repeat for training\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.shuffle(buffer_size=1000).repeat()\n",
    "\n",
    "    # Take advantage of multi-threading; 1=AUTOTUNE\n",
    "    dataset = dataset.prefetch(buffer_size=1)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_input_layers():\n",
    "    \"\"\"Creates dictionary of input layers for each feature.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of `tf.Keras.layers.Input` layers for each feature.\n",
    "    \"\"\"\n",
    "    deep_inputs = {\n",
    "        colname: tf.keras.layers.Input(\n",
    "            name=colname, shape=(), dtype=\"float32\")\n",
    "        for colname in [\"mother_age\", \"gestation_weeks\"]\n",
    "    }\n",
    "\n",
    "    wide_inputs = {\n",
    "        colname: tf.keras.layers.Input(\n",
    "            name=colname, shape=(), dtype=\"string\")\n",
    "        for colname in [\"is_male\", \"plurality\"]\n",
    "    }\n",
    "\n",
    "    inputs = {**wide_inputs, **deep_inputs}\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def categorical_fc(name, values):\n",
    "    \"\"\"Helper function to wrap categorical feature by indicator column.\n",
    "\n",
    "    Args:\n",
    "        name: str, name of feature.\n",
    "        values: list, list of strings of categorical values.\n",
    "    Returns:\n",
    "        Categorical and indicator column of categorical feature.\n",
    "    \"\"\"\n",
    "    cat_column = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "            key=name, vocabulary_list=values)\n",
    "    ind_column = tf.feature_column.indicator_column(\n",
    "        categorical_column=cat_column)\n",
    "\n",
    "    return cat_column, ind_column\n",
    "\n",
    "\n",
    "def create_feature_columns(nembeds):\n",
    "    \"\"\"Creates wide and deep dictionaries of feature columns from inputs.\n",
    "\n",
    "    Args:\n",
    "        nembeds: int, number of dimensions to embed categorical column down to.\n",
    "    Returns:\n",
    "        Wide and deep dictionaries of feature columns.\n",
    "    \"\"\"\n",
    "    deep_fc = {\n",
    "        colname: tf.feature_column.numeric_column(key=colname)\n",
    "        for colname in [\"mother_age\", \"gestation_weeks\"]\n",
    "    }\n",
    "    wide_fc = {}\n",
    "    is_male, wide_fc[\"is_male\"] = categorical_fc(\n",
    "        \"is_male\", [\"True\", \"False\", \"Unknown\"])\n",
    "    plurality, wide_fc[\"plurality\"] = categorical_fc(\n",
    "        \"plurality\", [\"Single(1)\", \"Twins(2)\", \"Triplets(3)\",\n",
    "                      \"Quadruplets(4)\", \"Quintuplets(5)\", \"Multiple(2+)\"])\n",
    "\n",
    "    # Bucketize the float fields. This makes them wide\n",
    "    age_buckets = tf.feature_column.bucketized_column(\n",
    "        source_column=deep_fc[\"mother_age\"],\n",
    "        boundaries=np.arange(15, 45, 1).tolist())\n",
    "    wide_fc[\"age_buckets\"] = tf.feature_column.indicator_column(\n",
    "        categorical_column=age_buckets)\n",
    "\n",
    "    gestation_buckets = tf.feature_column.bucketized_column(\n",
    "        source_column=deep_fc[\"gestation_weeks\"],\n",
    "        boundaries=np.arange(17, 47, 1).tolist())\n",
    "    wide_fc[\"gestation_buckets\"] = tf.feature_column.indicator_column(\n",
    "        categorical_column=gestation_buckets)\n",
    "\n",
    "    # Cross all the wide columns, have to do the crossing before we one-hot\n",
    "    crossed = tf.feature_column.crossed_column(\n",
    "        keys=[age_buckets, gestation_buckets],\n",
    "        hash_bucket_size=1000)\n",
    "    deep_fc[\"crossed_embeds\"] = tf.feature_column.embedding_column(\n",
    "        categorical_column=crossed, dimension=nembeds)\n",
    "\n",
    "    return wide_fc, deep_fc\n",
    "\n",
    "\n",
    "def get_model_outputs(wide_inputs, deep_inputs, dnn_hidden_units):\n",
    "    \"\"\"Creates model architecture and returns outputs.\n",
    "\n",
    "    Args:\n",
    "        wide_inputs: Dense tensor used as inputs to wide side of model.\n",
    "        deep_inputs: Dense tensor used as inputs to deep side of model.\n",
    "        dnn_hidden_units: List of integers where length is number of hidden\n",
    "            layers and ith element is the number of neurons at ith layer.\n",
    "    Returns:\n",
    "        Dense tensor output from the model.\n",
    "    \"\"\"\n",
    "    # Hidden layers for the deep side\n",
    "    layers = [int(x) for x in dnn_hidden_units]\n",
    "    deep = deep_inputs\n",
    "    for layerno, numnodes in enumerate(layers):\n",
    "        deep = tf.keras.layers.Dense(\n",
    "            units=numnodes,\n",
    "            activation=\"relu\",\n",
    "            name=\"dnn_{}\".format(layerno+1))(deep)\n",
    "    deep_out = deep\n",
    "\n",
    "    # Linear model for the wide side\n",
    "    wide_out = tf.keras.layers.Dense(\n",
    "        units=10, activation=\"relu\", name=\"linear\")(wide_inputs)\n",
    "\n",
    "    # Concatenate the two sides\n",
    "    both = tf.keras.layers.concatenate(\n",
    "        inputs=[deep_out, wide_out], name=\"both\")\n",
    "\n",
    "    # Final output is a linear activation because this is regression\n",
    "    output = tf.keras.layers.Dense(\n",
    "        units=1, activation=\"linear\", name=\"weight\")(both)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"Calculates RMSE evaluation metric.\n",
    "\n",
    "    Args:\n",
    "        y_true: tensor, true labels.\n",
    "        y_pred: tensor, predicted labels.\n",
    "    Returns:\n",
    "        Tensor with value of RMSE between true and predicted labels.\n",
    "    \"\"\"\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def build_wide_deep_model(dnn_hidden_units=[64, 32], nembeds=3):\n",
    "    \"\"\"Builds wide and deep model using Keras Functional API.\n",
    "\n",
    "    Returns:\n",
    "        `tf.keras.models.Model` object.\n",
    "    \"\"\"\n",
    "    # Create input layers\n",
    "    inputs = create_input_layers()\n",
    "\n",
    "    # Create feature columns for both wide and deep\n",
    "    wide_fc, deep_fc = create_feature_columns(nembeds)\n",
    "\n",
    "    # The constructor for DenseFeatures takes a list of numeric columns\n",
    "    # The Functional API in Keras requires: LayerConstructor()(inputs)\n",
    "    wide_inputs = tf.keras.layers.DenseFeatures(\n",
    "        feature_columns=wide_fc.values(), name=\"wide_inputs\")(inputs)\n",
    "    deep_inputs = tf.keras.layers.DenseFeatures(\n",
    "        feature_columns=deep_fc.values(), name=\"deep_inputs\")(inputs)\n",
    "\n",
    "    # Get output of model given inputs\n",
    "    output = get_model_outputs(wide_inputs, deep_inputs, dnn_hidden_units)\n",
    "\n",
    "    # Build model and compile it all together\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[rmse, \"mse\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_evaluate(args):\n",
    "    model = build_wide_deep_model(args[\"nnsize\"], args[\"nembeds\"])\n",
    "    print(\"Here is our Wide-and-Deep architecture so far:\\n\")\n",
    "    print(model.summary())\n",
    "\n",
    "    trainds = load_dataset(\n",
    "        args[\"train_data_path\"],\n",
    "        args[\"batch_size\"],\n",
    "        tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    evalds = load_dataset(\n",
    "        args[\"eval_data_path\"], 1000, tf.estimator.ModeKeys.EVAL)\n",
    "    if args[\"eval_steps\"]:\n",
    "        evalds = evalds.take(count=args[\"eval_steps\"])\n",
    "\n",
    "    num_batches = args[\"batch_size\"] * args[\"num_epochs\"]\n",
    "    steps_per_epoch = args[\"train_examples\"] // num_batches\n",
    "\n",
    "    checkpoint_path = os.path.join(args[\"output_dir\"], \"checkpoints/babyweight\")\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path, verbose=1, save_weights_only=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        trainds,\n",
    "        validation_data=evalds,\n",
    "        epochs=args[\"num_epochs\"],\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        verbose=2,  # 0=silent, 1=progress bar, 2=one line per epoch\n",
    "        callbacks=[cp_callback])\n",
    "\n",
    "    EXPORT_PATH = os.path.join(\n",
    "        args[\"output_dir\"], datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    tf.saved_model.save(\n",
    "        obj=model, export_dir=EXPORT_PATH)  # with default serving function\n",
    "    print(\"Exported trained model to {}\".format(EXPORT_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train locally\n",
    "\n",
    "After moving the code to a package, make sure it works as a standalone. Note, we incorporated the `--train_examples` flag so that we don't try to train on the entire dataset while we are developing our pipeline. Once we are sure that everything is working on a subset, we can change it so that we can train on all the data. Even for this subset, this takes about *3 minutes* in which you won't see any output ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run trainer module package locally.\n",
    "\n",
    "We can run a very small training job over a single file with a small batch size, 1 epoch, 1 train example, and 1 eval step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is our Wide-and-Deep architecture so far:\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "gestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "is_male (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mother_age (InputLayer)         [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "plurality (InputLayer)          [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "deep_inputs (DenseFeatures)     (None, 5)            3000        gestation_weeks[0][0]            \n",
      "                                                                 is_male[0][0]                    \n",
      "                                                                 mother_age[0][0]                 \n",
      "                                                                 plurality[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dnn_1 (Dense)                   (None, 128)          768         deep_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dnn_2 (Dense)                   (None, 32)           4128        dnn_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "wide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "                                                                 is_male[0][0]                    \n",
      "                                                                 mother_age[0][0]                 \n",
      "                                                                 plurality[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dnn_3 (Dense)                   (None, 4)            132         dnn_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "linear (Dense)                  (None, 10)           720         wide_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "both (Concatenate)              (None, 14)           0           dnn_3[0][0]                      \n",
      "                                                                 linear[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weight (Dense)                  (None, 1)            15          both[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 8,763\n",
      "Trainable params: 8,763\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "mode = train\n",
      "mode = eval\n",
      "Train for 10 steps, validate for 1 steps\n",
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: saving model to babyweight_trained/checkpoints/babyweight\n",
      "10/10 - 3s - loss: 8.7626 - rmse: 2.6177 - mse: 8.7626 - val_loss: 2.8766 - val_rmse: 1.6961 - val_mse: 2.8766\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: saving model to babyweight_trained/checkpoints/babyweight\n",
      "10/10 - 0s - loss: 3.7634 - rmse: 1.8733 - mse: 3.7634 - val_loss: 2.1518 - val_rmse: 1.4669 - val_mse: 2.1518\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: saving model to babyweight_trained/checkpoints/babyweight\n",
      "10/10 - 0s - loss: 1.4293 - rmse: 1.1661 - mse: 1.4293 - val_loss: 2.1744 - val_rmse: 1.4746 - val_mse: 2.1744\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: saving model to babyweight_trained/checkpoints/babyweight\n",
      "10/10 - 0s - loss: 1.9580 - rmse: 1.3507 - mse: 1.9580 - val_loss: 1.5461 - val_rmse: 1.2434 - val_mse: 1.5461\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: saving model to babyweight_trained/checkpoints/babyweight\n",
      "10/10 - 0s - loss: 1.8938 - rmse: 1.3371 - mse: 1.8938 - val_loss: 1.6900 - val_rmse: 1.3000 - val_mse: 1.6900\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 00006: saving model to babyweight_trained/checkpoints/babyweight\n",
      "10/10 - 0s - loss: 1.5862 - rmse: 1.2189 - mse: 1.5862 - val_loss: 1.8496 - val_rmse: 1.3600 - val_mse: 1.8496\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 00007: saving model to babyweight_trained/checkpoints/babyweight\n",
      "10/10 - 0s - loss: 1.7102 - rmse: 1.2807 - mse: 1.7102 - val_loss: 1.3785 - val_rmse: 1.1741 - val_mse: 1.3785\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 00008: saving model to babyweight_trained/checkpoints/babyweight\n",
      "10/10 - 0s - loss: 1.1809 - rmse: 1.0657 - mse: 1.1809 - val_loss: 1.3404 - val_rmse: 1.1578 - val_mse: 1.3404\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 00009: saving model to babyweight_trained/checkpoints/babyweight\n",
      "10/10 - 0s - loss: 1.5710 - rmse: 1.1937 - mse: 1.5710 - val_loss: 1.2855 - val_rmse: 1.1338 - val_mse: 1.2855\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 00010: saving model to babyweight_trained/checkpoints/babyweight\n",
      "10/10 - 0s - loss: 1.2826 - rmse: 1.1179 - mse: 1.2826 - val_loss: 1.2535 - val_rmse: 1.1196 - val_mse: 1.2535\n",
      "Exported trained model to babyweight_trained/20191030174517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4276: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4331: BucketizedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "2019-10-30 17:45:10.701202: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2019-10-30 17:45:10.709821: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2019-10-30 17:45:10.710156: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56465037f020 executing computations on platform Host. Devices:\n",
      "2019-10-30 17:45:10.710186: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "2019-10-30 17:45:10.710544: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4331: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/data/experimental/ops/readers.py:521: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/data/experimental/ops/readers.py:215: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "2019-10-30 17:45:19.039904: W tensorflow/python/util/util.cc:299] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=babyweight_trained\n",
    "rm -rf ${OUTDIR}\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/babyweight\n",
    "python3 -m trainer.task \\\n",
    "    --job-dir=./tmp \\\n",
    "    --train_data_path=train.csv \\\n",
    "    --eval_data_path=eval.csv \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --batch_size=10 \\\n",
    "    --num_epochs=1 \\\n",
    "    --train_examples=1 \\\n",
    "    --eval_steps=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dockerized module\n",
    "\n",
    "Since we are using TensorFlow 2.0 and it is new, we will use a container image to run the code on AI Platform.\n",
    "\n",
    "Once TensorFlow 2.0 is natively supported on AI Platform, you will be able to simply do (without having to build a container):\n",
    "<pre>\n",
    "gcloud ai-platform jobs submit training ${JOBNAME} \\\n",
    "    --region=${REGION} \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=$(pwd)/babyweight/trainer \\\n",
    "    --job-dir=${OUTDIR} \\\n",
    "    --staging-bucket=gs://${BUCKET} \\\n",
    "    --scale-tier=STANDARD_1 \\\n",
    "    --runtime-version=${TFVERSION} \\\n",
    "    -- \\\n",
    "    --train_data_path=gs://${BUCKET}/babyweight/data/train*.csv \\\n",
    "    --eval_data_path=gs://${BUCKET}/babyweight/data/eval*.csv \\\n",
    "    --output_dir=${OUTDIR}\n",
    "    --batch_size=32 \\\n",
    "    --num_epochs=10 \\\n",
    "    --train_examples=20000 \\\n",
    "    --eval_steps=100\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dockerfile\n",
    "\n",
    "We need to create a container with everything we need to be able to run our model. This includes our trainer module package, python3, as well as the libraries we use such as the most up to date TensorFlow 2.0 version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile babyweight/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-cpu\n",
    "COPY trainer /babyweight/trainer\n",
    "RUN apt update && \\\n",
    "    apt install --yes python3-pip && \\\n",
    "    pip3 install --upgrade --quiet tf-nightly-2.0-preview\n",
    "\n",
    "ENV PYTHONPATH ${PYTHONPATH}:/babyweight\n",
    "ENTRYPOINT [\"python3\", \"babyweight/trainer/task.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and push container image to repo\n",
    "\n",
    "Now that we have created our Dockerfile, we need to build and push our container image to our project's container repo. To do this, we'll create a small shell script that we can call from the bash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile babyweight/push_docker.sh\n",
    "export PROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\n",
    "export IMAGE_REPO_NAME=babyweight_training_container\n",
    "export IMAGE_URI=gcr.io/$PROJECT_ID/$IMAGE_REPO_NAME\n",
    "\n",
    "echo \"Building  $IMAGE_URI\"\n",
    "docker build -f Dockerfile -t $IMAGE_URI ./\n",
    "echo \"Pushing $IMAGE_URI\"\n",
    "docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you get a permissions/stat error when running push_docker.sh from Notebooks, do it from CloudShell:\n",
    "\n",
    "Open CloudShell on the GCP Console\n",
    "* git clone https://github.com/GoogleCloudPlatform/training-data-analyst\n",
    "* cd training-data-analyst/courses/machine_learning/deepdive2/structured/solutions/babyweight\n",
    "* bash push_docker.sh\n",
    "\n",
    "This step takes 5-10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building  gcr.io/qwiklabs-gcp-4b437f7e5bfff9dd/babyweight_training_container\n",
      "Sending build context to Docker daemon  36.35kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/tf2-cpu\n",
      " ---> bed936671274\n",
      "Step 2/5 : COPY trainer /babyweight/trainer\n",
      " ---> Using cache\n",
      " ---> 3c07d08c2528\n",
      "Step 3/5 : RUN apt update &&     apt install --yes python3-pip &&     pip3 install --upgrade --quiet tf-nightly-2.0-preview\n",
      " ---> Running in 4600f5d7e84a\n",
      "\u001b[91m\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "\u001b[0mGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Get:2 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6372 B]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]\n",
      "Get:4 http://packages.cloud.google.com/apt cloud-sdk-bionic/main amd64 Packages [92.7 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [5944 B]\n",
      "Get:6 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [700 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [782 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [12.6 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [23.2 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1303 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [995 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [9022 B]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [2496 B]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [4227 B]\n",
      "Fetched 17.3 MB in 10s (1770 kB/s)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "48 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\u001b[91m\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "\u001b[0mReading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  dh-python libexpat1-dev libpython3-dev libpython3.6 libpython3.6-dev\n",
      "  libpython3.6-minimal libpython3.6-stdlib python-pip-whl python3-asn1crypto\n",
      "  python3-cffi-backend python3-crypto python3-cryptography python3-dev\n",
      "  python3-distutils python3-idna python3-keyring python3-keyrings.alt\n",
      "  python3-lib2to3 python3-pkg-resources python3-secretstorage\n",
      "  python3-setuptools python3-six python3-wheel python3-xdg python3.6\n",
      "  python3.6-dev python3.6-minimal\n",
      "Suggested packages:\n",
      "  python-crypto-doc python-cryptography-doc python3-cryptography-vectors\n",
      "  gnome-keyring libkf5wallet-bin gir1.2-gnomekeyring-1.0\n",
      "  python-secretstorage-doc python-setuptools-doc python3.6-venv python3.6-doc\n",
      "  binfmt-support\n",
      "The following NEW packages will be installed:\n",
      "  dh-python libexpat1-dev libpython3-dev libpython3.6-dev python-pip-whl\n",
      "  python3-asn1crypto python3-cffi-backend python3-crypto python3-cryptography\n",
      "  python3-dev python3-distutils python3-idna python3-keyring\n",
      "  python3-keyrings.alt python3-lib2to3 python3-pip python3-pkg-resources\n",
      "  python3-secretstorage python3-setuptools python3-six python3-wheel\n",
      "  python3-xdg python3.6-dev\n",
      "The following packages will be upgraded:\n",
      "  libpython3.6 libpython3.6-minimal libpython3.6-stdlib python3.6\n",
      "  python3.6-minimal\n",
      "5 upgraded, 23 newly installed, 0 to remove and 43 not upgraded.\n",
      "Need to get 54.1 MB of archives.\n",
      "After this operation, 89.3 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpython3.6 amd64 3.6.8-1~18.04.3 [1415 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3.6 amd64 3.6.8-1~18.04.3 [202 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpython3.6-stdlib amd64 3.6.8-1~18.04.3 [1712 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3.6-minimal amd64 3.6.8-1~18.04.3 [1610 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpython3.6-minimal amd64 3.6.8-1~18.04.3 [533 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-lib2to3 all 3.6.8-1~18.04 [76.5 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-distutils all 3.6.8-1~18.04 [141 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 dh-python all 3.20180325ubuntu2 [89.2 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libexpat1-dev amd64 2.2.5-3ubuntu0.2 [122 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpython3.6-dev amd64 3.6.8-1~18.04.3 [44.8 MB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpython3-dev amd64 3.6.7-1~18.04 [7328 B]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python-pip-whl all 9.0.1-2.3~ubuntu1.18.04.1 [1653 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-asn1crypto all 0.24.0-1 [72.8 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-cffi-backend amd64 1.11.5-1 [64.6 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-crypto amd64 2.6.1-8ubuntu2 [244 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-idna all 2.6-1 [32.5 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-six all 1.11.0-2 [11.4 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-cryptography amd64 2.1.4-1ubuntu1.3 [221 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3.6-dev amd64 3.6.8-1~18.04.3 [508 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-dev amd64 3.6.7-1~18.04 [1288 B]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-secretstorage all 2.3.1-2 [12.1 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-keyring all 10.6.0-1 [26.7 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-keyrings.alt all 3.0-1 [16.6 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python3-pip all 9.0.1-2.3~ubuntu1.18.04.1 [114 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-pkg-resources all 39.0.1-2 [98.8 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-setuptools all 39.0.1-2 [248 kB]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-wheel all 0.30.0-0.2 [36.5 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-xdg all 0.25-4ubuntu1 [31.4 kB]\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mFetched 54.1 MB in 23s (2360 kB/s)\n",
      "(Reading database ... 85082 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libpython3.6_3.6.8-1~18.04.3_amd64.deb ...\n",
      "Unpacking libpython3.6:amd64 (3.6.8-1~18.04.3) over (3.6.8-1~18.04.2) ...\n",
      "Preparing to unpack .../01-python3.6_3.6.8-1~18.04.3_amd64.deb ...\n",
      "Unpacking python3.6 (3.6.8-1~18.04.3) over (3.6.8-1~18.04.2) ...\n",
      "Preparing to unpack .../02-libpython3.6-stdlib_3.6.8-1~18.04.3_amd64.deb ...\n",
      "Unpacking libpython3.6-stdlib:amd64 (3.6.8-1~18.04.3) over (3.6.8-1~18.04.2) ...\n",
      "Preparing to unpack .../03-python3.6-minimal_3.6.8-1~18.04.3_amd64.deb ...\n",
      "Unpacking python3.6-minimal (3.6.8-1~18.04.3) over (3.6.8-1~18.04.2) ...\n",
      "Preparing to unpack .../04-libpython3.6-minimal_3.6.8-1~18.04.3_amd64.deb ...\n",
      "Unpacking libpython3.6-minimal:amd64 (3.6.8-1~18.04.3) over (3.6.8-1~18.04.2) ...\n",
      "Selecting previously unselected package python3-lib2to3.\n",
      "Preparing to unpack .../05-python3-lib2to3_3.6.8-1~18.04_all.deb ...\n",
      "Unpacking python3-lib2to3 (3.6.8-1~18.04) ...\n",
      "Selecting previously unselected package python3-distutils.\n",
      "Preparing to unpack .../06-python3-distutils_3.6.8-1~18.04_all.deb ...\n",
      "Unpacking python3-distutils (3.6.8-1~18.04) ...\n",
      "Selecting previously unselected package dh-python.\n",
      "Preparing to unpack .../07-dh-python_3.20180325ubuntu2_all.deb ...\n",
      "Unpacking dh-python (3.20180325ubuntu2) ...\n",
      "Selecting previously unselected package libexpat1-dev:amd64.\n",
      "Preparing to unpack .../08-libexpat1-dev_2.2.5-3ubuntu0.2_amd64.deb ...\n",
      "Unpacking libexpat1-dev:amd64 (2.2.5-3ubuntu0.2) ...\n",
      "Selecting previously unselected package libpython3.6-dev:amd64.\n",
      "Preparing to unpack .../09-libpython3.6-dev_3.6.8-1~18.04.3_amd64.deb ...\n",
      "Unpacking libpython3.6-dev:amd64 (3.6.8-1~18.04.3) ...\n",
      "Selecting previously unselected package libpython3-dev:amd64.\n",
      "Preparing to unpack .../10-libpython3-dev_3.6.7-1~18.04_amd64.deb ...\n",
      "Unpacking libpython3-dev:amd64 (3.6.7-1~18.04) ...\n",
      "Selecting previously unselected package python-pip-whl.\n",
      "Preparing to unpack .../11-python-pip-whl_9.0.1-2.3~ubuntu1.18.04.1_all.deb ...\n",
      "Unpacking python-pip-whl (9.0.1-2.3~ubuntu1.18.04.1) ...\n",
      "Selecting previously unselected package python3-asn1crypto.\n",
      "Preparing to unpack .../12-python3-asn1crypto_0.24.0-1_all.deb ...\n",
      "Unpacking python3-asn1crypto (0.24.0-1) ...\n",
      "Selecting previously unselected package python3-cffi-backend.\n",
      "Preparing to unpack .../13-python3-cffi-backend_1.11.5-1_amd64.deb ...\n",
      "Unpacking python3-cffi-backend (1.11.5-1) ...\n",
      "Selecting previously unselected package python3-crypto.\n",
      "Preparing to unpack .../14-python3-crypto_2.6.1-8ubuntu2_amd64.deb ...\n",
      "Unpacking python3-crypto (2.6.1-8ubuntu2) ...\n",
      "Selecting previously unselected package python3-idna.\n",
      "Preparing to unpack .../15-python3-idna_2.6-1_all.deb ...\n",
      "Unpacking python3-idna (2.6-1) ...\n",
      "Selecting previously unselected package python3-six.\n",
      "Preparing to unpack .../16-python3-six_1.11.0-2_all.deb ...\n",
      "Unpacking python3-six (1.11.0-2) ...\n",
      "Selecting previously unselected package python3-cryptography.\n",
      "Preparing to unpack .../17-python3-cryptography_2.1.4-1ubuntu1.3_amd64.deb ...\n",
      "Unpacking python3-cryptography (2.1.4-1ubuntu1.3) ...\n",
      "Selecting previously unselected package python3.6-dev.\n",
      "Preparing to unpack .../18-python3.6-dev_3.6.8-1~18.04.3_amd64.deb ...\n",
      "Unpacking python3.6-dev (3.6.8-1~18.04.3) ...\n",
      "Selecting previously unselected package python3-dev.\n",
      "Preparing to unpack .../19-python3-dev_3.6.7-1~18.04_amd64.deb ...\n",
      "Unpacking python3-dev (3.6.7-1~18.04) ...\n",
      "Selecting previously unselected package python3-secretstorage.\n",
      "Preparing to unpack .../20-python3-secretstorage_2.3.1-2_all.deb ...\n",
      "Unpacking python3-secretstorage (2.3.1-2) ...\n",
      "Selecting previously unselected package python3-keyring.\n",
      "Preparing to unpack .../21-python3-keyring_10.6.0-1_all.deb ...\n",
      "Unpacking python3-keyring (10.6.0-1) ...\n",
      "Selecting previously unselected package python3-keyrings.alt.\n",
      "Preparing to unpack .../22-python3-keyrings.alt_3.0-1_all.deb ...\n",
      "Unpacking python3-keyrings.alt (3.0-1) ...\n",
      "Selecting previously unselected package python3-pip.\n",
      "Preparing to unpack .../23-python3-pip_9.0.1-2.3~ubuntu1.18.04.1_all.deb ...\n",
      "Unpacking python3-pip (9.0.1-2.3~ubuntu1.18.04.1) ...\n",
      "Selecting previously unselected package python3-pkg-resources.\n",
      "Preparing to unpack .../24-python3-pkg-resources_39.0.1-2_all.deb ...\n",
      "Unpacking python3-pkg-resources (39.0.1-2) ...\n",
      "Selecting previously unselected package python3-setuptools.\n",
      "Preparing to unpack .../25-python3-setuptools_39.0.1-2_all.deb ...\n",
      "Unpacking python3-setuptools (39.0.1-2) ...\n",
      "Selecting previously unselected package python3-wheel.\n",
      "Preparing to unpack .../26-python3-wheel_0.30.0-0.2_all.deb ...\n",
      "Unpacking python3-wheel (0.30.0-0.2) ...\n",
      "Selecting previously unselected package python3-xdg.\n",
      "Preparing to unpack .../27-python3-xdg_0.25-4ubuntu1_all.deb ...\n",
      "Unpacking python3-xdg (0.25-4ubuntu1) ...\n",
      "Setting up python-pip-whl (9.0.1-2.3~ubuntu1.18.04.1) ...\n",
      "Processing triggers for mime-support (3.60ubuntu1) ...\n",
      "Setting up python3-cffi-backend (1.11.5-1) ...\n",
      "Setting up python3-crypto (2.6.1-8ubuntu2) ...\n",
      "Setting up python3-idna (2.6-1) ...\n",
      "Setting up python3-xdg (0.25-4ubuntu1) ...\n",
      "Setting up python3-six (1.11.0-2) ...\n",
      "Setting up python3-wheel (0.30.0-0.2) ...\n",
      "Setting up python3-pkg-resources (39.0.1-2) ...\n",
      "Setting up libpython3.6-minimal:amd64 (3.6.8-1~18.04.3) ...\n",
      "Setting up python3-asn1crypto (0.24.0-1) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
      "Setting up libexpat1-dev:amd64 (2.2.5-3ubuntu0.2) ...\n",
      "Setting up python3-lib2to3 (3.6.8-1~18.04) ...\n",
      "Setting up python3-distutils (3.6.8-1~18.04) ...\n",
      "Setting up python3-cryptography (2.1.4-1ubuntu1.3) ...\n",
      "Setting up libpython3.6-stdlib:amd64 (3.6.8-1~18.04.3) ...\n",
      "Setting up python3-keyrings.alt (3.0-1) ...\n",
      "Setting up python3.6-minimal (3.6.8-1~18.04.3) ...\n",
      "Setting up python3-pip (9.0.1-2.3~ubuntu1.18.04.1) ...\n",
      "Setting up python3-setuptools (39.0.1-2) ...\n",
      "Setting up python3-secretstorage (2.3.1-2) ...\n",
      "Setting up dh-python (3.20180325ubuntu2) ...\n",
      "Setting up libpython3.6:amd64 (3.6.8-1~18.04.3) ...\n",
      "Setting up python3.6 (3.6.8-1~18.04.3) ...\n",
      "Setting up python3-keyring (10.6.0-1) ...\n",
      "Setting up libpython3.6-dev:amd64 (3.6.8-1~18.04.3) ...\n",
      "Setting up python3.6-dev (3.6.8-1~18.04.3) ...\n",
      "Setting up libpython3-dev:amd64 (3.6.7-1~18.04) ...\n",
      "Setting up python3-dev (3.6.7-1~18.04) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
      "Removing intermediate container 4600f5d7e84a\n",
      " ---> bd32de3d9d7d\n",
      "Step 4/5 : ENV PYTHONPATH ${PYTHONPATH}:/babyweight\n",
      " ---> Running in 7f2a7b00dc22\n",
      "Removing intermediate container 7f2a7b00dc22\n",
      " ---> 3f5eb73fe990\n",
      "Step 5/5 : ENTRYPOINT [\"python3\", \"babyweight/trainer/task.py\"]\n",
      " ---> Running in 5e7037953f34\n",
      "Removing intermediate container 5e7037953f34\n",
      " ---> 35bb7a827663\n",
      "Successfully built 35bb7a827663\n",
      "Successfully tagged gcr.io/qwiklabs-gcp-4b437f7e5bfff9dd/babyweight_training_container:latest\n",
      "Pushing gcr.io/qwiklabs-gcp-4b437f7e5bfff9dd/babyweight_training_container\n",
      "The push refers to repository [gcr.io/qwiklabs-gcp-4b437f7e5bfff9dd/babyweight_training_container]\n",
      "a8f2b0322007: Preparing\n",
      "c3f121ae99b2: Preparing\n",
      "bf90a9083c07: Preparing\n",
      "11d81998383e: Preparing\n",
      "95d20553f9d8: Preparing\n",
      "db39d1d921f4: Preparing\n",
      "30a800c3bce3: Preparing\n",
      "b4e8d60ebd43: Preparing\n",
      "e1331da2fed4: Preparing\n",
      "47b98e0067a4: Preparing\n",
      "1540993b0bb8: Preparing\n",
      "aeb8ac7a4b4c: Preparing\n",
      "3502212822b5: Preparing\n",
      "a792cec8f1b3: Preparing\n",
      "fd6e4fea6397: Preparing\n",
      "e661f78768b4: Preparing\n",
      "cb4cde3af37a: Preparing\n",
      "122be11ab4a2: Preparing\n",
      "7beb13bce073: Preparing\n",
      "f7eae43028b3: Preparing\n",
      "6cebf3abed5f: Preparing\n",
      "1540993b0bb8: Waiting\n",
      "aeb8ac7a4b4c: Waiting\n",
      "3502212822b5: Waiting\n",
      "a792cec8f1b3: Waiting\n",
      "fd6e4fea6397: Waiting\n",
      "e661f78768b4: Waiting\n",
      "cb4cde3af37a: Waiting\n",
      "122be11ab4a2: Waiting\n",
      "7beb13bce073: Waiting\n",
      "f7eae43028b3: Waiting\n",
      "6cebf3abed5f: Waiting\n",
      "db39d1d921f4: Waiting\n",
      "30a800c3bce3: Waiting\n",
      "b4e8d60ebd43: Waiting\n",
      "e1331da2fed4: Waiting\n",
      "47b98e0067a4: Waiting\n",
      "bf90a9083c07: Layer already exists\n",
      "95d20553f9d8: Layer already exists\n",
      "11d81998383e: Layer already exists\n",
      "db39d1d921f4: Layer already exists\n",
      "30a800c3bce3: Layer already exists\n",
      "b4e8d60ebd43: Layer already exists\n",
      "e1331da2fed4: Layer already exists\n",
      "47b98e0067a4: Layer already exists\n",
      "1540993b0bb8: Layer already exists\n",
      "aeb8ac7a4b4c: Layer already exists\n",
      "a792cec8f1b3: Layer already exists\n",
      "fd6e4fea6397: Layer already exists\n",
      "3502212822b5: Layer already exists\n",
      "cb4cde3af37a: Layer already exists\n",
      "e661f78768b4: Layer already exists\n",
      "122be11ab4a2: Layer already exists\n",
      "7beb13bce073: Layer already exists\n",
      "f7eae43028b3: Layer already exists\n",
      "6cebf3abed5f: Layer already exists\n",
      "c3f121ae99b2: Pushed\n",
      "a8f2b0322007: Pushed\n",
      "latest: digest: sha256:24a648d56f000fd0fb9bbd9bdd25a05c0e589e80ea2ab475f45da19f9eae1b63 size: 4714\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd babyweight\n",
    "bash push_docker.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test container locally\n",
    "\n",
    "Before we submit our training job to Cloud AI Platform, let's make sure our container that we just built and pushed to our project's container repo works perfectly. We can do that by calling our container in bash and passing the necessary user_args for our task.py's parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running  gcr.io/qwiklabs-gcp-4b437f7e5bfff9dd/babyweight_training_container\n",
      "Here is our Wide-and-Deep architecture so far:\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "gestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "is_male (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mother_age (InputLayer)         [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "plurality (InputLayer)          [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "deep_inputs (DenseFeatures)     (None, 5)            3000        gestation_weeks[0][0]            \n",
      "                                                                 is_male[0][0]                    \n",
      "                                                                 mother_age[0][0]                 \n",
      "                                                                 plurality[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dnn_1 (Dense)                   (None, 128)          768         deep_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dnn_2 (Dense)                   (None, 32)           4128        dnn_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "wide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "                                                                 is_male[0][0]                    \n",
      "                                                                 mother_age[0][0]                 \n",
      "                                                                 plurality[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dnn_3 (Dense)                   (None, 4)            132         dnn_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "linear (Dense)                  (None, 10)           720         wide_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "both (Concatenate)              (None, 14)           0           dnn_3[0][0]                      \n",
      "                                                                 linear[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weight (Dense)                  (None, 1)            15          both[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 8,763\n",
      "Trainable params: 8,763\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "mode = train\n",
      "mode = eval\n",
      "Train for 10 steps, validate for 1 steps\n",
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: saving model to gs://qwiklabs-gcp-4b437f7e5bfff9dd/babyweight/trained_model/checkpoints/babyweight\n",
      "10/10 - 17s - loss: 3.8755 - rmse: 1.9246 - mse: 3.8755 - val_loss: 6.2796 - val_rmse: 2.5059 - val_mse: 6.2796\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: saving model to gs://qwiklabs-gcp-4b437f7e5bfff9dd/babyweight/trained_model/checkpoints/babyweight\n",
      "10/10 - 8s - loss: 4.4790 - rmse: 2.0677 - mse: 4.4790 - val_loss: 4.3228 - val_rmse: 2.0791 - val_mse: 4.3228\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: saving model to gs://qwiklabs-gcp-4b437f7e5bfff9dd/babyweight/trained_model/checkpoints/babyweight\n",
      "10/10 - 8s - loss: 3.1877 - rmse: 1.7655 - mse: 3.1877 - val_loss: 3.2621 - val_rmse: 1.8061 - val_mse: 3.2621\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: saving model to gs://qwiklabs-gcp-4b437f7e5bfff9dd/babyweight/trained_model/checkpoints/babyweight\n",
      "10/10 - 8s - loss: 3.2268 - rmse: 1.7080 - mse: 3.2268 - val_loss: 2.9845 - val_rmse: 1.7276 - val_mse: 2.9845\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: saving model to gs://qwiklabs-gcp-4b437f7e5bfff9dd/babyweight/trained_model/checkpoints/babyweight\n",
      "10/10 - 8s - loss: 2.9043 - rmse: 1.6701 - mse: 2.9043 - val_loss: 3.1875 - val_rmse: 1.7853 - val_mse: 3.1875\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 00006: saving model to gs://qwiklabs-gcp-4b437f7e5bfff9dd/babyweight/trained_model/checkpoints/babyweight\n",
      "10/10 - 9s - loss: 3.3367 - rmse: 1.8113 - mse: 3.3367 - val_loss: 2.7325 - val_rmse: 1.6530 - val_mse: 2.7325\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 00007: saving model to gs://qwiklabs-gcp-4b437f7e5bfff9dd/babyweight/trained_model/checkpoints/babyweight\n",
      "10/10 - 7s - loss: 2.8142 - rmse: 1.6597 - mse: 2.8142 - val_loss: 2.6752 - val_rmse: 1.6356 - val_mse: 2.6752\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 00008: saving model to gs://qwiklabs-gcp-4b437f7e5bfff9dd/babyweight/trained_model/checkpoints/babyweight\n",
      "10/10 - 8s - loss: 2.4923 - rmse: 1.5440 - mse: 2.4923 - val_loss: 2.9194 - val_rmse: 1.7086 - val_mse: 2.9194\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 00009: saving model to gs://qwiklabs-gcp-4b437f7e5bfff9dd/babyweight/trained_model/checkpoints/babyweight\n",
      "10/10 - 8s - loss: 3.2297 - rmse: 1.7768 - mse: 3.2297 - val_loss: 3.1855 - val_rmse: 1.7848 - val_mse: 3.1855\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 00010: saving model to gs://qwiklabs-gcp-4b437f7e5bfff9dd/babyweight/trained_model/checkpoints/babyweight\n",
      "10/10 - 8s - loss: 3.0952 - rmse: 1.7117 - mse: 3.0952 - val_loss: 3.4157 - val_rmse: 1.8482 - val_mse: 3.4157\n",
      "Exported trained model to gs://qwiklabs-gcp-4b437f7e5bfff9dd/babyweight/trained_model/20191030175118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.5/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4276: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.5/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4331: BucketizedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "2019-10-30 17:49:47.083403: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "User settings:\n",
      "\n",
      "   KMP_AFFINITY=granularity=fine,verbose,compact,1,0\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_SETTINGS=1\n",
      "\n",
      "Effective settings:\n",
      "\n",
      "   KMP_ABORT_DELAY=0\n",
      "   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\n",
      "   KMP_ALIGN_ALLOC=64\n",
      "   KMP_ALL_THREADPRIVATE=128\n",
      "   KMP_ATOMIC_MODE=2\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_CPUINFO_FILE: value is not defined\n",
      "   KMP_DETERMINISTIC_REDUCTION=false\n",
      "   KMP_DEVICE_THREAD_LIMIT=2147483647\n",
      "   KMP_DISP_HAND_THREAD=false\n",
      "   KMP_DISP_NUM_BUFFERS=7\n",
      "   KMP_DUPLICATE_LIB_OK=false\n",
      "   KMP_FORCE_REDUCTION: value is not defined\n",
      "   KMP_FOREIGN_THREADS_THREADPRIVATE=true\n",
      "   KMP_FORKJOIN_BARRIER='2,2'\n",
      "   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_FORKJOIN_FRAMES=true\n",
      "   KMP_FORKJOIN_FRAMES_MODE=3\n",
      "   KMP_GTID_MODE=3\n",
      "   KMP_HANDLE_SIGNALS=false\n",
      "   KMP_HOT_TEAMS_MAX_LEVEL=1\n",
      "   KMP_HOT_TEAMS_MODE=0\n",
      "   KMP_INIT_AT_FORK=true\n",
      "   KMP_ITT_PREPARE_DELAY=0\n",
      "   KMP_LIBRARY=throughput\n",
      "   KMP_LOCK_KIND=queuing\n",
      "   KMP_MALLOC_POOL_INCR=1M\n",
      "   KMP_MWAIT_HINTS=0\n",
      "   KMP_NUM_LOCKS_IN_BLOCK=1\n",
      "   KMP_PLAIN_BARRIER='2,2'\n",
      "   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_REDUCTION_BARRIER='1,1'\n",
      "   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_SCHEDULE='static,balanced;guided,iterative'\n",
      "   KMP_SETTINGS=true\n",
      "   KMP_SPIN_BACKOFF_PARAMS='4096,100'\n",
      "   KMP_STACKOFFSET=64\n",
      "   KMP_STACKPAD=0\n",
      "   KMP_STACKSIZE=8M\n",
      "   KMP_STORAGE_MAP=false\n",
      "   KMP_TASKING=2\n",
      "   KMP_TASKLOOP_MIN_TASKS=0\n",
      "   KMP_TASK_STEALING_CONSTRAINT=1\n",
      "   KMP_TEAMS_THREAD_LIMIT=4\n",
      "   KMP_TOPOLOGY_METHOD=all\n",
      "   KMP_USER_LEVEL_MWAIT=false\n",
      "   KMP_USE_YIELD=1\n",
      "   KMP_VERSION=false\n",
      "   KMP_WARNINGS=true\n",
      "   OMP_AFFINITY_FORMAT='OMP: pid %P tid %i thread %n bound to OS proc set {%A}'\n",
      "   OMP_ALLOCATOR=omp_default_mem_alloc\n",
      "   OMP_CANCELLATION=false\n",
      "   OMP_DEBUG=disabled\n",
      "   OMP_DEFAULT_DEVICE=0\n",
      "   OMP_DISPLAY_AFFINITY=false\n",
      "   OMP_DISPLAY_ENV=false\n",
      "   OMP_DYNAMIC=false\n",
      "   OMP_MAX_ACTIVE_LEVELS=2147483647\n",
      "   OMP_MAX_TASK_PRIORITY=0\n",
      "   OMP_NESTED=false\n",
      "   OMP_NUM_THREADS: value is not defined\n",
      "   OMP_PLACES: value is not defined\n",
      "   OMP_PROC_BIND='intel'\n",
      "   OMP_SCHEDULE='static'\n",
      "   OMP_STACKSIZE=8M\n",
      "   OMP_TARGET_OFFLOAD=DEFAULT\n",
      "   OMP_THREAD_LIMIT=2147483647\n",
      "   OMP_TOOL=enabled\n",
      "   OMP_TOOL_LIBRARIES: value is not defined\n",
      "   OMP_WAIT_POLICY=PASSIVE\n",
      "   KMP_AFFINITY='verbose,warnings,respect,granularity=fine,compact,1,0'\n",
      "\n",
      "2019-10-30 17:49:47.536790: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2019-10-30 17:49:47.537312: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562631508020 executing computations on platform Host. Devices:\n",
      "2019-10-30 17:49:47.537355: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.\n",
      "OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\n",
      "OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-3\n",
      "OMP: Info #156: KMP_AFFINITY: 4 available OS procs\n",
      "OMP: Info #157: KMP_AFFINITY: Uniform topology\n",
      "OMP: Info #179: KMP_AFFINITY: 1 packages x 2 cores/pkg x 2 threads/core (2 total cores)\n",
      "OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:\n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 0 thread 1 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 1 thread 1 \n",
      "OMP: Info #250: KMP_AFFINITY: pid 1 tid 1 thread 0 bound to OS proc set 0\n",
      "2019-10-30 17:49:47.566316: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.5/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4331: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.5/site-packages/tensorflow_core/python/data/experimental/ops/readers.py:521: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.5/site-packages/tensorflow_core/python/data/experimental/ops/readers.py:215: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "OMP: Info #250: KMP_AFFINITY: pid 1 tid 31 thread 1 bound to OS proc set 1\n",
      "OMP: Info #250: KMP_AFFINITY: pid 1 tid 33 thread 2 bound to OS proc set 2\n",
      "OMP: Info #250: KMP_AFFINITY: pid 1 tid 26 thread 3 bound to OS proc set 3\n",
      "OMP: Info #250: KMP_AFFINITY: pid 1 tid 27 thread 4 bound to OS proc set 0\n",
      "OMP: Info #250: KMP_AFFINITY: pid 1 tid 38 thread 6 bound to OS proc set 2\n",
      "OMP: Info #250: KMP_AFFINITY: pid 1 tid 37 thread 5 bound to OS proc set 1\n",
      "OMP: Info #250: KMP_AFFINITY: pid 1 tid 39 thread 7 bound to OS proc set 3\n",
      "OMP: Info #250: KMP_AFFINITY: pid 1 tid 40 thread 8 bound to OS proc set 0\n",
      "OMP: Info #250: KMP_AFFINITY: pid 1 tid 41 thread 9 bound to OS proc set 1\n",
      "OMP: Info #250: KMP_AFFINITY: pid 1 tid 42 thread 10 bound to OS proc set 2\n",
      "OMP: Info #250: KMP_AFFINITY: pid 1 tid 28 thread 11 bound to OS proc set 3\n",
      "OMP: Info #250: KMP_AFFINITY: pid 1 tid 46 thread 12 bound to OS proc set 0\n",
      "OMP: Info #250: KMP_AFFINITY: pid 1 tid 51 thread 12 bound to OS proc set 0\n",
      "OMP: Info #250: KMP_AFFINITY: pid 1 tid 56 thread 12 bound to OS proc set 0\n",
      "OMP: Info #250: KMP_AFFINITY: pid 1 tid 61 thread 12 bound to OS proc set 0\n",
      "OMP: Info #250: KMP_AFFINITY: pid 1 tid 66 thread 12 bound to OS proc set 0\n",
      "OMP: Info #250: KMP_AFFINITY: pid 1 tid 71 thread 12 bound to OS proc set 0\n",
      "OMP: Info #250: KMP_AFFINITY: pid 1 tid 76 thread 12 bound to OS proc set 0\n",
      "OMP: Info #250: KMP_AFFINITY: pid 1 tid 81 thread 12 bound to OS proc set 0\n",
      "OMP: Info #250: KMP_AFFINITY: pid 1 tid 86 thread 12 bound to OS proc set 0\n",
      "OMP: Info #250: KMP_AFFINITY: pid 1 tid 91 thread 12 bound to OS proc set 0\n",
      "2019-10-30 17:51:19.949620: W tensorflow/python/util/util.cc:299] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.5/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\n",
    "export IMAGE_REPO_NAME=babyweight_training_container\n",
    "export IMAGE_URI=gcr.io/$PROJECT_ID/$IMAGE_REPO_NAME\n",
    "echo \"Running  $IMAGE_URI\"\n",
    "docker run $IMAGE_URI \\\n",
    "    --train_data_path=gs://${BUCKET}/babyweight/data/train*.csv \\\n",
    "    --train_data_path=gs://${BUCKET}/babyweight/data/train*.csv \\\n",
    "    --eval_data_path=gs://${BUCKET}/babyweight/data/eval*.csv \\\n",
    "    --output_dir=gs://${BUCKET}/babyweight/trained_model \\\n",
    "    --batch_size=10 \\\n",
    "    --num_epochs=10 \\\n",
    "    --train_examples=1 \\\n",
    "    --eval_steps=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on Cloud AI Platform\n",
    "\n",
    "Once the code works in standalone mode, you can run it on Cloud AI Platform. Because this is on the entire dataset, it will take a while. The training run took about <b> two hours </b> for me. You can monitor the job from the GCP console in the Cloud AI Platform section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/babyweight/trained_model\n",
    "JOBID=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "echo ${OUTDIR} ${REGION} ${JOBNAME}\n",
    "gsutil -m rm -rf ${OUTDIR}\n",
    "\n",
    "IMAGE=gcr.io/$PROJECT/babyweight_training_container\n",
    "\n",
    "gcloud ai-platform jobs submit training ${JOBID} \\\n",
    "    --staging-bucket=gs://${BUCKET} \\\n",
    "    --region=${REGION} \\\n",
    "    --master-image-uri=${IMAGE} \\\n",
    "    --master-machine-type=n1-standard-4 \\\n",
    "    --scale-tier=CUSTOM \\\n",
    "    -- \\\n",
    "    --train_data_path=gs://${BUCKET}/babyweight/data/train*.csv \\\n",
    "    --eval_data_path=gs://${BUCKET}/babyweight/data/eval*.csv \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --batch_size=32 \\\n",
    "    --num_epochs=10 \\\n",
    "    --train_examples=20000 \\\n",
    "    --eval_steps=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran it, I used train_examples=2000000. When training finished, I filtered in the Stackdriver log on the word \"dict\" and saw that the last line was:\n",
    "<pre>\n",
    "Saving dict for global step 5714290: average_loss = 1.06473, global_step = 5714290, loss = 34882.4, rmse = 1.03186\n",
    "</pre>\n",
    "The final RMSE was 1.03 pounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning.\n",
    "\n",
    "All of these are command-line parameters to my program.  To do hyperparameter tuning, create `hyperparam.yaml` and pass it as `--config hyperparam.yaml`.\n",
    "This step will take <b>up to 2 hours</b> -- you can increase `maxParallelTrials` or reduce `maxTrials` to get it done faster.  Since `maxParallelTrials` is the number of initial seeds to start searching from, you don't want it to be too large; otherwise, all you have is a random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "    scaleTier: STANDARD_1\n",
    "    hyperparameters:\n",
    "        hyperparameterMetricTag: rmse\n",
    "        goal: MINIMIZE\n",
    "        maxTrials: 20\n",
    "        maxParallelTrials: 5\n",
    "        enableTrialEarlyStopping: True\n",
    "        params:\n",
    "        - parameterName: batch_size\n",
    "          type: INTEGER\n",
    "          minValue: 8\n",
    "          maxValue: 512\n",
    "          scaleType: UNIT_LOG_SCALE\n",
    "        - parameterName: nembeds\n",
    "          type: INTEGER\n",
    "          minValue: 3\n",
    "          maxValue: 30\n",
    "          scaleType: UNIT_LINEAR_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/babyweight/hyperparam\n",
    "JOBNAME=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "echo ${OUTDIR} ${REGION} ${JOBNAME}\n",
    "gsutil -m rm -rf ${OUTDIR}\n",
    "gcloud ai-platform jobs submit training ${JOBNAME} \\\n",
    "    --staging-bucket=gs://${BUCKET} \\\n",
    "    --region=${REGION} \\\n",
    "    --master-image-uri=${IMAGE} \\\n",
    "    --master-machine-type=n1-standard-4 \\\n",
    "    --scale-tier=CUSTOM \\\n",
    "    --config=hyperparam.yaml \\\n",
    "    -- \\\n",
    "    --train_data_path=gs://${BUCKET}/babyweight/data/train*.csv \\\n",
    "    --eval_data_path=gs://${BUCKET}/babyweight/data/eval*.csv \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --batch_size=32 \\\n",
    "    --num_epochs=10 \\\n",
    "    --train_examples=20000 \\\n",
    "    --eval_steps=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Repeat training </h2>\n",
    "<p>\n",
    "This time with tuned parameters (note last line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/babyweight/trained_model_tuned\n",
    "JOBNAME=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "echo ${OUTDIR} ${REGION} ${JOBNAME}\n",
    "gsutil -m rm -rf ${OUTDIR}\n",
    "gcloud ai-platform jobs submit training ${JOBNAME} \\\n",
    "    --staging-bucket=gs://${BUCKET} \\\n",
    "    --region=${REGION} \\\n",
    "    --master-image-uri=${IMAGE} \\\n",
    "    --master-machine-type=n1-standard-4 \\\n",
    "    --scale-tier=CUSTOM \\\n",
    "    -- \\\n",
    "    --train_data_path=gs://${BUCKET}/babyweight/data/train*.csv \\\n",
    "    --eval_data_path=gs://${BUCKET}/babyweight/data/eval*.csv \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --batch_size=32 \\\n",
    "    --num_epochs=10 \\\n",
    "    --train_examples=20000 \\\n",
    "    --eval_steps=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Summary: \n",
    "In this lab we set up the environment, created the trainer module's task.py to hold hyperparameter argparsing code, created the trainer module's model.py to hold Keras model code, ran the trainer module package locally, submitted a training job to Cloud AI Platform, and submitted a hyperparameter tuning job to Cloud AI Platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
