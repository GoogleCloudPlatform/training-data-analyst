{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3o8Qof7Cy165"
   },
   "source": [
    "# Exporting a BigQuery ML Model for Online Prediction\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "1. Train and deploy a logistic regression model - also applies to DNN classifier, DNN regressor, k-means, linear regression, and matrix factorization models.\n",
    "2. Train and deploy a Boosted Tree classifier model - also applies to Boosted Tree regressor model.\n",
    "3. Train and deploy an AutoML classifier model - also applies to AutoML regressor model.\n",
    "\n",
    "## Introduction \n",
    "In this notebook, you will learn how to [export a BigQuery ML model](https://cloud.google.com/bigquery-ml/docs/exporting-models) and then deploy the model on AI Platform. You will use the iris table from the BigQuery public datasets and work through the three end-to-end scenarios.\n",
    "\n",
    "Each learning objective will correspond to a __#TODO__ in this student lab notebook -- try to complete this notebook first and then review the [solution notebook](../solutions/export_a_bigquery_ml_model.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJ7ByvoXzpVI"
   },
   "source": [
    "## Set up environment variables and load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo chown -R jupyter:jupyter /home/jupyter/training-data-analyst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mC9K9Dpx1ztf"
   },
   "source": [
    "Check that the Google BigQuery library is installed and if not, install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "colab_type": "code",
    "id": "RZUQtASG10xO",
    "outputId": "5612d6b0-9730-476a-a28f-8fdc14f4ecde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-bigquery==1.25.0\n",
      "Downloading google_cloud_bigquery-1.25.0-py2.py3-none-any.whl (169 kB)\n",
      "|████████████████████████████████| 169 kB 4.8 MB/s eta 0:00:01\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==1.25.0) (3.13.0)\n",
      "Requirement already satisfied: six<2.0.0dev,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==1.25.0) (2.1.0)\n",
      "Requirement already satisfied: google-api-core<2.0dev,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==1.25.0) (1.22.1)\n",
      "Collecting google-resumable-media<0.6dev,>=0.5.0\n",
      "Downloading google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==1.25.0) (1.20.1)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==1.25.0) (1.3.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.6.0->google-cloud-bigquery==1.25.0) (49.6.0.post20200814)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0dev,>=2.1.0->google-cloud-bigquery==1.25.0) (2020.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0dev,>=2.1.0->google-cloud-bigquery==1.25.0) (1.51.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0dev,>=2.1.0->google-cloud-bigquery==1.25.0) (2.24.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.9.0->google-cloud-bigquery==1.25.0) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= 3.5 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.9.0->google-cloud-bigquery==1.25.0) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.9.0->google-cloud-bigquery==1.25.0) (0.2.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=2.1.0->google-cloud-bigquery==1.25.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=2.1.0->google-cloud-bigquery==1.25.0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=2.1.0->google-cloud-bigquery==1.25.0) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=2.1.0->google-cloud-bigquery==1.25.0) (2020.6.20)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<5,>=3.1.4; python_version >= 3.5->google-auth<2.0dev,>=1.9.0->google-cloud-bigquery==1.25.0) (0.4.8)\n",
      "Installing collected packages: google-resumable-media, google-cloud-bigquery\n",
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",

      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",

      "google-cloud-storage 1.30.0 requires google-resumable-media<2.0dev,>=0.6.0, but you'll have google-resumable-media 0.5.1 which is incompatible.\n",
      "Successfully installed google-cloud-bigquery-1.25.0 google-resumable-media-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --user google-cloud-bigquery==1.25.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Restart your kernel to use updated packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kindly ignore the deprecation warnings and incompatibility errors related to google-cloud-storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set environment variables.\n",
    "\n",
    "Set environment variables so that we can use them throughout the entire lab. We will be using our project name for our bucket, so you only need to change your project and region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PROJECT=$(gcloud config list project --format \"value(core.project)\")\n",
    "echo \"Your current GCP Project Name is: \"$PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change environment variables\n",
    "PROJECT = \"cloud-training-demos\"  # REPLACE WITH YOUR PROJECT NAME\n",
    "BUCKET = \"BUCKET\"  # REPLACE WITH YOUR BUCKET NAME, DEFAULT BUCKET WILL BE PROJECT ID\n",
    "REGION = \"us-central1\"  # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "\n",
    "# Do not change these\n",
    "os.environ[\"BUCKET\"] = PROJECT if BUCKET == \"BUCKET\" else BUCKET # DEFAULT BUCKET WILL BE PROJECT ID\n",
    "os.environ[\"REGION\"] = REGION\n",
    "\n",
    "if PROJECT == \"cloud-training-demos\":\n",
    "    print(\"Don't forget to update your PROJECT name! Currently:\", PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a BigQuery Dataset and Google Cloud Storage Bucket\n",
    "\n",
    "A BigQuery dataset is a container for tables, views, and models built with BigQuery ML. Let's create one called **bqml_tutorial**. We'll do the same for a GCS bucket for our project too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "## Create a BigQuery dataset bqml_tutorial\n",
    "    \n",
    "    bq --location=US mk --dataset \\\n",
    "        --description \"bqml_tutorial\" \\\n",
    "        $PROJECT:bqml_tutorial\n",
    "    echo \"Here are your current datasets:\"\n",
    "    bq ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and deploy a logistic regression model\n",
    "\n",
    "**Train the model**\n",
    "\n",
    "Train a logistic regression model that predicts iris type using the BigQuery ML [CREATE MODEL](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create#create_model_statement) statement. This training job should take approximately 1 minute to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "bq query --use_legacy_sql=false \\\n",
    "  'CREATE MODEL `bqml_tutorial.iris_model`\n",
    "  OPTIONS (model_type=\"logistic_reg\",\n",
    "      max_iterations=10, input_label_cols=[\"species\"])\n",
    "  AS SELECT\n",
    "    *\n",
    "  FROM\n",
    "    `bigquery-public-data.ml_datasets.iris`;'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export the model**\n",
    "\n",
    "Export the model to a Cloud Storage bucket using the [bq command-line tool](https://cloud.google.com/bigquery/docs/bq-command-line-tool). For additional ways to export models, see [Exporting BigQuery ML models](https://cloud.google.com/bigquery-ml/docs/exporting-models#exporting_models). This extract job should take less than 1 minute to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bq extract -m bqml_tutorial.iris_model gs://$BUCKET/iris_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local deployment and serving**\n",
    "\n",
    "You can deploy exported TensorFlow models using the TensorFlow Serving Docker container. The following steps require you to install [Docker](https://hub.docker.com/search/?type=edition&offering=community).\n",
    "Download the exported model files to a temporary directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir tmp_dir\n",
    "gsutil cp -r gs://$BUCKET/iris_model tmp_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a version subdirectory**\n",
    "\n",
    "This step sets a version number (1 in this case) for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p serving_dir/iris_model/1\n",
    "cp -r tmp_dir/iris_model/* serving_dir/iris_model/1\n",
    "rm -r tmp_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pull the docker image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker pull tensorflow/serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the Docker container**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker run -p 8500:8500 --network=\"host\" --mount type=bind,source=`pwd`/serving_dir/iris_model,target=/models/iris_model -e MODEL_NAME=iris_model -t tensorflow/serving &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -d '{\"instances\": [{\"sepal_length\":5.0, \"sepal_width\":2.0, \"petal_length\":3.5, \"petal_width\":1.0}]}' -X POST http://localhost:8501/v1/models/iris_model:predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Online deployment and serving**\n",
    "\n",
    "This section uses the [gcloud command-line tool](https://cloud.google.com/sdk/gcloud) to deploy and run predictions against the exported model. For more details about deploying a model to AI Platform for online/batch predictions, see [Deploying models](https://cloud.google.com/ai-platform/prediction/docs/deploying-models).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Execute the following commands in the Cloud Shell of Cloud Platform Console till the Run predict command. Click Activate Cloud Shell icon to open the cloud shell and click Continue.**\n",
    "\n",
    "**Create a model resource**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME=\"IRIS_MODEL\"\n",
    "gcloud ai-platform models create $MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a model version**\n",
    "\n",
    "Set the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the BUCKET_NAME with your bucket name.\n",
    "MODEL_DIR=\"gs://<BUCKET_NAME>/iris_model\"\n",
    "VERSION_NAME=\"v1\"\n",
    "FRAMEWORK=\"TENSORFLOW\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud ai-platform versions create $VERSION_NAME --model=$MODEL_NAME --origin=$MODEL_DIR --runtime-version=2.1 --framework=$FRAMEWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step might take a few minutes to complete. You should see the message Creating version (this might take a few minutes).......\n",
    "\n",
    "Get information about your new version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud ai-platform versions describe $VERSION_NAME --model $MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Online prediction**\n",
    "\n",
    "The details about running online predictions against a deployed model are available at [Getting online predictions](https://cloud.google.com/ai-platform/prediction/docs/online-predict#requesting_predictions)\n",
    "Create a newline-delimited JSON file for inputs, for example **instances.json** file with the following content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"sepal_length\":5.0, \"sepal_width\":2.0, \"petal_length\":3.5, \"petal_width\":1.0}\n",
    "{\"sepal_length\":5.3, \"sepal_width\":3.7, \"petal_length\":1.5, \"petal_width\":0.2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup env variables for predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_FILE=\"instances.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud ai-platform predict --model $MODEL_NAME --version $VERSION_NAME --json-instances $INPUT_DATA_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and deploy a Boosted Tree classifier model\n",
    "\n",
    "**Train the model**\n",
    "\n",
    "Train a Boosted Tree classifier model that predicts iris type using the BigQuery ML [CREATE MODEL](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create#create_model_statement) statement. This training job should take approximately 7 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "bq query --use_legacy_sql=false \\\n",
    " 'CREATE MODEL `bqml_tutorial.boosted_tree_iris_model`\n",
    " OPTIONS (model_type=\"boosted_tree_classifier\",\n",
    " max_iterations=10, input_label_cols=[\"species\"])\n",
    " AS SELECT\n",
    " *\n",
    " FROM\n",
    " `bigquery-public-data.ml_datasets.iris`;'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export the model**\n",
    "\n",
    "Export the model to a Cloud Storage bucket using the [bq command-line tool](https://cloud.google.com/bigquery/docs/bq-command-line-tool). For additional ways to export models, see [Exporting BigQuery ML models](https://cloud.google.com/bigquery-ml/docs/exporting-models#exporting_models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bq extract --destination_format ML_XGBOOST_BOOSTER -m bqml_tutorial.boosted_tree_iris_model gs://$BUCKET/boosted_tree_iris_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local deployment and serving**\n",
    "\n",
    "In the exported files, there is a main.py file for local run.\n",
    "\n",
    "**Download the exported model files to a local directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir serving_dir\n",
    "gsutil cp -r gs://$BUCKET/boosted_tree_iris_model serving_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Extract predictor.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tar -xvf serving_dir/boosted_tree_iris_model/xgboost_predictor-0.1.tar.gz -C serving_dir/boosted_tree_iris_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install XGBoost library**\n",
    "Install the [XGBoost library](https://xgboost.readthedocs.io/en/latest/build.html) - version 0.82 or later.\n",
    "Run the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip3 install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd serving_dir/boosted_tree_iris_model/\n",
    "python main.py '[{\"sepal_length\":5.0, \"sepal_width\":2.0, \"petal_length\":3.5, \"petal_width\":1.0}]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Online deployment and serving**\n",
    "\n",
    "This section uses the [gcloud command-line tool](https://cloud.google.com/sdk/gcloud) to deploy and run predictions against the exported model.\n",
    "\n",
    "For more details about deploying a model to AI Platform for online/batch predictions using custom routines, see [Deploying models](https://cloud.google.com/ai-platform/prediction/docs/deploying-models).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Execute the following commands in the Cloud Shell of Cloud Platform Console till the Run predict command.**\n",
    "\n",
    "**Create a model resource**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME=\"BOOSTED_TREE_IRIS_MODEL\"\n",
    "gcloud ai-platform models create $MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a model version**\n",
    "\n",
    "Set the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the BUCKET_NAME with your bucket name.\n",
    "MODEL_DIR=\"gs://<BUCKET_NAME>/boosted_tree_iris_model\"\n",
    "VERSION_NAME=\"v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud beta ai-platform versions create $VERSION_NAME --model=$MODEL_NAME --origin=$MODEL_DIR --package-uris=${MODEL_DIR}/xgboost_predictor-0.1.tar.gz --prediction-class=predictor.Predictor --runtime-version=2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step might take a few minutes to complete. You should see the message Creating version (this might take a few minutes).......\n",
    "\n",
    "Get information about your new version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud ai-platform versions describe $VERSION_NAME --model $MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Online prediction**\n",
    "\n",
    "For more details about running online predictions against a deployed model, see [Requesting predictions](https://cloud.google.com/ai-platform/prediction/docs/online-predict#requesting_predictions).\n",
    "\n",
    "Create a newline-delimited JSON file for inputs. For example, **instances.json** file with the following content. Ignore if already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"sepal_length\":5.0, \"sepal_width\":2.0, \"petal_length\":3.5, \"petal_width\":1.0}\n",
    "{\"sepal_length\":5.3, \"sepal_width\":3.7, \"petal_length\":1.5, \"petal_width\":0.2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup env variables for predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_FILE=\"instances.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud ai-platform predict --model $MODEL_NAME --version $VERSION_NAME --json-instances $INPUT_DATA_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and deploy an AutoML classifier model\n",
    "\n",
    "**Train the model**\n",
    "\n",
    "Train an AutoML classifier model that predicts iris type using the BigQuery ML [CREATE MODEL](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create#create_model_statement) statement. AutoML models need at least 1000 rows of input data. Because ml_datasets.iris only has 150 rows, we duplicate the data 10 times. **This training job should take around 2 hours to complete**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "bq query --use_legacy_sql=false \\\n",
    "  'CREATE MODEL `bqml_tutorial.automl_iris_model`\n",
    "  OPTIONS (model_type=\"automl_classifier\",\n",
    "      budget_hours=1, input_label_cols=[\"species\"])\n",
    "  AS SELECT\n",
    "    * EXCEPT(multiplier)\n",
    "  FROM\n",
    "    `bigquery-public-data.ml_datasets.iris`, unnest(GENERATE_ARRAY(1, 10)) as multiplier;'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export the model**\n",
    "\n",
    "Export the model to a Cloud Storage bucket using the [bq command-line tool](https://cloud.google.com/bigquery/docs/bq-command-line-tool). For additional ways to export models, see [Exporting BigQuery ML models](https://cloud.google.com/bigquery-ml/docs/exporting-models#exporting_models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bq extract -m bqml_tutorial.automl_iris_model gs://$BUCKET/automl_iris_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local deployment and serving**\n",
    "\n",
    "For details about building AutoML containers, see [Exporting models](https://cloud.google.com/automl-tables/docs/model-export). The following steps require you to install [Docker](https://hub.docker.com/search/?type=edition&offering=community).\n",
    "\n",
    "**Copy exported model files to a local directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir automl_serving_dir\n",
    "gsutil cp -r gs://$BUCKET/automl_iris_model/* automl_serving_dir/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pull AutoML Docker image**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker pull gcr.io/cloud-automl-tables-public/model_server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start Docker container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker run -v `pwd`/automl_serving_dir:/models/default/0000001 -p 8080:8080 -it gcr.io/cloud-automl-tables-public/model_server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the prediction**\n",
    "\n",
    "Create a newline-delimited JSON file for inputs. For example, **input.json** file with the following contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"instances\": [{\"sepal_length\":5.0, \"sepal_width\":2.0, \"petal_length\":3.5, \"petal_width\":1.0},\n",
    "{\"sepal_length\":5.3, \"sepal_width\":3.7, \"petal_length\":1.5, \"petal_width\":0.2}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the predict call\n"   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X POST --data @input.json http://localhost:8080/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Online deployment and serving**\n",
    "\n",
    "Online prediction for AutoML regressor and AutoML classifier models is not supported in AI Platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
