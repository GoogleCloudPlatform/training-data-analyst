{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative filtering on Google Analytics data (operationalized)\n",
    "\n",
    "This notebook demonstrates how to implement a WALS matrix refactorization approach to do collaborative filtering. Unlike [wals.ipynb](wals.ipynb), this notebook uses TensorFlow Transform to carry out the preprocessing. This way, these steps are automated:\n",
    "* Mapping visitorId (a string) to userId (an enumeration)\n",
    "* Mapping contentId (a string) to itemId (an enumeration)\n",
    "* Removing already viewed items from the batch prediction output\n",
    "* Replacing the userId and visitorId in batch prediction output by visitorId and contentId"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Beam only works in Python 2 at the moment, so we're going to switch to the Python 2 kernel. In the above menu, click the dropdown arrow and select `python2`. ![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only [specific combinations](https://github.com/tensorflow/transform#compatible-versions) of TensorFlow, Beam and TensorFlow Transform are supported.  After running the following cell, Reset the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source activate py2env\n",
    "pip uninstall -y google-cloud-dataflow\n",
    "conda install -y pytz==2018.4\n",
    "pip install apache-beam[gcp] tensorflow_transform==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apache-beam==2.9.0\n",
      "mesh-tensorflow==0.1.9\n",
      "tensorflow==1.13.1\n",
      "tensorflow-datasets==1.3.2\n",
      "tensorflow-estimator==1.13.0\n",
      "tensorflow-hub==0.7.0\n",
      "tensorflow-metadata==0.14.0\n",
      "tensorflow-probability==0.7.0rc0\n",
      "tensorflow-serving-api==1.13.0\n",
      "tensorflow-transform==0.8.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip freeze | grep -e 'flow\\|beam'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now reset notebook's session kernel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = \"cloud-training-demos\" # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = \"cloud-training-demos-ml\" # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = \"us-central1\" # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "\n",
    "# Do not change these\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = \"1.13\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset for WALS using TF Transform\n",
    "<p>\n",
    "For collaborative filtering, we don't need to know anything about either the users or the content. Essentially, all we need to know is userId, itemId, and rating that the particular user gave the particular item.\n",
    "<p>\n",
    "In this case, we are working with newspaper articles. The company doesn't ask their users to rate the articles. However, we can use the time-spent on the page as a proxy for rating.\n",
    "<p>\n",
    "Normally, we would also add a time filter to this (\"latest 7 days\"), but our dataset is itself limited to a few days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "bq = bigquery.Client(project = PROJECT)\n",
    "\n",
    "sql = \"\"\"\n",
    "WITH CTE_visitor_page_content AS (\n",
    "    SELECT\n",
    "        # Schema: https://support.google.com/analytics/answer/3437719?hl=en\n",
    "        # For a completely unique visit-session ID, we combine combination of fullVisitorId and visitNumber:\n",
    "        CONCAT(fullVisitorID,'-',CAST(visitNumber AS STRING)) AS visitorId,\n",
    "        (SELECT MAX(IF(index=10, value, NULL)) FROM UNNEST(hits.customDimensions)) AS latestContentId,  \n",
    "        (LEAD(hits.time, 1) OVER (PARTITION BY fullVisitorId ORDER BY hits.time ASC) - hits.time) AS session_duration \n",
    "    FROM\n",
    "        `cloud-training-demos.GA360_test.ga_sessions_sample`,   \n",
    "        UNNEST(hits) AS hits\n",
    "    WHERE \n",
    "        # only include hits on pages\n",
    "        hits.type = \"PAGE\"\n",
    "GROUP BY   \n",
    "        fullVisitorId,\n",
    "        visitNumber,\n",
    "        latestContentId,\n",
    "        hits.time )\n",
    "-- Aggregate web stats\n",
    "SELECT   \n",
    "    visitorId,\n",
    "    latestContentId as contentId,\n",
    "    SUM(session_duration) AS session_duration\n",
    "FROM\n",
    "    CTE_visitor_page_content\n",
    "WHERE\n",
    "    latestContentId IS NOT NULL \n",
    "GROUP BY\n",
    "    visitorId, \n",
    "    latestContentId\n",
    "HAVING \n",
    "    session_duration > 0\n",
    "\"\"\"\n",
    "\n",
    "df = bq.query(sql).to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "bq = bigquery.Client(project = PROJECT)\n",
    "df = bq.query(query + \" LIMIT 100\").to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%writefile requirements.txt\n",
    "tensorflow-transform==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "import apache_beam as beam\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow_transform.beam import impl as beam_impl\n",
    "\n",
    "def preprocess_tft(rowdict):\n",
    "    median = 57937 #tft.quantiles(rowdict[\"session_duration\"], 11, epsilon=0.001)[5]\n",
    "    result = {\n",
    "      \"userId\" : tft.string_to_int(rowdict[\"visitorId\"], vocab_filename=\"vocab_users\"),\n",
    "      \"itemId\" : tft.string_to_int(rowdict[\"contentId\"], vocab_filename=\"vocab_items\"),\n",
    "      \"rating\" : 0.3 * rowdict[\"session_duration\"] / median\n",
    "    }\n",
    "    # cap the rating at 1.0\n",
    "    result[\"rating\"] = tf.where(condition = tf.less(x = result[\"rating\"], y = tf.ones(shape = tf.shape(input = result[\"rating\"]))),\n",
    "                                x = result[\"rating\"], \n",
    "                                y = tf.ones(shape = tf.shape(input = result[\"rating\"])))\n",
    "    return result\n",
    "  \n",
    "def preprocess(query, in_test_mode):\n",
    "    import os\n",
    "    import os.path\n",
    "    import tempfile\n",
    "    import tensorflow as tf\n",
    "    from apache_beam.io import tfrecordio\n",
    "    from tensorflow_transform.coders import example_proto_coder\n",
    "    from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "    from tensorflow_transform.tf_metadata import dataset_schema\n",
    "    from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "\n",
    "    def write_count(a, outdir, basename):\n",
    "        filename = os.path.join(outdir, basename)\n",
    "        (a \n",
    "         | \"{}_1\".format(basename) >> beam.Map(lambda x: (1, 1)) \n",
    "         | \"{}_2\".format(basename) >> beam.combiners.Count.PerKey()\n",
    "         | \"{}_3\".format(basename) >> beam.Map(lambda (k, v): v)\n",
    "         | \"{}_write\".format(basename) >> beam.io.WriteToText(file_path_prefix=filename, num_shards=1))\n",
    "\n",
    "    def to_tfrecord(key_vlist, indexCol):\n",
    "        (key, vlist) = key_vlist\n",
    "        return {\n",
    "            \"key\": [key],\n",
    "            \"indices\": [value[indexCol] for value in vlist],\n",
    "            \"values\":  [value[\"rating\"] for value in vlist]\n",
    "        }\n",
    "  \n",
    "    job_name = \"preprocess-wals-features\" + \"-\" + datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")    \n",
    "    if in_test_mode:\n",
    "        import shutil\n",
    "        print \"Launching local job ... hang on\"\n",
    "        OUTPUT_DIR = \"./preproc_tft\"\n",
    "        shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "    else:\n",
    "        print \"Launching Dataflow job {} ... hang on\".format(job_name)\n",
    "        OUTPUT_DIR = \"gs://{0}/wals/preproc_tft/\".format(BUCKET)\n",
    "        import subprocess\n",
    "        subprocess.call(\"gcloud storage rm --recursive {}\".format(OUTPUT_DIR).split())\n",    "\n",
    "    options = {\n",
    "    \"staging_location\": os.path.join(OUTPUT_DIR, \"tmp\", \"staging\"),\n",
    "    \"temp_location\": os.path.join(OUTPUT_DIR, \"tmp\"),\n",
    "    \"job_name\": job_name,\n",
    "    \"project\": PROJECT,\n",
    "    \"max_num_workers\": 16,\n",
    "    \"teardown_policy\": \"TEARDOWN_ALWAYS\",\n",
    "    \"save_main_session\": False,\n",
    "    \"requirements_file\": \"requirements.txt\"\n",
    "    }\n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "    if in_test_mode:\n",
    "        RUNNER = \"DirectRunner\"\n",
    "    else:\n",
    "        RUNNER = \"DataflowRunner\"\n",
    "\n",
    "  # Set up metadata  \n",
    "    raw_data_schema = {\n",
    "        colname : dataset_schema.ColumnSchema(tf.string, [], dataset_schema.FixedColumnRepresentation()) \n",
    "            for colname in \"visitorId,contentId\".split(\",\")\n",
    "    }\n",
    "    raw_data_schema.update({\n",
    "        colname : dataset_schema.ColumnSchema(tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "            for colname in \"session_duration\".split(\",\")\n",
    "    })\n",
    "    raw_data_metadata = dataset_metadata.DatasetMetadata(dataset_schema.Schema(raw_data_schema))\n",
    " \n",
    "  # Run Beam  \n",
    "    with beam.Pipeline(RUNNER, options=opts) as p:\n",
    "        with beam_impl.Context(temp_dir=os.path.join(OUTPUT_DIR, \"tmp\")):\n",
    "            # read raw data\n",
    "            selquery = query\n",
    "            if in_test_mode:\n",
    "                 selquery = selquery + \" LIMIT 100\"\n",
    "            raw_data = (p \n",
    "                        | \"read\" >> beam.io.Read(beam.io.BigQuerySource(query=selquery, use_standard_sql=True)))\n",
    "    \n",
    "            # analyze and transform\n",
    "            raw_dataset = (raw_data, raw_data_metadata)\n",
    "            transformed_dataset, transform_fn = (\n",
    "                    raw_dataset | beam_impl.AnalyzeAndTransformDataset(preprocess_tft))         \n",
    "            transformed_data, transformed_metadata = transformed_dataset\n",
    "            _ = (transform_fn\n",
    "                 | \"WriteTransformFn\" >>\n",
    "                 transform_fn_io.WriteTransformFn(os.path.join(OUTPUT_DIR, \"transform_fn\")))\n",
    "            \n",
    "            # do a group-by to create users_for_item and items_for_user\n",
    "            users_for_item = (transformed_data \n",
    "                              | \"map_items\" >> beam.Map(lambda x : (x[\"itemId\"], x))\n",
    "                              | \"group_items\" >> beam.GroupByKey()\n",
    "                              | \"totfr_items\" >> beam.Map(lambda item_userlist : to_tfrecord(item_userlist, \"userId\")))\n",
    "            items_for_user = (transformed_data\n",
    "                              | \"map_users\" >> beam.Map(lambda x : (x[\"userId\"], x))\n",
    "                              | \"group_users\" >> beam.GroupByKey()\n",
    "                              | \"totfr_users\" >> beam.Map(lambda item_userlist : to_tfrecord(item_userlist, \"itemId\")))\n",
    "            \n",
    "            output_schema = {\n",
    "                \"key\" : dataset_schema.ColumnSchema(tf.int64, [1], dataset_schema.FixedColumnRepresentation()),\n",
    "                \"indices\": dataset_schema.ColumnSchema(tf.int64, [], dataset_schema.ListColumnRepresentation()),\n",
    "                \"values\": dataset_schema.ColumnSchema(tf.float32, [], dataset_schema.ListColumnRepresentation())\n",
    "            }\n",
    "\n",
    "            _ = users_for_item | \"users_for_item\" >> tfrecordio.WriteToTFRecord(\n",
    "                    os.path.join(OUTPUT_DIR, \"users_for_item\"),\n",
    "                    coder = example_proto_coder.ExampleProtoCoder(\n",
    "                            dataset_schema.Schema(output_schema)))\n",
    "            _ = items_for_user | \"items_for_user\" >> tfrecordio.WriteToTFRecord(\n",
    "                    os.path.join(OUTPUT_DIR, \"items_for_user\"),\n",
    "                    coder = example_proto_coder.ExampleProtoCoder(\n",
    "                            dataset_schema.Schema(output_schema)))\n",
    "            \n",
    "            write_count(users_for_item, OUTPUT_DIR, \"nitems\")\n",
    "            write_count(items_for_user, OUTPUT_DIR, \"nusers\") \n",
    "     \n",
    "preprocess(query, in_test_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud storage ls gs://${BUCKET}/wals/preproc_tft/"   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud storage ls gs://${BUCKET}/wals/preproc_tft/transform_fn/transform_fn/\n",    "gcloud storage ls gs://${BUCKET}/wals/preproc_tft/transform_fn/transform_fn/assets/"   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, we created the following data files from the BiqQuery resultset:\n",
    "<ol>\n",
    "<li> ```users_for_item``` contains all the users/ratings for each item in TFExample format.  The items and users here are integers (not strings) i.e. itemId not contentId and userId not visitorId. The rating is scaled.\n",
    "<li> ```items_for_user``` contains all the items/ratings for each user in TFExample format.  The items and users here are integers (not strings) i.e. itemId not contentId and userId not visitorId. The rating is scaled.\n",
    "<li> ```vocab_items``` contains the mapping from the contentId to the enumerated itemId\n",
    "<li> ```vocab_items``` contains the mapping from the visitorId to the enumerated userId\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with WALS\n",
    "\n",
    "Once you have the dataset, do matrix factorization with WALS using the [WALSMatrixFactorization](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/factorization/WALSMatrixFactorization) in the contrib directory.\n",
    "This is an estimator model, so it should be relatively familiar.\n",
    "<p>\n",
    "As usual, we write an input_fn to provide the data to the model, and then create the Estimator to do train_and_evaluate.\n",
    "Because it is in contrib and hasn't moved over to tf.estimator yet, we use tf.contrib.learn.Experiment to handle the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tensorflow.contrib.factorization import WALSMatrixFactorization\n",
    "\n",
    "def read_dataset(mode, args):\n",
    "    def decode_example(protos, vocab_size):\n",
    "        features = {\n",
    "            \"key\": tf.FixedLenFeature(shape = [1], dtype = tf.int64),\n",
    "            \"indices\": tf.VarLenFeature(dtype = tf.int64),\n",
    "            \"values\": tf.VarLenFeature(dtype = tf.float32)}\n",
    "        parsed_features = tf.parse_single_example(serialized = protos, features = features)\n",
    "        values = tf.sparse_merge(sp_ids = parsed_features[\"indices\"], sp_values = parsed_features[\"values\"], vocab_size = vocab_size)\n",
    "        # Save key to remap after batching\n",
    "        # This is a temporary workaround to assign correct row numbers in each batch.\n",
    "        # You can ignore details of this part and remap_keys().\n",
    "        key = parsed_features[\"key\"]\n",
    "        decoded_sparse_tensor = tf.SparseTensor(indices = tf.concat(values = [values.indices, [key]], axis = 0), \n",
    "                                                values = tf.concat(values = [values.values, [0.0]], axis = 0), \n",
    "                                                dense_shape = values.dense_shape)\n",
    "        return decoded_sparse_tensor\n",
    "  \n",
    "  \n",
    "    def remap_keys(sparse_tensor):\n",
    "        # Current indices of our SparseTensor that we need to fix\n",
    "        bad_indices = sparse_tensor.indices # shape = (current_batch_size * (number_of_items/users[i] + 1), 2)\n",
    "        # Current values of our SparseTensor that we need to fix\n",
    "        bad_values = sparse_tensor.values # shape = (current_batch_size * (number_of_items/users[i] + 1),)\n",
    "\n",
    "        # Since batch is ordered, the last value for a batch index is the user\n",
    "        # Find where the batch index chages to extract the user rows\n",
    "        # 1 where user, else 0\n",
    "        user_mask = tf.concat(values = [bad_indices[1:,0] - bad_indices[:-1,0], tf.constant(value = [1], dtype = tf.int64)], axis = 0) # shape = (current_batch_size * (number_of_items/users[i] + 1), 2)\n",
    "\n",
    "        # Mask out the user rows from the values\n",
    "        good_values = tf.boolean_mask(tensor = bad_values, mask = tf.equal(x = user_mask, y = 0)) # shape = (current_batch_size * number_of_items/users[i],)\n",
    "        item_indices = tf.boolean_mask(tensor = bad_indices, mask = tf.equal(x = user_mask, y = 0)) # shape = (current_batch_size * number_of_items/users[i],)\n",
    "        user_indices = tf.boolean_mask(tensor = bad_indices, mask = tf.equal(x = user_mask, y = 1))[:, 1] # shape = (current_batch_size,)\n",
    "\n",
    "        good_user_indices = tf.gather(params = user_indices, indices = item_indices[:,0]) # shape = (current_batch_size * number_of_items/users[i],)\n",
    "\n",
    "        # User and item indices are rank 1, need to make rank 1 to concat\n",
    "        good_user_indices_expanded = tf.expand_dims(input = good_user_indices, axis = -1) # shape = (current_batch_size * number_of_items/users[i], 1)\n",
    "        good_item_indices_expanded = tf.expand_dims(input = item_indices[:, 1], axis = -1) # shape = (current_batch_size * number_of_items/users[i], 1)\n",
    "        good_indices = tf.concat(values = [good_user_indices_expanded, good_item_indices_expanded], axis = 1) # shape = (current_batch_size * number_of_items/users[i], 2)\n",
    "\n",
    "        remapped_sparse_tensor = tf.SparseTensor(indices = good_indices, values = good_values, dense_shape = sparse_tensor.dense_shape)\n",
    "        return remapped_sparse_tensor\n",
    "\n",
    "    \n",
    "    def parse_tfrecords(filename, vocab_size):\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # indefinitely\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "\n",
    "        files = tf.gfile.Glob(filename = os.path.join(args[\"input_path\"], filename))\n",
    "\n",
    "        # Create dataset from file list\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        dataset = dataset.map(map_func = lambda x: decode_example(x, vocab_size))\n",
    "        dataset = dataset.repeat(count = num_epochs)\n",
    "        dataset = dataset.batch(batch_size = args[\"batch_size\"])\n",
    "        dataset = dataset.map(map_func = lambda x: remap_keys(x))\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "  \n",
    "    def _input_fn():\n",
    "        features = {\n",
    "            WALSMatrixFactorization.INPUT_ROWS: parse_tfrecords('items_for_user-*-of-*', args['nitems']),\n",
    "            WALSMatrixFactorization.INPUT_COLS: parse_tfrecords('users_for_item-*-of-*', args['nusers']),\n",
    "            WALSMatrixFactorization.PROJECT_ROW: tf.constant(True)\n",
    "        }\n",
    "        return features, None\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_k(user, item_factors, k):\n",
    "    all_items = tf.matmul(a = tf.expand_dims(input = user, axis = 0), b = tf.transpose(a = item_factors))\n",
    "    topk = tf.nn.top_k(input = all_items, k = k)\n",
    "    return tf.cast(x = topk.indices, dtype = tf.int64)\n",
    "    \n",
    "def batch_predict(args):\n",
    "    import numpy as np\n",
    "  \n",
    "    # Read vocabulary into Python list for quick index-ed lookup\n",
    "    def create_lookup(filename):\n",
    "        from tensorflow.python.lib.io import file_io\n",
    "        dirname = os.path.join(args[\"input_path\"], \"transform_fn/transform_fn/assets/\")\n",
    "        with file_io.FileIO(os.path.join(dirname, filename), mode = \"r\") as ifp:\n",
    "            return [x.rstrip() for x in ifp]\n",
    "    originalItemIds = create_lookup(\"vocab_items\")\n",
    "    originalUserIds = create_lookup(\"vocab_users\")\n",
    "  \n",
    "  with tf.Session() as sess:\n",
    "    estimator = tf.contrib.factorization.WALSMatrixFactorization(\n",
    "        num_rows = args[\"nusers\"], \n",
    "        num_cols = args[\"nitems\"],\n",
    "        embedding_dimension = args[\"n_embeds\"],\n",
    "        model_dir = args[\"output_dir\"])\n",
    "           \n",
    "    # But for in-vocab data, the row factors are already in the checkpoint\n",
    "    user_factors = tf.convert_to_tensor(value = estimator.get_row_factors()[0]) # (nusers, nembeds)\n",
    "    # In either case, we have to assume catalog doesn\"t change, so col_factors are read in\n",
    "    item_factors = tf.convert_to_tensor(value = estimator.get_col_factors()[0])# (nitems, nembeds)\n",
    "\n",
    "    # For each user, find the top K items\n",
    "    topk = tf.squeeze(input = tf.map_fn(fn = lambda user: find_top_k(user, item_factors, args[\"topk\"]), elems = user_factors, dtype = tf.int64))\n",
    "\n",
    "    with file_io.FileIO(os.path.join(args[\"output_dir\"], \"batch_pred.txt\"), mode = 'w') as f:\n",
    "        for userId, best_items_for_user in enumerate(topk.eval()):\n",
    "            f.write(originalUserIds[userId] + \"\\t\") # write userId \\t item1,item2,item3...\n",
    "            f.write(','.join(originalItemIds[itemId] for itemId in best_items_for_user) + '\\n')\n",
    "\n",
    "def train_and_evaluate(args):\n",
    "    train_steps = int(0.5 + (1.0 * args[\"num_epochs\"] * args[\"nusers\"]) / args[\"batch_size\"])\n",
    "    steps_in_epoch = int(0.5 + args[\"nusers\"] / args[\"batch_size\"])\n",
    "    print(\"Will train for {} steps, evaluating once every {} steps\".format(train_steps, steps_in_epoch))\n",
    "    def experiment_fn(output_dir):\n",
    "        return tf.contrib.learn.Experiment(\n",
    "            tf.contrib.factorization.WALSMatrixFactorization(\n",
    "                num_rows = args[\"nusers\"], \n",
    "                num_cols = args[\"nitems\"],\n",
    "                embedding_dimension = args[\"n_embeds\"],\n",
    "                model_dir = args[\"output_dir\"]),\n",
    "            train_input_fn = read_dataset(tf.estimator.ModeKeys.TRAIN, args),\n",
    "            eval_input_fn = read_dataset(tf.estimator.ModeKeys.EVAL, args),\n",
    "            train_steps = train_steps,\n",
    "            eval_steps = 1,\n",
    "            min_eval_frequency = steps_in_epoch\n",
    "        )\n",
    "\n",
    "    from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "    learn_runner.run(experiment_fn = experiment_fn, output_dir = args[\"output_dir\"])\n",
    "    \n",
    "    batch_predict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(path = \"wals_trained\", ignore_errors=True)\n",
    "train_and_evaluate({\n",
    "    \"output_dir\": \"wals_trained\",\n",
    "    \"input_path\": \"gs://{}/wals/preproc_tft\".format(BUCKET),\n",
    "    \"num_epochs\": 0.05,\n",
    "    \"nitems\": 5668,\n",
    "    \"nusers\": 82802,\n",
    "\n",
    "    \"batch_size\": 512,\n",
    "    \"n_embeds\": 10,\n",
    "    \"topk\": 3\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls wals_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head wals_trained/batch_pred.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run as a Python module\n",
    "\n",
    "Let's run it as Python module for just a few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "NITEMS=$(gcloud storage cat gs://${BUCKET}/wals/preproc_tft/nitems-00000-of-00001)\n",    "NUSERS=$(gcloud storage cat gs://${BUCKET}/wals/preproc_tft/nusers-00000-of-00001)\n",    "\n",
    "rm -rf wals_trained\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/wals_tft\n",
    "python -m trainer.task \\\n",
    "    --output_dir=${PWD}/wals_trained \\\n",
    "    --input_path=gs://${BUCKET}/wals/preproc_tft \\\n",
    "    --num_epochs=0.01 --nitems=$NITEMS --nusers=$NUSERS \\\n",
    "    --job-dir=./tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/wals_tft/model_trained\n",
    "JOBNAME=wals_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gcloud storage rm --recursive --continue-on-error $OUTDIR\n",    "\n",
    "NITEMS=$(gcloud storage cat gs://${BUCKET}/wals/preproc_tft/nitems-00000-of-00001)\n",    "NUSERS=$(gcloud storage cat gs://${BUCKET}/wals/preproc_tft/nusers-00000-of-00001)\n",    "\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=${PWD}/wals_tft/trainer \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --scale-tier=BASIC_GPU \\\n",
    "    --runtime-version=1.6 \\\n",
    "    -- \\\n",
    "    --output_dir=$OUTDIR \\\n",
    "    --input_path=gs://${BUCKET}/wals/preproc_tft \\\n",
    "    --num_epochs=10 --nitems=$NITEMS --nusers=$NUSERS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took <b>10 minutes</b> for me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get row and column factors\n",
    "\n",
    "Once you have a trained WALS model, you can get row and column factors (user and item embeddings) from the checkpoint file. We'll look at how to use these in the section on building a recommendation system using deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_factors(args):\n",
    "    with tf.Session() as sess:\n",
    "        estimator = tf.contrib.factorization.WALSMatrixFactorization(\n",
    "            num_rows = args[\"nusers\"], \n",
    "            num_cols = args[\"nitems\"],\n",
    "            embedding_dimension = args[\"n_embeds\"],\n",
    "            model_dir = args[\"output_dir\"])\n",
    "        row_factors = estimator.get_row_factors()[0]\n",
    "        col_factors = estimator.get_col_factors()[0]\n",
    "    return row_factors, col_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"output_dir\": \"gs://{}/wals_tft/model_trained\".format(BUCKET),\n",
    "    \"nitems\": 5668,\n",
    "    \"nusers\": 82802,\n",
    "    \"n_embeds\": 10\n",
    "  }\n",
    "\n",
    "user_embeddings, item_embeddings = get_factors(args)\n",
    "print user_embeddings[:3]\n",
    "print item_embeddings[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the embedding vectors using dimensional reduction techniques such as PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 3)\n",
    "pca.fit(user_embeddings)\n",
    "user_embeddings_pca = pca.transform(user_embeddings)\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(111, projection = \"3d\")\n",
    "xs, ys, zs = user_embeddings_pca[::150].T\n",
    "ax.scatter(xs, ys, zs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "# Copyright 2018 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
