{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations on GCP with TensorFlow and WALS with Cloud Composer\n",
    "***\n",
    "This lab is adapted from the original [solution](https://github.com/GoogleCloudPlatform/tensorflow-recommendation-wals) created by [lukmanr](https://github.com/GoogleCloudPlatform/tensorflow-recommendation-wals/commits?author=lukmanr) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project deploys a solution for a recommendation service on GCP, using the WALS algorithm in TensorFlow. Components include:\n",
    "\n",
    "- Recommendation model code, and scripts to train and tune the model on ML Engine\n",
    "- A REST endpoint using Google Cloud Endpoints for serving recommendations\n",
    "- An Airflow server managed by Cloud Composer for running scheduled model training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Cloud Composer Instance\n",
    "- Create a Cloud Composer [instance](https://console.cloud.google.com/composer/environments/create?project=)\n",
    "    1. Specify 'composer' for name\n",
    "    2. Choose a location\n",
    "    3. Keep the remaining settings at their defaults\n",
    "    4. Select Create\n",
    "\n",
    "This takes 15 - 20 minutes. Continue with the rest of the lab as you will be using Cloud Composer near the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /usr/local/envs/py2env\n",
      "\n",
      "  added / updated specs: \n",
      "    - pytz==2018.4\n",
      "\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "    pytz: 2018.7-py27_0 defaults --> 2018.4-py27_0 defaults\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /usr/local/envs/py2env\n",
      "\n",
      "  added / updated specs: \n",
      "    - pyopenssl\n",
      "\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    cryptography: 0.5.4-py27_0  defaults --> 2.3.1-py27h1ba5d50_2 defaults\n",
      "    pyopenssl:    0.15.1-py27_2 defaults --> 18.0.0-py27_0        defaults\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Collecting sh\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/22/17b22ef5b049f12080f5815c41bf94de3c229217609e469001a8f80c1b3d/sh-1.12.14-py2.py3-none-any.whl\n",
      "Collecting pip\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/d7/90f34cb0d83a6c5631cf71dfe64cc1054598c843a92b400e55675cc2ac37/pip-18.1-py2.py3-none-any.whl (1.3MB)\n",
      "Installing collected packages: sh, pip\n",
      "  Found existing installation: pip 18.0\n",
      "    Uninstalling pip-18.0:\n",
      "      Successfully uninstalled pip-18.0\n",
      "Successfully installed pip-18.1 sh-1.12.14\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "source activate py2env\n",
    "conda install -y pytz==2018.4\n",
    "conda update -y pyopenssl\n",
    "pip install sh --upgrade pip # needed to execute shell scripts later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart the kernel here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup environment variables\n",
    "<span style=\"color: blue\">__Replace the below settings with your own.__</span> Note: you can leave AIRFLOW_BUCKET blank and come back to it after your Composer instance is created which automatically will create an Airflow bucket for you. \n",
    "\n",
    "### 1. Make a GCS bucket with the name recserve_[YOUR-PROJECT-ID]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "BUCKET = 'alexhanna-dev-ml' # REPLACE WITH A BUCKET NAME\n",
    "PROJECT = 'alexhanna-dev' # REPLACE WITH YOUR PROJECT ID\n",
    "REGION = 'us-central1' # REPLACE WITH YOUR REGION e.g. us-central1\n",
    "\n",
    "# do not change these\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = 'recserve_' + BUCKET\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not creating recserve_bucket since it already exists.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# create GCS bucket with recserve_PROJECT_NAME if not exists\n",
    "exists=$(gsutil ls -d | grep -w gs://${BUCKET}/)\n",
    "if [ -n \"$exists\" ]; then\n",
    "   echo \"Not creating recserve_bucket since it already exists.\"\n",
    "else\n",
    "   echo \"Creating recserve_ bucket\"\n",
    "   gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Google App Engine permissions\n",
    "1. In [IAM](https://console.cloud.google.com/iam-admin/iam?project=), change permissions for \"Compute Engine default service account\" from Editor to Owner. This is required so you can create and deploy App Engine versions from within Cloud Datalab. Note: the alternative is to run all app engine commands directly in Cloud Shell instead of from within Cloud Datalab.<br/><br/>\n",
    "\n",
    "2. Create an App Engine instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are creating an app for project [alexhanna-dev].\n",
      "WARNING: Creating an App Engine application for a project is irreversible and the region\n",
      "cannot be changed. More information about regions is at\n",
      "<https://cloud.google.com/appengine/docs/locations>.\n",
      "\n",
      "ERROR: (gcloud.app.create) INVALID_ARGUMENT: Unrecognized value \"us-central1\".\n",
      "- '@type': type.googleapis.com/google.rpc.BadRequest\n",
      "  fieldViolations:\n",
      "  - description: Unrecognized value \"us-central1\".\n",
      "    field: location\n",
      "Updating the app [alexhanna-dev]...\n",
      "Waiting for operation [apps/alexhanna-dev/operations/6faa942e-4a12-42f1-8025-bce85961cdf2] to complete...\n",
      "...............done.\n",
      ".done.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# run app engine creation commands\n",
    "gcloud app create --region ${REGION} # see: https://cloud.google.com/compute/docs/regions-zones/\n",
    "gcloud app update --no-split-health-checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part One: Setup and Train the WALS Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload sample data to BigQuery \n",
    "This tutorial comes with a sample Google Analytics data set, containing page tracking events from the Austrian news site Kurier.at. The schema file '''ga_sessions_sample_schema.json''' is located in the folder data in the tutorial code, and the data file '''ga_sessions_sample.json.gz''' is located in a public Cloud Storage bucket associated with this tutorial. To upload this data set to BigQuery:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy sample data files into our bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/endtoend/data/ga_sessions_sample.json.gz [Content-Type=application/json]...\n",
      "/ [0/1 files][    0.0 B/121.3 MiB]   0% Done                                    \r",
      "/ [0/1 files][121.3 MiB/121.3 MiB]  99% Done                                    \r",
      "-\r",
      "- [1/1 files][121.3 MiB/121.3 MiB] 100% Done     0.0 B/s                        \r\n",
      "Operation completed over 1 objects/121.3 MiB.                                    \n",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/endtoend/data/recommendation_events.csv...\n",
      "/ [0/1 files][    0.0 B/ 10.0 MiB]   0% Done                                    \r",
      "/ [1/1 files][ 10.0 MiB/ 10.0 MiB] 100% Done                                    \r\n",
      "Operation completed over 1 objects/10.0 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil -m cp gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/endtoend/data/ga_sessions_sample.json.gz gs://${BUCKET}/data/ga_sessions_sample.json.gz\n",
    "gsutil -m cp gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/endtoend/data/recommendation_events.csv data/recommendation_events.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create empty BigQuery dataset and load sample JSON data\n",
    "Note: Ingesting the 400K rows of sample data. This usually takes 5-7 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating GA360_test dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/account].\n",
      "ERROR: (bq) Your current active account [alexhanna@google.com] does not have any valid credentials\n",
      "Please run:\n",
      "\n",
      "  $ gcloud auth login\n",
      "\n",
      "to obtain new credentials, or if you have already logged in with a\n",
      "different account:\n",
      "\n",
      "  $ gcloud config set account ACCOUNT\n",
      "\n",
      "to select an already authenticated account to use.\n",
      "ERROR: (bq) Your current active account [alexhanna@google.com] does not have any valid credentials\n",
      "Please run:\n",
      "\n",
      "  $ gcloud auth login\n",
      "\n",
      "to obtain new credentials, or if you have already logged in with a\n",
      "different account:\n",
      "\n",
      "  $ gcloud config set account ACCOUNT\n",
      "\n",
      "to select an already authenticated account to use.\n",
      "ERROR: (bq) Your current active account [alexhanna@google.com] does not have any valid credentials\n",
      "Please run:\n",
      "\n",
      "  $ gcloud auth login\n",
      "\n",
      "to obtain new credentials, or if you have already logged in with a\n",
      "different account:\n",
      "\n",
      "  $ gcloud config set account ACCOUNT\n",
      "\n",
      "to select an already authenticated account to use.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud config set account alexhanna@google.com\n",
    "\n",
    "# create BigQuery dataset if it doesn't already exist\n",
    "exists=$(bq ls -d | grep -w GA360_test)\n",
    "if [ -n \"$exists\" ]; then\n",
    "   echo \"Not creating GA360_test since it already exists.\"\n",
    "else\n",
    "   echo \"Creating GA360_test dataset.\"\n",
    "   bq --project_id=${PROJECT} mk GA360_test \n",
    "fi\n",
    "\n",
    "# create the schema and load our sample Google Analytics session data\n",
    "bq load --source_format=NEWLINE_DELIMITED_JSON \\\n",
    " GA360_test.ga_sessions_sample \\\n",
    " gs://${BUCKET}/data/ga_sessions_sample.json.gz \\\n",
    " data/ga_sessions_sample_schema.json # can't load schema files from GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install WALS model training package and model data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a distributable package. Copy the package up to the code folder in the bucket you created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating distributable package\n",
      "running sdist\n",
      "running egg_info\n",
      "creating wals_ml_engine.egg-info\n",
      "writing requirements to wals_ml_engine.egg-info/requires.txt\n",
      "writing wals_ml_engine.egg-info/PKG-INFO\n",
      "writing top-level names to wals_ml_engine.egg-info/top_level.txt\n",
      "writing dependency_links to wals_ml_engine.egg-info/dependency_links.txt\n",
      "writing manifest file 'wals_ml_engine.egg-info/SOURCES.txt'\n",
      "reading manifest file 'wals_ml_engine.egg-info/SOURCES.txt'\n",
      "writing manifest file 'wals_ml_engine.egg-info/SOURCES.txt'\n",
      "running check\n",
      "creating wals_ml_engine-0.1\n",
      "creating wals_ml_engine-0.1/trainer\n",
      "creating wals_ml_engine-0.1/wals_ml_engine.egg-info\n",
      "copying files to wals_ml_engine-0.1...\n",
      "copying README.md -> wals_ml_engine-0.1\n",
      "copying setup.py -> wals_ml_engine-0.1\n",
      "copying trainer/__init__.py -> wals_ml_engine-0.1/trainer\n",
      "copying trainer/model.py -> wals_ml_engine-0.1/trainer\n",
      "copying trainer/task.py -> wals_ml_engine-0.1/trainer\n",
      "copying trainer/util.py -> wals_ml_engine-0.1/trainer\n",
      "copying trainer/wals.py -> wals_ml_engine-0.1/trainer\n",
      "copying wals_ml_engine.egg-info/PKG-INFO -> wals_ml_engine-0.1/wals_ml_engine.egg-info\n",
      "copying wals_ml_engine.egg-info/SOURCES.txt -> wals_ml_engine-0.1/wals_ml_engine.egg-info\n",
      "copying wals_ml_engine.egg-info/dependency_links.txt -> wals_ml_engine-0.1/wals_ml_engine.egg-info\n",
      "copying wals_ml_engine.egg-info/requires.txt -> wals_ml_engine-0.1/wals_ml_engine.egg-info\n",
      "copying wals_ml_engine.egg-info/top_level.txt -> wals_ml_engine-0.1/wals_ml_engine.egg-info\n",
      "Writing wals_ml_engine-0.1/setup.cfg\n",
      "creating dist\n",
      "Creating tar archive\n",
      "removing 'wals_ml_engine-0.1' (and everything under it)\n",
      "copying ML package to bucket\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "Copying file://dist/wals_ml_engine-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [0 files][    0.0 B/  8.3 KiB]                                                \r",
      "/ [1 files][  8.3 KiB/  8.3 KiB]                                                \r\n",
      "Operation completed over 1 objects/8.3 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd wals_ml_engine\n",
    "\n",
    "echo \"creating distributable package\"\n",
    "python setup.py sdist\n",
    "\n",
    "echo \"copying ML package to bucket\"\n",
    "gsutil cp dist/wals_ml_engine-0.1.tar.gz gs://${BUCKET}/code/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run the WALS model on the sample data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2017 Google Inc. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "\n",
      "usage () {\n",
      "  echo \"usage: mltrain.sh [local | train | tune] [gs://]<input_file>.csv\n",
      "                  [--data-type ratings|web_views]\n",
      "                  [--delimiter <delim>]\n",
      "                  [--use-optimized]\n",
      "                  [--headers]\n",
      "\n",
      "Use 'local' to train locally with a local data file, and 'train' and 'tune' to\n",
      "run on ML Engine.  For ML Engine jobs the input file must reside on GCS.\n",
      "\n",
      "Optional args:\n",
      "  --data-type:      Default to 'ratings', meaning MovieLens ratings from 0-5.\n",
      "                    Set to 'web_views' for Google Analytics data.\n",
      "  --delimiter:      CSV delimiter, default to '\\t'.\n",
      "  --use-optimized:  Use optimized hyperparamters, default False.\n",
      "  --headers:        Default False for 'ratings', True for 'web_views'.\n",
      "\n",
      "Examples:\n",
      "\n",
      "# train locally with unoptimized hyperparams\n",
      "./mltrain.sh local ../data/recommendation_events.csv --data-type web_views\n",
      "\n",
      "# train on ML Engine with optimized hyperparams\n",
      "./mltrain.sh train gs://rec_serve/data/recommendation_events.csv --data-type web_views --use-optimized\n",
      "\n",
      "# tune hyperparams on ML Engine:\n",
      "./mltrain.sh tune gs://rec_serve/data/recommendation_events.csv --data-type web_views\n",
      "\"\n",
      "\n",
      "}\n",
      "\n",
      "date\n",
      "\n",
      "TIME=`date +\"%Y%m%d_%H%M%S\"`\n",
      "\n",
      "# CHANGE TO YOUR BUCKET\n",
      "BUCKET=\"gs://rec_serve\"\n",
      "\n",
      "if [[ $# < 2 ]]; then\n",
      "  usage\n",
      "  exit 1\n",
      "fi\n",
      "\n",
      "# set job vars\n",
      "TRAIN_JOB=\"$1\"\n",
      "TRAIN_FILE=\"$2\"\n",
      "JOB_NAME=wals_ml_${TRAIN_JOB}_${TIME}\n",
      "REGION=us-central1\n",
      "\n",
      "# add additional args\n",
      "shift; shift\n",
      "ARGS=\"--train-files ${TRAIN_FILE} --verbose-logging $@\"\n",
      "\n",
      "if [[ ${TRAIN_JOB} == \"local\" ]]; then\n",
      "\n",
      "  mkdir -p jobs/${JOB_NAME}\n",
      "\n",
      "  gcloud ml-engine local train \\\n",
      "    --module-name trainer.task \\\n",
      "    --package-path trainer \\\n",
      "    -- \\\n",
      "    --job-dir jobs/${JOB_NAME} \\\n",
      "    ${ARGS}\n",
      "\n",
      "elif [[ ${TRAIN_JOB} == \"train\" ]]; then\n",
      "\n",
      "  gcloud ml-engine jobs submit training ${JOB_NAME} \\\n",
      "    --region $REGION \\\n",
      "    --scale-tier=CUSTOM \\\n",
      "    --job-dir ${BUCKET}/jobs/${JOB_NAME} \\\n",
      "    --module-name trainer.task \\\n",
      "    --package-path trainer \\\n",
      "    --config trainer/config/config_train.json \\\n",
      "    -- \\\n",
      "    ${ARGS}\n",
      "\n",
      "elif [[ $TRAIN_JOB == \"tune\" ]]; then\n",
      "\n",
      "  # set configuration for tuning\n",
      "  CONFIG_TUNE=\"trainer/config/config_tune.json\"\n",
      "  for i in $ARGS ; do\n",
      "    if [[ \"$i\" == \"web_views\" ]]; then\n",
      "      CONFIG_TUNE=\"trainer/config/config_tune_web.json\"\n",
      "      break\n",
      "    fi\n",
      "  done\n",
      "\n",
      "  gcloud ml-engine jobs submit training ${JOB_NAME} \\\n",
      "    --region ${REGION} \\\n",
      "    --scale-tier=CUSTOM \\\n",
      "    --job-dir ${BUCKET}/jobs/${JOB_NAME} \\\n",
      "    --module-name trainer.task \\\n",
      "    --package-path trainer \\\n",
      "    --config ${CONFIG_TUNE} \\\n",
      "    -- \\\n",
      "    --hypertune \\\n",
      "    ${ARGS}\n",
      "\n",
      "else\n",
      "  usage\n",
      "fi\n",
      "\n",
      "date\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# view the ML train local script before running\n",
    "cat wals_ml_engine/mltrain.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov  5 18:24:26 UTC 2018\n",
      "Mon Nov  5 18:24:29 UTC 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/envs/py3env/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/local/envs/py3env/lib/python3.5/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/content/datalab/evan/training-data-analyst/courses/machine_learning/deepdive/10_recommend/endtoend/wals_ml_engine/trainer/task.py\", line 22, in <module>\n",
      "    import model\n",
      "ImportError: No module named 'model'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd wals_ml_engine\n",
    "\n",
    "# train locally with unoptimized hyperparams\n",
    "./mltrain.sh local ../data/recommendation_events.csv --data-type web_views --use-optimized\n",
    "\n",
    "# Options if we wanted to train on CMLE. We will do this with Cloud Composer later\n",
    "# train on ML Engine with optimized hyperparams\n",
    "# ./mltrain.sh train ../data/recommendation_events.csv --data-type web_views --use-optimized\n",
    "\n",
    "# tune hyperparams on ML Engine:\n",
    "# ./mltrain.sh tune ../data/recommendation_events.csv --data-type web_views\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take a couple minutes, and create a job directory under wals_ml_engine/jobs like \"wals_ml_local_20180102_012345/model\", containing the model files saved as numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the locally trained model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mwals_ml_local_20181104_041359\u001b[0m/  \u001b[01;34mwals_ml_local_20181104_042158\u001b[0m/\r\n",
      "\u001b[01;34mwals_ml_local_20181104_041949\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls wals_ml_engine/jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Copy the model files from this directory to the model folder in the project bucket:\n",
    "In the case of multiple models, take the most recent (tail -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendation model file numpy arrays in bucket:\n",
      "gs://recserve_qwiklabs-gcp-906ba67f3fedfccb/model/col.npy\n",
      "gs://recserve_qwiklabs-gcp-906ba67f3fedfccb/model/item.npy\n",
      "gs://recserve_qwiklabs-gcp-906ba67f3fedfccb/model/row.npy\n",
      "gs://recserve_qwiklabs-gcp-906ba67f3fedfccb/model/user.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://wals_ml_engine/jobs/wals_ml_local_20181104_042158/model/col.npy [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/642.1 KiB]                                                \r",
      "/ [1 files][642.1 KiB/642.1 KiB]                                                \r",
      "Copying file://wals_ml_engine/jobs/wals_ml_local_20181104_042158/model/item.npy [Content-Type=application/octet-stream]...\n",
      "/ [1 files][642.1 KiB/685.0 KiB]                                                \r",
      "/ [2 files][685.0 KiB/685.0 KiB]                                                \r",
      "-\r",
      "Copying file://wals_ml_engine/jobs/wals_ml_local_20181104_042158/model/row.npy [Content-Type=application/octet-stream]...\n",
      "- [2 files][685.0 KiB/  9.9 MiB]                                                \r",
      "- [3 files][  9.9 MiB/  9.9 MiB]                                                \r",
      "Copying file://wals_ml_engine/jobs/wals_ml_local_20181104_042158/model/user.npy [Content-Type=application/octet-stream]...\n",
      "- [3 files][  9.9 MiB/ 10.5 MiB]                                                \r",
      "\\\r",
      "\\ [4 files][ 10.5 MiB/ 10.5 MiB]                                                \r\n",
      "Operation completed over 4 objects/10.5 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "export JOB_MODEL=$(find wals_ml_engine/jobs -name \"model\" | tail -1)\n",
    "gsutil cp ${JOB_MODEL}/* gs://${BUCKET}/model/\n",
    "  \n",
    "echo \"Recommendation model file numpy arrays in bucket:\"  \n",
    "gsutil ls gs://${BUCKET}/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the recserve endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare the deploy template for the Cloud Endpoint API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "# Copyright 2017 Google Inc. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "set -euo pipefail\n",
      "\n",
      "source util.sh\n",
      "\n",
      "main() {\n",
      "  # Get our working project, or exit if it's not set.\n",
      "  local project_id=$(get_project_id)\n",
      "  if [[ -z \"$project_id\" ]]; then\n",
      "    exit 1\n",
      "  fi\n",
      "  local temp_file=$(mktemp)\n",
      "  export TEMP_FILE=\"${temp_file}.yaml\"\n",
      "  mv \"$temp_file\" \"$TEMP_FILE\"\n",
      "\n",
      "  # Because the included API is a template, we have to do some string\n",
      "  # substitution before we can deploy it. Sed does this nicely.\n",
      "  < \"$API_FILE\" sed -E \"s/YOUR-PROJECT-ID/${project_id}/g\" > \"$TEMP_FILE\"\n",
      "  echo \"Preparing config for deploying service in $API_FILE...\"\n",
      "  echo \"To deploy:  gcloud endpoints services deploy $TEMP_FILE\"\n",
      "}\n",
      "\n",
      "# Defaults.\n",
      "API_FILE=\"../app/openapi.yaml\"\n",
      "\n",
      "if [[ \"$#\" == 0 ]]; then\n",
      "  : # Use defaults.\n",
      "elif [[ \"$#\" == 1 ]]; then\n",
      "  API_FILE=\"$1\"\n",
      "else\n",
      "  echo \"Wrong number of arguments specified.\"\n",
      "  echo \"Usage: deploy_api.sh [api-file]\"\n",
      "  exit 1\n",
      "fi\n",
      "\n",
      "main \"$@\"\n",
      "\n",
      "Copy and run the deploy script generated below:\n",
      "Preparing config for deploying service in ../app/openapi.yaml...\n",
      "To deploy:  gcloud endpoints services deploy /tmp/tmp.WZyCHtnnng.yaml\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "cd scripts\n",
    "cat prepare_deploy_api.sh\n",
    "\n",
    "printf \"\\nCopy and run the deploy script generated below:\\n\"\n",
    "./prepare_deploy_api.sh                         # Prepare config file for the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output somthing like:\n",
    "\n",
    "...\n",
    "To deploy:  gcloud endpoints services deploy /var/folders/1m/r3slmhp92074pzdhhfjvnw0m00dhhl/T/tmp.n6QVl5hO.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run the endpoints deploy command output above:\n",
    "<span style=\"color: blue\">Be sure to __replace the below [FILE_NAME]__ with the results from above before running.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gcloud endpoints services deploy [REPLACE_WITH_TEMP_FILE_NAME.yaml]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare the deploy template for the App Engine App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "# Copyright 2017 Google Inc. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "set -euo pipefail\n",
      "\n",
      "source util.sh\n",
      "\n",
      "main() {\n",
      "  # Get our working project, or exit if it's not set.\n",
      "  local project_id=\"$(get_project_id)\"\n",
      "  if [[ -z \"$project_id\" ]]; then\n",
      "    exit 1\n",
      "  fi\n",
      "  # Try to create an App Engine project in our selected region.\n",
      "  # If it already exists, return a success (\"|| true\").\n",
      "  echo \"gcloud app create --region=$REGION\"\n",
      "  gcloud app create --region=\"$REGION\" || true\n",
      "\n",
      "  # Prepare the necessary variables for substitution in our app configuration\n",
      "  # template, and create a temporary file to hold the templatized version.\n",
      "  local service_name=\"${project_id}.appspot.com\"\n",
      "  local config_id=$(get_latest_config_id \"$service_name\")\n",
      "  export TEMP_FILE=\"${APP}_deploy.yaml\"\n",
      "  < \"$APP\" \\\n",
      "    sed -E \"s/SERVICE_NAME/${service_name}/g\" \\\n",
      "    | sed -E \"s/SERVICE_CONFIG_ID/${config_id}/g\" \\\n",
      "    > \"$TEMP_FILE\"\n",
      "\n",
      "  echo \"To deploy:  gcloud -q app deploy $TEMP_FILE\"\n",
      "}\n",
      "\n",
      "# Defaults.\n",
      "APP=\"../app/app_template.yaml\"\n",
      "REGION=\"us-east1\"\n",
      "SERVICE_NAME=\"default\"\n",
      "\n",
      "if [[ \"$#\" == 0 ]]; then\n",
      "  : # Use defaults.\n",
      "elif [[ \"$#\" == 1 ]]; then\n",
      "  APP=\"$1\"\n",
      "elif [[ \"$#\" == 2 ]]; then\n",
      "  APP=\"$1\"\n",
      "  REGION=\"$2\"\n",
      "else\n",
      "  echo \"Wrong number of arguments specified.\"\n",
      "  echo \"Usage: deploy_app.sh [app-template] [region]\"\n",
      "  exit 1\n",
      "fi\n",
      "\n",
      "main \"$@\"\n",
      "gcloud app create --region=us-east1\n",
      "To deploy:  gcloud -q app deploy ../app/app_template.yaml_deploy.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are creating an app for project [qwiklabs-gcp-906ba67f3fedfccb].\n",
      "WARNING: Creating an App Engine application for a project is irreversible and the region\n",
      "cannot be changed. More information about regions is at\n",
      "<https://cloud.google.com/appengine/docs/locations>.\n",
      "\n",
      "ERROR: (gcloud.app.create) The project [qwiklabs-gcp-906ba67f3fedfccb] already contains an App Engine application. You can deploy your application using `gcloud app deploy`.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "cd scripts\n",
    "\n",
    "# view the app deployment script\n",
    "cat prepare_deploy_app.sh\n",
    "\n",
    "# prepare to deploy \n",
    "./prepare_deploy_app.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can ignore the script output \"ERROR: (gcloud.app.create) The project [...] already contains an App Engine application. You can deploy your application using gcloud app deploy.\" This is expected.\n",
    "\n",
    "The script will output something like:\n",
    "\n",
    " ...\n",
    "   To deploy:  gcloud -q app deploy app/app_template.yaml_deploy.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run the command above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gcloud -q app deploy app/app_template.yaml_deploy.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take 7 - 10 minutes to deploy the app. While you wait, consider starting on Part Two below and completing the Cloud Composer DAG file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the API for Article Recommendations\n",
    "Lastly, you are able to test the recommendation model API by submitting a query request. Note the example userId passed and numRecs desired as the URL parameters for the model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl \"https://qwiklabs-gcp-906ba67f3fedfccb.appspot.com/recommendation?userId=5448543647176335931&numRecs=5\"\n",
      "{\"articles\":[\"299824032\",\"1701682\",\"299935287\",\"299959410\",\"298157062\"]}\n",
      "\n",
      "This command will exit automatically in 300 seconds.\n",
      "Generating traffic to https://qwiklabs-gcp-906ba67f3fedfccb.appspot.com/recommendation?userId=5448543647176335931&numRecs=5...\n",
      "Press Ctrl-C to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100    73  100    73    0     0    158      0 --:--:-- --:--:-- --:--:--   158\r",
      "100    73  100    73    0     0    158      0 --:--:-- --:--:-- --:--:--   158\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "cd scripts\n",
    "./query_api.sh          # Query the API.\n",
    "./generate_traffic.sh   # Send traffic to the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the call is successful, you will see the article IDs recommended for that specific user by the WALS ML model <br/>\n",
    "(Example: curl \"https://qwiklabs-gcp-12345.appspot.com/recommendation?userId=5448543647176335931&numRecs=5\"\n",
    "{\"articles\":[\"299824032\",\"1701682\",\"299935287\",\"299959410\",\"298157062\"]} )\n",
    "\n",
    "__Part One is done!__ You have successfully created the back-end architecture for serving your ML recommendation system. But we're not done yet, we still need to automatically retrain and redeploy our model once new data comes in. For that we will use [Cloud Composer](https://cloud.google.com/composer/) and [Apache Airflow](https://airflow.apache.org/).<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Part Two: Setup a scheduled workflow with Cloud Composer\n",
    "In this section you will complete a partially written training.py DAG file and copy it to the DAGS folder in your Composer instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy your Airflow bucket name\n",
    "1. Navigate to your Cloud Composer [instance](https://console.cloud.google.com/composer/environments?project=)<br/><br/>\n",
    "2. Select __DAGs Folder__<br/><br/>\n",
    "3. You will be taken to the Google Cloud Storage bucket that Cloud Composer has created automatically for your Airflow instance<br/><br/>\n",
    "4. __Copy the bucket name__ into the variable below (example: us-central1-composer-08f6edeb-bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIRFLOW_BUCKET = 'us-central1-composer-08f6edeb-bucket' # REPLACE WITH AIRFLOW BUCKET NAME\n",
    "os.environ['AIRFLOW_BUCKET'] = AIRFLOW_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete the training.py DAG file\n",
    "Apache Airflow orchestrates tasks out to other services through a [DAG (Directed Acyclic Graph)](https://airflow.apache.org/concepts.html) file which specifies what services to call, what to do, and when to run these tasks. DAG files are written in python and are loaded automatically into Airflow once present in the Airflow/dags/ folder in your Cloud Composer bucket. \n",
    "\n",
    "Your task is to complete the partially written DAG file below which will enable the automatic retraining and redeployment of our WALS recommendation model. \n",
    "\n",
    "__Complete the #TODOs__ in the Airflow DAG file below and execute the code block to save the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting airflow/dags/training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile airflow/dags/training.py\n",
    "\n",
    "# Copyright 2018 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"DAG definition for recserv model training.\"\"\"\n",
    "\n",
    "import airflow\n",
    "from airflow import DAG\n",
    "\n",
    "# Reference for all available airflow operators: \n",
    "# https://github.com/apache/incubator-airflow/tree/master/airflow/contrib/operators\n",
    "from airflow.contrib.operators.bigquery_operator import BigQueryOperator\n",
    "from airflow.contrib.operators.bigquery_to_gcs import BigQueryToCloudStorageOperator\n",
    "from airflow.hooks.base_hook import BaseHook\n",
    "# from airflow.contrib.operators.mlengine_operator import MLEngineTrainingOperator\n",
    "# above mlengine_operator currently doesnt support custom MasterType so we import our own plugins:\n",
    "\n",
    "# custom plugins\n",
    "from airflow.operators.app_engine_admin_plugin import AppEngineVersionOperator\n",
    "from airflow.operators.ml_engine_plugin import MLEngineTrainingOperator\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "def _get_project_id():\n",
    "  \"\"\"Get project ID from default GCP connection.\"\"\"\n",
    "\n",
    "  extras = BaseHook.get_connection('google_cloud_default').extra_dejson\n",
    "  key = 'extra__google_cloud_platform__project'\n",
    "  if key in extras:\n",
    "    project_id = extras[key]\n",
    "  else:\n",
    "    raise ('Must configure project_id in google_cloud_default '\n",
    "           'connection from Airflow Console')\n",
    "  return project_id\n",
    "\n",
    "PROJECT_ID = _get_project_id()\n",
    "\n",
    "# Data set constants, used in BigQuery tasks.  You can change these\n",
    "# to conform to your data.\n",
    "\n",
    "# TODO: Specify your BigQuery dataset name and table name\n",
    "DATASET = 'GA360_test'\n",
    "TABLE_NAME = 'ga_sessions_sample'\n",
    "ARTICLE_CUSTOM_DIMENSION = '10'\n",
    "\n",
    "# TODO: Confirm bucket name and region\n",
    "# GCS bucket names and region, can also be changed.\n",
    "BUCKET = 'gs://recserve_' + PROJECT_ID\n",
    "REGION = 'us-east1'\n",
    "\n",
    "# The code package name comes from the model code in the wals_ml_engine\n",
    "# directory of the solution code base.\n",
    "PACKAGE_URI = BUCKET + '/code/wals_ml_engine-0.1.tar.gz'\n",
    "JOB_DIR = BUCKET + '/jobs'\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': airflow.utils.dates.days_ago(2),\n",
    "    'email': ['airflow@example.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 5,\n",
    "    'retry_delay': datetime.timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "# Default schedule interval using cronjob syntax - can be customized here\n",
    "# or in the Airflow console.\n",
    "\n",
    "# TODO: Specify a schedule interval in CRON syntax to run once a day at 2100 hours (9pm)\n",
    "# Reference: https://airflow.apache.org/scheduler.html\n",
    "schedule_interval = '00 21 * * *'\n",
    "\n",
    "# TODO: Title your DAG to be recommendations_training_v1\n",
    "dag = DAG('recommendations_training_v1', \n",
    "          default_args=default_args,\n",
    "          schedule_interval=schedule_interval)\n",
    "\n",
    "dag.doc_md = __doc__\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "# Task Definition\n",
    "#\n",
    "#\n",
    "\n",
    "# BigQuery training data query\n",
    "\n",
    "bql='''\n",
    "#legacySql\n",
    "SELECT\n",
    " fullVisitorId as clientId,\n",
    " ArticleID as contentId,\n",
    " (nextTime - hits.time) as timeOnPage,\n",
    "FROM(\n",
    "  SELECT\n",
    "    fullVisitorId,\n",
    "    hits.time,\n",
    "    MAX(IF(hits.customDimensions.index={0},\n",
    "           hits.customDimensions.value,NULL)) WITHIN hits AS ArticleID,\n",
    "    LEAD(hits.time, 1) OVER (PARTITION BY fullVisitorId, visitNumber\n",
    "                             ORDER BY hits.time ASC) as nextTime\n",
    "  FROM [{1}.{2}.{3}]\n",
    "  WHERE hits.type = \"PAGE\"\n",
    ") HAVING timeOnPage is not null and contentId is not null;\n",
    "'''\n",
    "\n",
    "bql = bql.format(ARTICLE_CUSTOM_DIMENSION, PROJECT_ID, DATASET, TABLE_NAME)\n",
    "\n",
    "# TODO: Complete the BigQueryOperator task to truncate the table if it already exists before writing\n",
    "# Reference: https://airflow.apache.org/integration.html#bigqueryoperator\n",
    "t1 = BigQueryOperator(\n",
    "    task_id='bq_rec_training_data',\n",
    "    bql=bql,\n",
    "    destination_dataset_table='%s.recommendation_events' % DATASET,\n",
    "    write_disposition='WRITE_TRUNCATE', # specify to truncate on writes\n",
    "    dag=dag)\n",
    "\n",
    "# BigQuery training data export to GCS\n",
    "\n",
    "# TODO: Fill in the missing operator name for task #2 which\n",
    "# takes a BigQuery dataset and table as input and exports it to GCS as a CSV\n",
    "training_file = BUCKET + '/data/recommendation_events.csv'\n",
    "t2 = BigQueryToCloudStorageOperator(\n",
    "    task_id='bq_export_op',\n",
    "    source_project_dataset_table='%s.recommendation_events' % DATASET,\n",
    "    destination_cloud_storage_uris=[training_file],\n",
    "    export_format='CSV',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "# ML Engine training job\n",
    "\n",
    "job_id = 'recserve_{0}'.format(datetime.datetime.now().strftime('%Y%m%d%H%M'))\n",
    "job_dir = BUCKET + '/jobs/' + job_id\n",
    "output_dir = BUCKET\n",
    "training_args = ['--job-dir', job_dir,\n",
    "                 '--train-files', training_file,\n",
    "                 '--output-dir', output_dir,\n",
    "                 '--data-type', 'web_views',\n",
    "                 '--use-optimized']\n",
    "\n",
    "# TODO: Fill in the missing operator name for task #3 which will\n",
    "# start a new training job to Cloud ML Engine\n",
    "# Reference: https://airflow.apache.org/integration.html#cloud-ml-engine\n",
    "# https://cloud.google.com/ml-engine/docs/tensorflow/machine-types\n",
    "t3 = MLEngineTrainingOperator(\n",
    "    task_id='ml_engine_training_op',\n",
    "    project_id=PROJECT_ID,\n",
    "    job_id=job_id,\n",
    "    package_uris=[PACKAGE_URI],\n",
    "    training_python_module='trainer.task',\n",
    "    training_args=training_args,\n",
    "    region=REGION,\n",
    "    scale_tier='CUSTOM',\n",
    "    master_type='complex_model_m_gpu',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# App Engine deploy new version\n",
    "\n",
    "t4 = AppEngineVersionOperator(\n",
    "    task_id='app_engine_deploy_version',\n",
    "    project_id=PROJECT_ID,\n",
    "    service_id='default',\n",
    "    region=REGION,\n",
    "    service_spec=None,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# TODO: Be sure to set_upstream dependencies for all tasks\n",
    "t2.set_upstream(t1)\n",
    "t3.set_upstream(t2)\n",
    "t4.set_upstream(t3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy local Airflow DAG file and plugins into the DAGs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://airflow/dags/training.py [Content-Type=text/x-python]...\n",
      "/ [0 files][    0.0 B/  5.9 KiB]                                                \r",
      "/ [1 files][  5.9 KiB/  5.9 KiB]                                                \r\n",
      "Operation completed over 1 objects/5.9 KiB.                                      \n",
      "Copying file://airflow/plugins/ml_engine_plugin.py [Content-Type=text/x-python]...\n",
      "/ [0 files][    0.0 B/  7.9 KiB]                                                \r",
      "/ [1 files][  7.9 KiB/  7.9 KiB]                                                \r",
      "Copying file://airflow/plugins/gae_admin_plugin.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  7.9 KiB/ 17.9 KiB]                                                \r",
      "/ [2 files][ 17.9 KiB/ 17.9 KiB]                                                \r\n",
      "Operation completed over 2 objects/17.9 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil cp airflow/dags/training.py gs://${AIRFLOW_BUCKET}/dags # overwrite if it exists\n",
    "gsutil cp -r airflow/plugins gs://${AIRFLOW_BUCKET} # copy custom plugins s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Navigate to your Cloud Composer [instance](https://console.cloud.google.com/composer/environments?project=)<br/><br/>\n",
    "\n",
    "3. Trigger a __manual run__ of your DAG for testing<br/><br/>\n",
    "\n",
    "3. Ensure your DAG runs successfully (all nodes outlined in dark green and 'success' tag shows)\n",
    "\n",
    "![Successful Airflow DAG run](./img/airflow_successful_run.jpg \"Successful Airflow DAG run\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting your DAG\n",
    "\n",
    "DAG not executing successfully? Follow these below steps to troubleshoot.\n",
    "\n",
    "Click on the name of a DAG to view a run (ex: recommendations_training_v1)\n",
    "\n",
    "1. Select a node in the DAG (red or yellow borders mean failed nodes)\n",
    "2. Select View Log\n",
    "3. Scroll to the bottom of the log to diagnose\n",
    "4. X Option: Clear and immediately restart the DAG after diagnosing the issue\n",
    "\n",
    "Tips:\n",
    "- If bq_rec_training_data immediately fails without logs, your DAG file is missing key parts and is not compiling\n",
    "- ml_engine_training_op will take 9 - 12 minutes to run. Monitor the training job in [ML Engine](https://console.cloud.google.com/mlengine/jobs?project=)\n",
    "- Lastly, check the [solution endtoend.ipynb](../endtoend/endtoend.ipynb) to compare your lab answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Viewing Airflow logs](./img/airflow_viewing_logs.jpg \"Viewing Airflow logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "You have made it to the end of the end-to-end recommendation system lab. You have successfully setup an automated workflow to retrain and redeploy your recommendation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Challenges\n",
    "\n",
    "Looking to solidify your Cloud Composer skills even more? Complete the __optional challenges__ below\n",
    "<br/><br/>\n",
    "### Challenge 1\n",
    "Use either the [BigQueryCheckOperator](https://airflow.apache.org/integration.html#bigquerycheckoperator) or the [BigQueryValueCheckOperator](https://airflow.apache.org/integration.html#bigqueryvaluecheckoperator) to create a new task in your DAG that ensures the SQL query for training data is returning valid results before it is passed to Cloud ML Engine for training. \n",
    "<br/><br/>\n",
    "Hint: Check for COUNT() = 0 or other health check\n",
    "<br/><br/><br/>\n",
    "### Challenge 2\n",
    "Create a Cloud Function to [automatically trigger](https://cloud.google.com/composer/docs/how-to/using/triggering-with-gcf) your DAG when a new recommendation_events.csv file is loaded into your Google Cloud Storage Bucket. \n",
    "<br/><br/>\n",
    "Hint: Check the [composer_gcf_trigger.ipynb lab](../composer_gcf_trigger/composertriggered.ipynb) for inspiration\n",
    "<br/><br/><br/>\n",
    "### Challenge 3\n",
    "Modify the BigQuery query in the DAG to only train on a portion of the data available in the dataset using a WHERE clause filtering on date. Next, parameterize the WHERE clause to be based on when the Airflow DAG is run\n",
    "<br/><br/>\n",
    "Hint: Make use of prebuilt [Airflow macros](https://airflow.incubator.apache.org/_modules/airflow/macros.html) like the below:\n",
    "\n",
    "_constants or can be dynamic based on Airflow macros_ <br/>\n",
    "max_query_date = '2018-02-01' # {{ macros.ds_add(ds, -7) }} <br/>\n",
    "min_query_date = '2018-01-01' # {{ macros.ds_add(ds, -1) }} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- Follow the latest [Airflow operators](https://github.com/apache/incubator-airflow/tree/master/airflow/contrib/operators) on github"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
