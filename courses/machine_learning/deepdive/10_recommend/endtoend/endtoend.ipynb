{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations on GCP with TensorFlow and WALS with Cloud Composer\n",
    "***\n",
    "This lab is adapted from the original [solution](https://github.com/GoogleCloudPlatform/tensorflow-recommendation-wals) created by [lukmanr](https://github.com/GoogleCloudPlatform/tensorflow-recommendation-wals/commits?author=lukmanr) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project deploys a solution for a recommendation service on GCP, using the WALS algorithm in TensorFlow. Components include:\n",
    "\n",
    "- Recommendation model code, and scripts to train and tune the model on ML Engine\n",
    "- A REST endpoint using Google Cloud Endpoints for serving recommendations\n",
    "- An Airflow server managed by Cloud Composer for running scheduled model training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Cloud Composer Instance\n",
    "- Create a Cloud Composer [instance](https://console.cloud.google.com/composer/environments/create?project=)\n",
    "    1. Specify 'composer' for name\n",
    "    2. Choose a location\n",
    "    3. Keep the remaining settings at their defaults\n",
    "    4. Select Create\n",
    "\n",
    "This takes 15 - 20 minutes. Continue with the rest of the lab as you will be using Cloud Composer near the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install sh --upgrade pip # needed to execute shell scripts later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup environment variables\n",
    "<span style=\"color: blue\">__Replace the below settings with your own.__</span> Note: you can leave AIRFLOW_BUCKET blank and come back to it after your Composer instance is created which automatically will create an Airflow bucket for you. <br><br>\n",
    "\n",
    "### 1. Make a GCS bucket with the name recserve_[YOUR-PROJECT-ID]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "BUCKET = 'BUCKET' # REPLACE WITH A BUCKET NAME (PUT YOUR PROJECT ID AND WE CREATE THE BUCKET ITSELF NEXT)\n",
    "PROJECT = 'PROJECT' # REPLACE WITH YOUR PROJECT ID\n",
    "REGION = 'us-central1' # REPLACE WITH YOUR REGION e.g. us-central1\n",
    "\n",
    "# do not change these\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = 'recserve_' + BUCKET\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# create GCS bucket with recserve_PROJECT_NAME if not exists\n",
    "exists=$(gsutil ls -d | grep -w gs://${BUCKET}/)\n",
    "if [ -n \"$exists\" ]; then\n",
    "   echo \"Not creating recserve_bucket since it already exists.\"\n",
    "else\n",
    "   echo \"Creating recserve_bucket\"\n",
    "   gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Google App Engine permissions\n",
    "1. In [IAM](https://console.cloud.google.com/iam-admin/iam?project=), __change permissions for \"Compute Engine default service account\" from Editor to Owner__. This is required so you can create and deploy App Engine versions from within Cloud Datalab. Note: the alternative is to run all app engine commands directly in Cloud Shell instead of from within Cloud Datalab.<br/><br/>\n",
    "\n",
    "2. Create an App Engine instance if you have not already by uncommenting and running the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# run app engine creation commands\n",
    "# gcloud app create --region ${REGION} # see: https://cloud.google.com/compute/docs/regions-zones/\n",
    "# gcloud app update --no-split-health-checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part One: Setup and Train the WALS Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload sample data to BigQuery \n",
    "This tutorial comes with a sample Google Analytics data set, containing page tracking events from the Austrian news site Kurier.at. The schema file '''ga_sessions_sample_schema.json''' is located in the folder data in the tutorial code, and the data file '''ga_sessions_sample.json.gz''' is located in a public Cloud Storage bucket associated with this tutorial. To upload this data set to BigQuery:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy sample data files into our bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil -m cp gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/endtoend/data/ga_sessions_sample.json.gz gs://${BUCKET}/data/ga_sessions_sample.json.gz\n",
    "gsutil -m cp gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/endtoend/data/recommendation_events.csv data/recommendation_events.csv\n",
    "gsutil -m cp gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/endtoend/data/recommendation_events.csv gs://${BUCKET}/data/recommendation_events.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create empty BigQuery dataset and load sample JSON data\n",
    "Note: Ingesting the 400K rows of sample data. This usually takes 5-7 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# create BigQuery dataset if it doesn't already exist\n",
    "exists=$(bq ls -d | grep -w GA360_test)\n",
    "if [ -n \"$exists\" ]; then\n",
    "   echo \"Not creating GA360_test since it already exists.\"\n",
    "else\n",
    "   echo \"Creating GA360_test dataset.\"\n",
    "   bq --project_id=${PROJECT} mk GA360_test \n",
    "fi\n",
    "\n",
    "# create the schema and load our sample Google Analytics session data\n",
    "bq load --source_format=NEWLINE_DELIMITED_JSON \\\n",
    " GA360_test.ga_sessions_sample \\\n",
    " gs://${BUCKET}/data/ga_sessions_sample.json.gz \\\n",
    " data/ga_sessions_sample_schema.json # can't load schema files from GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install WALS model training package and model data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a distributable package. Copy the package up to the code folder in the bucket you created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd wals_ml_engine\n",
    "\n",
    "echo \"creating distributable package\"\n",
    "python setup.py sdist\n",
    "\n",
    "echo \"copying ML package to bucket\"\n",
    "gsutil cp dist/wals_ml_engine-0.1.tar.gz gs://${BUCKET}/code/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run the WALS model on the sample data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# view the ML train local script before running\n",
    "cat wals_ml_engine/mltrain.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd wals_ml_engine\n",
    "\n",
    "# train locally with unoptimized hyperparams\n",
    "./mltrain.sh local ../data/recommendation_events.csv --data-type web_views --use-optimized\n",
    "\n",
    "# Options if we wanted to train on CMLE. We will do this with Cloud Composer later\n",
    "# train on ML Engine with optimized hyperparams\n",
    "# ./mltrain.sh train ../data/recommendation_events.csv --data-type web_views --use-optimized\n",
    "\n",
    "# tune hyperparams on ML Engine:\n",
    "# ./mltrain.sh tune ../data/recommendation_events.csv --data-type web_views\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take a couple minutes, and create a job directory under wals_ml_engine/jobs like \"wals_ml_local_20180102_012345/model\", containing the model files saved as numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the locally trained model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls wals_ml_engine/jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Copy the model files from this directory to the model folder in the project bucket:\n",
    "In the case of multiple models, take the most recent (tail -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "export JOB_MODEL=$(find wals_ml_engine/jobs -name \"model\" | tail -1)\n",
    "gsutil cp ${JOB_MODEL}/* gs://${BUCKET}/model/\n",
    "  \n",
    "echo \"Recommendation model file numpy arrays in bucket:\"  \n",
    "gsutil ls gs://${BUCKET}/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the recserve endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare the deploy template for the Cloud Endpoint API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "cd scripts\n",
    "cat prepare_deploy_api.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "printf \"\\nCopy and run the deploy script generated below:\\n\"\n",
    "cd scripts\n",
    "./prepare_deploy_api.sh                         # Prepare config file for the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output somthing like:\n",
    "\n",
    "```To deploy:  gcloud endpoints services deploy /var/folders/1m/r3slmhp92074pzdhhfjvnw0m00dhhl/T/tmp.n6QVl5hO.yaml```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run the endpoints deploy command output above:\n",
    "<span style=\"color: blue\">Be sure to __replace the below [FILE_NAME]__ with the results from above before running.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud endpoints services deploy [REPLACE_WITH_TEMP_FILE_NAME.yaml]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare the deploy template for the App Engine App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# view the app deployment script\n",
    "cat scripts/prepare_deploy_app.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# prepare to deploy \n",
    "cd scripts\n",
    "\n",
    "./prepare_deploy_app.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can ignore the script output \"ERROR: (gcloud.app.create) The project [...] already contains an App Engine application. You can deploy your application using gcloud app deploy.\" This is expected.\n",
    "\n",
    "The script will output something like:\n",
    "\n",
    "```To deploy:  gcloud -q app deploy app/app_template.yaml_deploy.yaml```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run the command above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud -q app deploy app/app_template.yaml_deploy.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take 7 - 10 minutes to deploy the app. While you wait, consider starting on Part Two below and completing the Cloud Composer DAG file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the API for Article Recommendations\n",
    "Lastly, you are able to test the recommendation model API by submitting a query request. Note the example userId passed and numRecs desired as the URL parameters for the model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd scripts\n",
    "./query_api.sh          # Query the API.\n",
    "./generate_traffic.sh   # Send traffic to the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the call is successful, you will see the article IDs recommended for that specific user by the WALS ML model <br/>\n",
    "(Example: curl \"https://qwiklabs-gcp-12345.appspot.com/recommendation?userId=5448543647176335931&numRecs=5\"\n",
    "{\"articles\":[\"299824032\",\"1701682\",\"299935287\",\"299959410\",\"298157062\"]} )\n",
    "\n",
    "__Part One is done!__ You have successfully created the back-end architecture for serving your ML recommendation system. But we're not done yet, we still need to automatically retrain and redeploy our model once new data comes in. For that we will use [Cloud Composer](https://cloud.google.com/composer/) and [Apache Airflow](https://airflow.apache.org/).<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Part Two: Setup a scheduled workflow with Cloud Composer\n",
    "In this section you will complete a partially written training.py DAG file and copy it to the DAGS folder in your Composer instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy your Airflow bucket name\n",
    "1. Navigate to your Cloud Composer [instance](https://console.cloud.google.com/composer/environments?project=)<br/><br/>\n",
    "2. Select __DAGs Folder__<br/><br/>\n",
    "3. You will be taken to the Google Cloud Storage bucket that Cloud Composer has created automatically for your Airflow instance<br/><br/>\n",
    "4. __Copy the bucket name__ into the variable below (example: us-central1-composer-08f6edeb-bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIRFLOW_BUCKET = 'us-central1-composer-21587538-bucket' # REPLACE WITH AIRFLOW BUCKET NAME\n",
    "os.environ['AIRFLOW_BUCKET'] = AIRFLOW_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete the training.py DAG file\n",
    "Apache Airflow orchestrates tasks out to other services through a [DAG (Directed Acyclic Graph)](https://airflow.apache.org/concepts.html) file which specifies what services to call, what to do, and when to run these tasks. DAG files are written in python and are loaded automatically into Airflow once present in the Airflow/dags/ folder in your Cloud Composer bucket. \n",
    "\n",
    "Your task is to complete the partially written DAG file below which will enable the automatic retraining and redeployment of our WALS recommendation model. \n",
    "\n",
    "__Complete the #TODOs__ in the Airflow DAG file below and execute the code block to save the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile airflow/dags/training.py\n",
    "\n",
    "# Copyright 2018 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"DAG definition for recserv model training.\"\"\"\n",
    "\n",
    "import airflow\n",
    "from airflow import DAG\n",
    "\n",
    "# Reference for all available airflow operators: \n",
    "# https://github.com/apache/incubator-airflow/tree/master/airflow/contrib/operators\n",
    "from airflow.contrib.operators.bigquery_operator import BigQueryOperator\n",
    "from airflow.contrib.operators.bigquery_to_gcs import BigQueryToCloudStorageOperator\n",
    "from airflow.hooks.base_hook import BaseHook\n",
    "# from airflow.contrib.operators.mlengine_operator import MLEngineTrainingOperator\n",
    "# above mlengine_operator currently doesnt support custom MasterType so we import our own plugins:\n",
    "\n",
    "# custom plugins\n",
    "from airflow.operators.app_engine_admin_plugin import AppEngineVersionOperator\n",
    "from airflow.operators.ml_engine_plugin import MLEngineTrainingOperator\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "def _get_project_id():\n",
    "  \"\"\"Get project ID from default GCP connection.\"\"\"\n",
    "\n",
    "  extras = BaseHook.get_connection('google_cloud_default').extra_dejson\n",
    "  key = 'extra__google_cloud_platform__project'\n",
    "  if key in extras:\n",
    "    project_id = extras[key]\n",
    "  else:\n",
    "    raise ('Must configure project_id in google_cloud_default '\n",
    "           'connection from Airflow Console')\n",
    "  return project_id\n",
    "\n",
    "PROJECT_ID = _get_project_id()\n",
    "\n",
    "# Data set constants, used in BigQuery tasks.  You can change these\n",
    "# to conform to your data.\n",
    "\n",
    "# TODO: Specify your BigQuery dataset name and table name\n",
    "DATASET = 'GA360_test'\n",
    "TABLE_NAME = 'ga_sessions_sample'\n",
    "ARTICLE_CUSTOM_DIMENSION = '10'\n",
    "\n",
    "# TODO: Confirm bucket name and region\n",
    "# GCS bucket names and region, can also be changed.\n",
    "BUCKET = 'gs://recserve_' + PROJECT_ID\n",
    "REGION = 'us-east1'\n",
    "\n",
    "# The code package name comes from the model code in the wals_ml_engine\n",
    "# directory of the solution code base.\n",
    "PACKAGE_URI = BUCKET + '/code/wals_ml_engine-0.1.tar.gz'\n",
    "JOB_DIR = BUCKET + '/jobs'\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': airflow.utils.dates.days_ago(2),\n",
    "    'email': ['airflow@example.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 5,\n",
    "    'retry_delay': datetime.timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "# Default schedule interval using cronjob syntax - can be customized here\n",
    "# or in the Airflow console.\n",
    "\n",
    "# TODO: Specify a schedule interval in CRON syntax to run once a day at 2100 hours (9pm)\n",
    "# Reference: https://airflow.apache.org/scheduler.html\n",
    "schedule_interval = '00 21 * * *'\n",
    "\n",
    "# TODO: Title your DAG to be recommendations_training_v1\n",
    "dag = DAG('recommendations_training_v1', \n",
    "          default_args=default_args,\n",
    "          schedule_interval=schedule_interval)\n",
    "\n",
    "dag.doc_md = __doc__\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "# Task Definition\n",
    "#\n",
    "#\n",
    "\n",
    "# BigQuery training data query\n",
    "\n",
    "bql='''\n",
    "#legacySql\n",
    "SELECT\n",
    " fullVisitorId as clientId,\n",
    " ArticleID as contentId,\n",
    " (nextTime - hits.time) as timeOnPage,\n",
    "FROM(\n",
    "  SELECT\n",
    "    fullVisitorId,\n",
    "    hits.time,\n",
    "    MAX(IF(hits.customDimensions.index={0},\n",
    "           hits.customDimensions.value,NULL)) WITHIN hits AS ArticleID,\n",
    "    LEAD(hits.time, 1) OVER (PARTITION BY fullVisitorId, visitNumber\n",
    "                             ORDER BY hits.time ASC) as nextTime\n",
    "  FROM [{1}.{2}.{3}]\n",
    "  WHERE hits.type = \"PAGE\"\n",
    ") HAVING timeOnPage is not null and contentId is not null;\n",
    "'''\n",
    "\n",
    "bql = bql.format(ARTICLE_CUSTOM_DIMENSION, PROJECT_ID, DATASET, TABLE_NAME)\n",
    "\n",
    "# TODO: Complete the BigQueryOperator task to truncate the table if it already exists before writing\n",
    "# Reference: https://airflow.apache.org/integration.html#bigqueryoperator\n",
    "t1 = BigQueryOperator(\n",
    "    task_id='bq_rec_training_data',\n",
    "    bql=bql,\n",
    "    destination_dataset_table='%s.recommendation_events' % DATASET,\n",
    "    write_disposition='WRITE_TRUNCATE', # specify to truncate on writes\n",
    "    dag=dag)\n",
    "\n",
    "# BigQuery training data export to GCS\n",
    "\n",
    "# TODO: Fill in the missing operator name for task #2 which\n",
    "# takes a BigQuery dataset and table as input and exports it to GCS as a CSV\n",
    "training_file = BUCKET + '/data/recommendation_events.csv'\n",
    "t2 = BigQueryToCloudStorageOperator(\n",
    "    task_id='bq_export_op',\n",
    "    source_project_dataset_table='%s.recommendation_events' % DATASET,\n",
    "    destination_cloud_storage_uris=[training_file],\n",
    "    export_format='CSV',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "# ML Engine training job\n",
    "\n",
    "job_id = 'recserve_{0}'.format(datetime.datetime.now().strftime('%Y%m%d%H%M'))\n",
    "job_dir = BUCKET + '/jobs/' + job_id\n",
    "output_dir = BUCKET\n",
    "training_args = ['--job-dir', job_dir,\n",
    "                 '--train-files', training_file,\n",
    "                 '--output-dir', output_dir,\n",
    "                 '--data-type', 'web_views',\n",
    "                 '--use-optimized']\n",
    "\n",
    "# TODO: Fill in the missing operator name for task #3 which will\n",
    "# start a new training job to Cloud ML Engine\n",
    "# Reference: https://airflow.apache.org/integration.html#cloud-ml-engine\n",
    "# https://cloud.google.com/ml-engine/docs/tensorflow/machine-types\n",
    "t3 = MLEngineTrainingOperator(\n",
    "    task_id='ml_engine_training_op',\n",
    "    project_id=PROJECT_ID,\n",
    "    job_id=job_id,\n",
    "    package_uris=[PACKAGE_URI],\n",
    "    training_python_module='trainer.task',\n",
    "    training_args=training_args,\n",
    "    region=REGION,\n",
    "    scale_tier='CUSTOM',\n",
    "    master_type='complex_model_m_gpu',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# App Engine deploy new version\n",
    "\n",
    "t4 = AppEngineVersionOperator(\n",
    "    task_id='app_engine_deploy_version',\n",
    "    project_id=PROJECT_ID,\n",
    "    service_id='default',\n",
    "    region=REGION,\n",
    "    service_spec=None,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# TODO: Be sure to set_upstream dependencies for all tasks\n",
    "t2.set_upstream(t1)\n",
    "t3.set_upstream(t2)\n",
    "t4.set_upstream(t3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy local Airflow DAG file and plugins into the DAGs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil cp airflow/dags/training.py gs://${AIRFLOW_BUCKET}/dags # overwrite if it exists\n",
    "gsutil cp -r airflow/plugins gs://${AIRFLOW_BUCKET} # copy custom plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Navigate to your Cloud Composer [instance](https://console.cloud.google.com/composer/environments?project=)<br/><br/>\n",
    "\n",
    "3. Trigger a __manual run__ of your DAG for testing<br/><br/>\n",
    "\n",
    "3. Ensure your DAG runs successfully (all nodes outlined in dark green and 'success' tag shows)\n",
    "\n",
    "![Successful Airflow DAG run](./img/airflow_successful_run.jpg \"Successful Airflow DAG run\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting your DAG\n",
    "\n",
    "DAG not executing successfully? Follow these below steps to troubleshoot.\n",
    "\n",
    "Click on the name of a DAG to view a run (ex: recommendations_training_v1)\n",
    "\n",
    "1. Select a node in the DAG (red or yellow borders mean failed nodes)\n",
    "2. Select View Log\n",
    "3. Scroll to the bottom of the log to diagnose\n",
    "4. X Option: Clear and immediately restart the DAG after diagnosing the issue\n",
    "\n",
    "Tips:\n",
    "- If bq_rec_training_data immediately fails without logs, your DAG file is missing key parts and is not compiling\n",
    "- ml_engine_training_op will take 9 - 12 minutes to run. Monitor the training job in [ML Engine](https://console.cloud.google.com/mlengine/jobs?project=)\n",
    "- Lastly, check the [solution endtoend.ipynb](../endtoend/endtoend.ipynb) to compare your lab answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Viewing Airflow logs](./img/airflow_viewing_logs.jpg \"Viewing Airflow logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "You have made it to the end of the end-to-end recommendation system lab. You have successfully setup an automated workflow to retrain and redeploy your recommendation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Challenges\n",
    "\n",
    "Looking to solidify your Cloud Composer skills even more? Complete the __optional challenges__ below\n",
    "<br/><br/>\n",
    "### Challenge 1\n",
    "Use either the [BigQueryCheckOperator](https://airflow.apache.org/integration.html#bigquerycheckoperator) or the [BigQueryValueCheckOperator](https://airflow.apache.org/integration.html#bigqueryvaluecheckoperator) to create a new task in your DAG that ensures the SQL query for training data is returning valid results before it is passed to Cloud ML Engine for training. \n",
    "<br/><br/>\n",
    "Hint: Check for COUNT() = 0 or other health check\n",
    "<br/><br/><br/>\n",
    "### Challenge 2\n",
    "Create a Cloud Function to [automatically trigger](https://cloud.google.com/composer/docs/how-to/using/triggering-with-gcf) your DAG when a new recommendation_events.csv file is loaded into your Google Cloud Storage Bucket. \n",
    "<br/><br/>\n",
    "Hint: Check the [composer_gcf_trigger.ipynb lab](../composer_gcf_trigger/composertriggered.ipynb) for inspiration\n",
    "<br/><br/><br/>\n",
    "### Challenge 3\n",
    "Modify the BigQuery query in the DAG to only train on a portion of the data available in the dataset using a WHERE clause filtering on date. Next, parameterize the WHERE clause to be based on when the Airflow DAG is run\n",
    "<br/><br/>\n",
    "Hint: Make use of prebuilt [Airflow macros](https://airflow.incubator.apache.org/_modules/airflow/macros.html) like the below:\n",
    "\n",
    "_constants or can be dynamic based on Airflow macros_ <br/>\n",
    "max_query_date = '2018-02-01' # {{ macros.ds_add(ds, -7) }} <br/>\n",
    "min_query_date = '2018-01-01' # {{ macros.ds_add(ds, -1) }} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- Follow the latest [Airflow operators](https://github.com/apache/incubator-airflow/tree/master/airflow/contrib/operators) on github"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
