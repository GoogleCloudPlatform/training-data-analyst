{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triggering a Cloud Composer Pipeline with a Google Cloud Function\n",
    "\n",
    "In this advanced lab you will learn how to create and run an [Apache Airflow](http://airflow.apache.org/) workflow in Cloud Composer that completes the following tasks:\n",
    "- Watches for new CSV data to be uploaded to a [Cloud Storage](https://cloud.google.com/storage/docs/) bucket\n",
    "- A [Cloud Function](https://cloud.google.com/composer/docs/how-to/using/triggering-with-gcf#getting_the_client_id) call triggers the [Cloud Composer Airflow DAG](https://cloud.google.com/composer/docs/how-to/using/writing-dags) to run when a new file is detected \n",
    "- The workflow finds the input file that triggered the workflow and executes a [Cloud Dataflow](https://cloud.google.com/dataflow/) job to transform and output the data to BigQuery  \n",
    "- Moves the original input file to a different Cloud Storage bucket for storing processed files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One: Create Cloud Composer environment and workflow\n",
    "First, create a Cloud Composer environment if you don't have one already by doing the following:\n",
    "1. In the Navigation menu under Big Data, select **Composer**\n",
    "2. Select **Create**\n",
    "3. Set the following parameters:\n",
    "    - Name: mlcomposer\n",
    "    - Location: us-central1\n",
    "    - Other values at defaults\n",
    "4. Select **Create**\n",
    "\n",
    "The environment creation process is completed when the green checkmark displays to the left of the environment name on the Environments page in the GCP Console.\n",
    "It can take up to 20 minutes for the environment to complete the setup process. Move on to the next section - Create Cloud Storage buckets and BigQuery dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'your-project-id' # REPLACE WITH YOUR PROJECT ID\n",
    "REGION = 'us-central1' # REPLACE WITH YOUR REGION e.g. us-central1\n",
    "\n",
    "# do not change these\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Cloud Storage buckets\n",
    "Create two Cloud Storage Multi-Regional buckets in your project. \n",
    "- project-id_input\n",
    "- project-id_output\n",
    "\n",
    "Run the below to automatically create the buckets and load some sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## create GCS buckets\n",
    "# TODO: Flag \"-d\" is not supported in \"gcloud storage ls\". Manual review required.\n",
    "exists=$(gcloud storage ls | grep -w gs://${PROJECT}_input/)\n",
    "if [ -n \"$exists\" ]; then\n",
    "   echo \"Skipping the creation of input bucket.\"\n",
    "else\n",
    "   echo \"Creating input bucket.\"\n",
    "   gcloud storage buckets create --location ${REGION} gs://${PROJECT}_input\n",
    "   echo \"Loading sample data for later\"\n",
    "   gcloud storage cp resources/usa_names.csv gs://${PROJECT}_input\n",
    "fi\n",
    "\n",
    "# TODO: Flag \"-d\" is not supported in \"gcloud storage ls\". Manual review required.\n",
    "exists=$(gcloud storage ls | grep -w gs://${PROJECT}_output/)\n",
    "if [ -n \"$exists\" ]; then\n",
    "   echo \"Skipping the creation of output bucket.\"\n",
    "else\n",
    "   echo \"Creating output bucket.\"\n",
    "   gcloud storage buckets create --location ${REGION} gs://${PROJECT}_output\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BigQuery Destination Dataset and Table\n",
    "Next, we'll create a data sink to store the ingested data from GCS<br><br>\n",
    "\n",
    "### Create a new Dataset\n",
    "1. In the Navigation menu, select **BigQuery**\n",
    "2. Then click on your qwiklabs project ID\n",
    "3. Click **Create Dataset**\n",
    "4. Name your dataset **ml_pipeline** and leave other values at defaults\n",
    "5. Click **Create Dataset**\n",
    "\n",
    "\n",
    "### Create a new empty table\n",
    "1. Click on the newly created dataset\n",
    "2. Click **Create Table**\n",
    "3. For Destination Table name specify **ingest_table**\n",
    "4. For schema click **Edit as Text** and paste in the below schema\n",
    "\n",
    "    state:\tSTRING,<br>\n",
    "    gender:\tSTRING,<br>\n",
    "    year:\tSTRING,<br>\n",
    "    name:\tSTRING,<br>\n",
    "    number:\tSTRING,<br>\n",
    "    created_date:\tSTRING,<br>\n",
    "    filename:\tSTRING,<br>\n",
    "    load_dt:\tDATE<br><br>\n",
    "\n",
    "5. Click **Create Table**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Airflow concepts\n",
    "While your Cloud Composer environment is building, let’s discuss the sample file you’ll be using in this lab.\n",
    "<br><br>\n",
    "[Airflow](https://airflow.apache.org/) is a platform to programmatically author, schedule and monitor workflows\n",
    "<br><br>\n",
    "Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies.\n",
    "<br><br>\n",
    "### Core concepts\n",
    "- [DAG](https://airflow.apache.org/concepts.html#dags) - A Directed Acyclic Graph  is a collection of tasks, organised to reflect their relationships and dependencies.\n",
    "- [Operator](https://airflow.apache.org/concepts.html#operators) - The description of a single task, it is usually atomic. For example, the BashOperator is used to execute bash command.\n",
    "- [Task](https://airflow.apache.org/concepts.html#tasks) - A parameterised instance of an Operator;  a node in the DAG.\n",
    "- [Task Instance](https://airflow.apache.org/concepts.html#task-instances) - A specific run of a task; characterised as: a DAG, a Task, and a point in time. It has an indicative state: *running, success, failed, skipped, …*<br><br>\n",
    "The rest of the Airflow concepts can be found [here](https://airflow.apache.org/concepts.html#).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete the DAG file\n",
    "Cloud Composer workflows are comprised of [DAGs (Directed Acyclic Graphs)](https://airflow.incubator.apache.org/concepts.html#dags). The code shown in simple_load_dag.py is the workflow code, also referred to as the DAG. \n",
    "<br><br>\n",
    "Open the file now to see how it is built. Next will be a detailed look at some of the key components of the file.\n",
    "<br><br>\n",
    "To orchestrate all the workflow tasks, the DAG imports the following operators:\n",
    "- DataFlowPythonOperator\n",
    "- PythonOperator\n",
    "<br><br>\n",
    "Action: <span style=\"color:blue\">**Complete the # TODOs in the simple_load_dag.py DAG file below**</span> file while you wait for your Composer environment to be setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting simple_load_dag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile simple_load_dag.py\n",
    "# Copyright 2018 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"A simple Airflow DAG that is triggered externally by a Cloud Function when a\n",
    "file lands in a GCS bucket.\n",
    "Once triggered the DAG performs the following steps:\n",
    "1. Triggers a Google Cloud Dataflow job with the input file information received\n",
    "   from the Cloud Function trigger.\n",
    "2. Upon completion of the Dataflow job, the input file is moved to a\n",
    "   gs://<target-bucket>/<success|failure>/YYYY-MM-DD/ location based on the\n",
    "   status of the previous step.\n",
    "\"\"\"\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from airflow import configuration\n",
    "from airflow import models\n",
    "from airflow.contrib.hooks import gcs_hook\n",
    "from airflow.contrib.operators import dataflow_operator\n",
    "from airflow.operators import python_operator\n",
    "from airflow.utils.trigger_rule import TriggerRule\n",
    "\n",
    "# We set the start_date of the DAG to the previous date. This will\n",
    "# make the DAG immediately available for scheduling.\n",
    "YESTERDAY = datetime.datetime.combine(\n",
    "    datetime.datetime.today() - datetime.timedelta(1),\n",
    "    datetime.datetime.min.time())\n",
    "\n",
    "# We define some variables that we will use in the DAG tasks.\n",
    "SUCCESS_TAG = 'success'\n",
    "FAILURE_TAG = 'failure'\n",
    "\n",
    "# An Airflow variable called gcp_completion_bucket is required.\n",
    "# This variable will contain the name of the bucket to move the processed\n",
    "# file to.\n",
    "\n",
    "# '_names' must appear in CSV filename to be ingested (adjust as needed)\n",
    "# we are only looking for files with the exact name usa_names.csv (you can specify wildcards if you like)\n",
    "INPUT_BUCKET_CSV = 'gs://'+models.Variable.get('gcp_input_location')+'/usa_names.csv' \n",
    "\n",
    "# TODO: Populate the models.Variable.get() with the actual variable name for your output bucket\n",
    "COMPLETION_BUCKET = 'gs://'+models.Variable.get('gcp_completion_bucket')\n",
    "DS_TAG = '{{ ds }}'\n",
    "DATAFLOW_FILE = os.path.join(\n",
    "    configuration.get('core', 'dags_folder'), 'dataflow', 'process_delimited.py')\n",
    "\n",
    "# The following additional Airflow variables should be set:\n",
    "# gcp_project:         Google Cloud Platform project id.\n",
    "# gcp_temp_location:   Google Cloud Storage location to use for Dataflow temp location.\n",
    "DEFAULT_DAG_ARGS = {\n",
    "    'start_date': YESTERDAY,\n",
    "    'retries': 2,\n",
    "\n",
    "    # TODO: Populate the models.Variable.get() with the variable name for your GCP Project\n",
    "    'project_id': models.Variable.get('gcp_project'),\n",
    "    'dataflow_default_options': {\n",
    "        'project': models.Variable.get('gcp_project'),\n",
    "\n",
    "        # TODO: Populate the models.Variable.get() with the variable name for temp location\n",
    "        'temp_location': 'gs://'+models.Variable.get('gcp_temp_location'),\n",
    "        'runner': 'DataflowRunner'\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def move_to_completion_bucket(target_bucket, target_infix, **kwargs):\n",
    "    \"\"\"A utility method to move an object to a target location in GCS.\"\"\"\n",
    "    # Here we establish a connection hook to GoogleCloudStorage.\n",
    "    # Google Cloud Composer automatically provides a google_cloud_storage_default\n",
    "    # connection id that is used by this hook.\n",
    "    conn = gcs_hook.GoogleCloudStorageHook()\n",
    "\n",
    "    # The external trigger (Google Cloud Function) that initiates this DAG\n",
    "    # provides a dag_run.conf dictionary with event attributes that specify\n",
    "    # the information about the GCS object that triggered this DAG.\n",
    "    # We extract the bucket and object name from this dictionary.\n",
    "    source_bucket = models.Variable.get('gcp_input_location')\n",
    "    source_object = models.Variable.get('gcp_input_location')+'/usa_names.csv' \n",
    "    completion_ds = kwargs['ds']\n",
    "\n",
    "    target_object = os.path.join(target_infix, completion_ds, source_object)\n",
    "\n",
    "    logging.info('Copying %s to %s',\n",
    "                 os.path.join(source_bucket, source_object),\n",
    "                 os.path.join(target_bucket, target_object))\n",
    "    conn.copy(source_bucket, source_object, target_bucket, target_object)\n",
    "\n",
    "    logging.info('Deleting %s',\n",
    "                 os.path.join(source_bucket, source_object))\n",
    "    conn.delete(source_bucket, source_object)\n",
    "\n",
    "\n",
    "# Setting schedule_interval to None as this DAG is externally trigger by a Cloud Function.\n",
    "# The following Airflow variables should be set for this DAG to function:\n",
    "# bq_output_table: BigQuery table that should be used as the target for\n",
    "#                  Dataflow in <dataset>.<tablename> format.\n",
    "#                  e.g. lake.usa_names\n",
    "# input_field_names: Comma separated field names for the delimited input file.\n",
    "#                  e.g. state,gender,year,name,number,created_date\n",
    "\n",
    "# TODO: Name the DAG id GcsToBigQueryTriggered\n",
    "with models.DAG(dag_id='GcsToBigQueryTriggered',\n",
    "                description='A DAG triggered by an external Cloud Function',\n",
    "                schedule_interval=None, default_args=DEFAULT_DAG_ARGS) as dag:\n",
    "    # Args required for the Dataflow job.\n",
    "    job_args = {\n",
    "        'input': INPUT_BUCKET_CSV,\n",
    "\n",
    "        # TODO: Populate the models.Variable.get() with the variable name for BQ table\n",
    "        'output': models.Variable.get('bq_output_table'),\n",
    "\n",
    "        # TODO: Populate the models.Variable.get() with the variable name for input field names\n",
    "        'fields': models.Variable.get('input_field_names'),\n",
    "        'load_dt': DS_TAG\n",
    "    }\n",
    "\n",
    "    # Main Dataflow task that will process and load the input delimited file.\n",
    "    # TODO: Specify the type of operator we need to call to invoke DataFlow\n",
    "    dataflow_task = dataflow_operator.DataFlowPythonOperator(\n",
    "        task_id=\"process-delimited-and-push\",\n",
    "        py_file=DATAFLOW_FILE,\n",
    "        options=job_args)\n",
    "\n",
    "    # Here we create two conditional tasks, one of which will be executed\n",
    "    # based on whether the dataflow_task was a success or a failure.\n",
    "    success_move_task = python_operator.PythonOperator(task_id='success-move-to-completion',\n",
    "                                                       python_callable=move_to_completion_bucket,\n",
    "                                                       # A success_tag is used to move\n",
    "                                                       # the input file to a success\n",
    "                                                       # prefixed folder.\n",
    "                                                       op_args=[models.Variable.get('gcp_completion_bucket'), SUCCESS_TAG],\n",
    "                                                       provide_context=True,\n",
    "                                                       trigger_rule=TriggerRule.ALL_SUCCESS)\n",
    "\n",
    "    failure_move_task = python_operator.PythonOperator(task_id='failure-move-to-completion',\n",
    "                                                       python_callable=move_to_completion_bucket,\n",
    "                                                       # A failure_tag is used to move\n",
    "                                                       # the input file to a failure\n",
    "                                                       # prefixed folder.\n",
    "                                                       op_args=[models.Variable.get('gcp_completion_bucket'), FAILURE_TAG],\n",
    "                                                       provide_context=True,\n",
    "                                                       trigger_rule=TriggerRule.ALL_FAILED)\n",
    "\n",
    "    # The success_move_task and failure_move_task are both downstream from the\n",
    "    # dataflow_task.\n",
    "    dataflow_task >> success_move_task\n",
    "    dataflow_task >> failure_move_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing environment information\n",
    "Now that you have a completed DAG, it's time to copy it to your Cloud Composer environment and finish the setup of your workflow.<br><br>\n",
    "1. Go back to **Composer** to check on the status of your environment.\n",
    "2. Once your environment has been created, click the **name of the environment** to see its details.\n",
    "<br><br>\n",
    "The Environment details page provides information, such as the Airflow web UI URL, Google Kubernetes Engine cluster ID, name of the Cloud Storage bucket connected to the DAGs folder.\n",
    "<br><br>\n",
    "Cloud Composer uses Cloud Storage to store Apache Airflow DAGs, also known as workflows. Each environment has an associated Cloud Storage bucket. Cloud Composer schedules only the DAGs in the Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Airflow variables\n",
    "Our DAG relies on variables to pass in values like the GCP Project. We can set these in the Admin UI.\n",
    "\n",
    "Airflow variables are an Airflow-specific concept that is distinct from [environment variables](https://cloud.google.com/composer/docs/how-to/managing/environment-variables). In this step, you'll set the following six [Airflow variables](https://airflow.apache.org/concepts.html#variables) used by the DAG we will deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this to display which key value pairs to input\n",
    "import pandas as pd\n",
    "pd.DataFrame([\n",
    "  ('gcp_project', PROJECT),\n",
    "  ('gcp_input_location', PROJECT + '_input'),\n",
    "  ('gcp_temp_location', PROJECT + '_output/tmp'),\n",
    "  ('gcp_completion_bucket', PROJECT + '_output'),\n",
    "  ('input_field_names', 'state,gender,year,name,number,created_date'),\n",
    "  ('bq_output_table', 'ml_pipeline.ingest_table')\n",
    "], columns = ['Key', 'Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Set the variables using the Airflow webserver UI\n",
    "1. In your Airflow environment, select **Admin** > **Variables**\n",
    "2. Populate each key value in the table with the required variables from the above table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Set the variables using the Airflow CLI\n",
    "The next gcloud composer command executes the Airflow CLI sub-command [variables](https://airflow.apache.org/cli.html#variables). The sub-command passes the arguments to the gcloud command line tool.<br><br>\n",
    "To set the three variables, run the gcloud composer command **once for each row** from the above table. Just as an example, to set the variable `gcp_project` you could do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud composer environments run ENVIRONMENT_NAME \\\n",
    " --location ${REGION} variables -- \\\n",
    " --set gcp_project ${PROJECT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy your Airflow bucket name\n",
    "1. Navigate to your Cloud Composer [instance](https://console.cloud.google.com/composer/environments?project=)<br/><br/>\n",
    "2. Select __DAGs Folder__<br/><br/>\n",
    "3. You will be taken to the Google Cloud Storage bucket that Cloud Composer has created automatically for your Airflow instance<br/><br/>\n",
    "4. __Copy the bucket name__ into the variable below (example: us-central1-composer-08f6edeb-bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIRFLOW_BUCKET = 'us-central1-composer-21587538-bucket' # REPLACE WITH AIRFLOW BUCKET NAME\n",
    "os.environ['AIRFLOW_BUCKET'] = AIRFLOW_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy your Airflow files to your Airflow bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud storage cp simple_load_dag.py gs://${AIRFLOW_BUCKET}/dags # overwrite DAG file if it exists\n",
    "gcloud storage cp --recursive dataflow/process_delimited.py gs://${AIRFLOW_BUCKET}/dags/dataflow/ # copy Dataflow job to be ran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Navigating Using the Airflow UI\n",
    "To access the Airflow web interface using the GCP Console:\n",
    "1. Go back to the **Composer Environments** page.\n",
    "2. In the **Airflow webserver** column for the environment, click the new window icon. \n",
    "3. The Airflow web UI opens in a new browser window. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger DAG run manually\n",
    "Running your DAG manually ensures that it operates successfully even in the absence of triggered events. \n",
    "1. Trigger the DAG manually **click the play button** under Links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Part Two: Trigger DAG run automatically from a file upload to GCS\n",
    "Now that your manual workflow runs successfully, you will now trigger it based on an external event. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Cloud Function to trigger your workflow\n",
    "We will be following this [reference guide](https://cloud.google.com/composer/docs/how-to/using/triggering-with-gcf) to setup our Cloud Function\n",
    "1. In the code block below, uncomment the project_id, location, and composer_environment and populate them\n",
    "2. Run the below code to get your **CLIENT_ID** (needed later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "import requests\n",
    "import six.moves.urllib.parse\n",
    "\n",
    "# Authenticate with Google Cloud.\n",
    "# See: https://cloud.google.com/docs/authentication/getting-started\n",
    "credentials, _ = google.auth.default(\n",
    "    scopes=['https://www.googleapis.com/auth/cloud-platform'])\n",
    "authed_session = google.auth.transport.requests.AuthorizedSession(\n",
    "    credentials)\n",
    "\n",
    "project_id = 'your-project-id'\n",
    "location = 'us-central1'\n",
    "composer_environment = 'composer'\n",
    "\n",
    "environment_url = (\n",
    "    'https://composer.googleapis.com/v1beta1/projects/{}/locations/{}'\n",
    "    '/environments/{}').format(project_id, location, composer_environment)\n",
    "composer_response = authed_session.request('GET', environment_url)\n",
    "environment_data = composer_response.json()\n",
    "airflow_uri = environment_data['config']['airflowUri']\n",
    "\n",
    "# The Composer environment response does not include the IAP client ID.\n",
    "# Make a second, unauthenticated HTTP request to the web server to get the\n",
    "# redirect URI.\n",
    "redirect_response = requests.get(airflow_uri, allow_redirects=False)\n",
    "redirect_location = redirect_response.headers['location']\n",
    "\n",
    "# Extract the client_id query parameter from the redirect.\n",
    "parsed = six.moves.urllib.parse.urlparse(redirect_location)\n",
    "query_string = six.moves.urllib.parse.parse_qs(parsed.query)\n",
    "print(query_string['client_id'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grant Service Account Permissions\n",
    "\n",
    "To authenticate to Cloud IAP, grant the Appspot Service Account (used by Cloud Functions) the Service Account Token Creator role on itself. To do this, execute the following command in [Cloud Shell](https://console.cloud.google.com/?cloudshell=true). Be sure to replace 'your-project-id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute the following in Cloud Shell, it will not work here\n",
    "gcloud iam service-accounts add-iam-policy-binding \\\n",
    "your-project-id@appspot.gserviceaccount.com \\\n",
    "--member=serviceAccount:your-project-id@appspot.gserviceaccount.com \\\n",
    "--role=roles/iam.serviceAccountTokenCreator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Cloud Function\n",
    "\n",
    "1. Navigate to Compute > **Cloud Functions**\n",
    "2. Select **Create function**\n",
    "3. For name specify **'gcs-dag-trigger-function'**\n",
    "4. For trigger type select **'Cloud Storage'**\n",
    "5. For event type select '**Finalize/Create'**\n",
    "6. For bucket, **specify the input bucket** you created earlier \n",
    "\n",
    "Important: be sure to select the input bucket and not the output bucket to avoid an endless triggering loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### populate index.js\n",
    "Complete the four required constants defined below in index.js code and **paste it into the Cloud Function editor** (the js code will not run in this notebook). The constants are: \n",
    "- PROJECT_ID\n",
    "- CLIENT_ID (from earlier)\n",
    "- WEBSERVER_ID (part of Airflow webserver URL) \n",
    "- DAG_NAME (GcsToBigQueryTriggered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'use strict';\n",
    "\n",
    "const fetch = require('node-fetch');\n",
    "const FormData = require('form-data');\n",
    "\n",
    "/**\n",
    " * Triggered from a message on a Cloud Storage bucket.\n",
    " *\n",
    " * IAP authorization based on:\n",
    " * https://stackoverflow.com/questions/45787676/how-to-authenticate-google-cloud-functions-for-access-to-secure-app-engine-endpo\n",
    " * and\n",
    " * https://cloud.google.com/iap/docs/authentication-howto\n",
    " *\n",
    " * @param {!Object} data The Cloud Functions event data.\n",
    " * @returns {Promise}\n",
    " */\n",
    "exports.triggerDag = async data => {\n",
    "  // Fill in your Composer environment information here.\n",
    "\n",
    "  // The project that holds your function\n",
    "  const PROJECT_ID = 'your-project-id';\n",
    "  // Navigate to your webserver's login page and get this from the URL\n",
    "  const CLIENT_ID = 'your-iap-client-id';\n",
    "  // This should be part of your webserver's URL:\n",
    "  // {tenant-project-id}.appspot.com\n",
    "  const WEBSERVER_ID = 'your-tenant-project-id';\n",
    "  // The name of the DAG you wish to trigger\n",
    "  const DAG_NAME = 'GcsToBigQueryTriggered';\n",
    "\n",
    "  // Other constants\n",
    "  const WEBSERVER_URL = `https://${WEBSERVER_ID}.appspot.com/api/experimental/dags/${DAG_NAME}/dag_runs`;\n",
    "  const USER_AGENT = 'gcf-event-trigger';\n",
    "  const BODY = {conf: JSON.stringify(data)};\n",
    "\n",
    "  // Make the request\n",
    "  try {\n",
    "    const iap = await authorizeIap(CLIENT_ID, PROJECT_ID, USER_AGENT);\n",
    "\n",
    "    return makeIapPostRequest(\n",
    "      WEBSERVER_URL,\n",
    "      BODY,\n",
    "      iap.idToken,\n",
    "      USER_AGENT,\n",
    "      iap.jwt\n",
    "    );\n",
    "  } catch (err) {\n",
    "    throw new Error(err);\n",
    "  }\n",
    "};\n",
    "\n",
    "/**\n",
    " * @param {string} clientId The client id associated with the Composer webserver application.\n",
    " * @param {string} projectId The id for the project containing the Cloud Function.\n",
    " * @param {string} userAgent The user agent string which will be provided with the webserver request.\n",
    " */\n",
    "const authorizeIap = async (clientId, projectId, userAgent) => {\n",
    "  const SERVICE_ACCOUNT = `${projectId}@appspot.gserviceaccount.com`;\n",
    "  const JWT_HEADER = Buffer.from(\n",
    "    JSON.stringify({alg: 'RS256', typ: 'JWT'})\n",
    "  ).toString('base64');\n",
    "\n",
    "  let jwt = '';\n",
    "  let jwtClaimset = '';\n",
    "\n",
    "  // Obtain an Oauth2 access token for the appspot service account\n",
    "  const res = await fetch(\n",
    "    `http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/${SERVICE_ACCOUNT}/token`,\n",
    "    {\n",
    "      headers: {'User-Agent': userAgent, 'Metadata-Flavor': 'Google'},\n",
    "    }\n",
    "  );\n",
    "  const tokenResponse = await res.json();\n",
    "  if (tokenResponse.error) {\n",
    "    return Promise.reject(tokenResponse.error);\n",
    "  }\n",
    "\n",
    "  const accessToken = tokenResponse.access_token;\n",
    "  const iat = Math.floor(new Date().getTime() / 1000);\n",
    "  const claims = {\n",
    "    iss: SERVICE_ACCOUNT,\n",
    "    aud: 'https://www.googleapis.com/oauth2/v4/token',\n",
    "    iat: iat,\n",
    "    exp: iat + 60,\n",
    "    target_audience: clientId,\n",
    "  };\n",
    "  jwtClaimset = Buffer.from(JSON.stringify(claims)).toString('base64');\n",
    "  const toSign = [JWT_HEADER, jwtClaimset].join('.');\n",
    "\n",
    "  const blob = await fetch(\n",
    "    `https://iam.googleapis.com/v1/projects/${projectId}/serviceAccounts/${SERVICE_ACCOUNT}:signBlob`,\n",
    "    {\n",
    "      method: 'POST',\n",
    "      body: JSON.stringify({\n",
    "        bytesToSign: Buffer.from(toSign).toString('base64'),\n",
    "      }),\n",
    "      headers: {\n",
    "        'User-Agent': userAgent,\n",
    "        Authorization: `Bearer ${accessToken}`,\n",
    "      },\n",
    "    }\n",
    "  );\n",
    "  const blobJson = await blob.json();\n",
    "  if (blobJson.error) {\n",
    "    return Promise.reject(blobJson.error);\n",
    "  }\n",
    "\n",
    "  // Request service account signature on header and claimset\n",
    "  const jwtSignature = blobJson.signature;\n",
    "  jwt = [JWT_HEADER, jwtClaimset, jwtSignature].join('.');\n",
    "  const form = new FormData();\n",
    "  form.append('grant_type', 'urn:ietf:params:oauth:grant-type:jwt-bearer');\n",
    "  form.append('assertion', jwt);\n",
    "\n",
    "  const token = await fetch('https://www.googleapis.com/oauth2/v4/token', {\n",
    "    method: 'POST',\n",
    "    body: form,\n",
    "  });\n",
    "  const tokenJson = await token.json();\n",
    "  if (tokenJson.error) {\n",
    "    return Promise.reject(tokenJson.error);\n",
    "  }\n",
    "\n",
    "  return {\n",
    "    jwt: jwt,\n",
    "    idToken: tokenJson.id_token,\n",
    "  };\n",
    "};\n",
    "\n",
    "/**\n",
    " * @param {string} url The url that the post request targets.\n",
    " * @param {string} body The body of the post request.\n",
    " * @param {string} idToken Bearer token used to authorize the iap request.\n",
    " * @param {string} userAgent The user agent to identify the requester.\n",
    " */\n",
    "const makeIapPostRequest = async (url, body, idToken, userAgent) => {\n",
    "  const res = await fetch(url, {\n",
    "    method: 'POST',\n",
    "    headers: {\n",
    "      'User-Agent': userAgent,\n",
    "      Authorization: `Bearer ${idToken}`,\n",
    "    },\n",
    "    body: JSON.stringify(body),\n",
    "  });\n",
    "\n",
    "  if (!res.ok) {\n",
    "    const err = await res.text();\n",
    "    throw new Error(err);\n",
    "  }\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### populate package.json\n",
    "Copy and paste the below into **package.json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"name\": \"nodejs-docs-samples-functions-composer-storage-trigger\",\n",
    "  \"version\": \"0.0.1\",\n",
    "  \"dependencies\": {\n",
    "    \"form-data\": \"^2.3.2\",\n",
    "    \"node-fetch\": \"^2.2.0\"\n",
    "  },\n",
    "  \"engines\": {\n",
    "    \"node\": \">=8.0.0\"\n",
    "  },\n",
    "  \"private\": true,\n",
    "  \"license\": \"Apache-2.0\",\n",
    "  \"author\": \"Google Inc.\",\n",
    "  \"repository\": {\n",
    "    \"type\": \"git\",\n",
    "    \"url\": \"https://github.com/GoogleCloudPlatform/nodejs-docs-samples.git\"\n",
    "  },\n",
    "  \"devDependencies\": {\n",
    "    \"@google-cloud/nodejs-repo-tools\": \"^3.3.0\",\n",
    "    \"mocha\": \"^6.0.0\",\n",
    "    \"proxyquire\": \"^2.1.0\",\n",
    "    \"sinon\": \"^7.2.7\"\n",
    "  },\n",
    "  \"scripts\": {\n",
    "    \"test\": \"mocha test/*.test.js --timeout=20000\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. For **Function to execute**, specify **triggerDag** (note: case sensitive)\n",
    "11. Select **Create**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload CSVs and Monitor\n",
    "1. Practice uploading and editing CSVs named usa_names.csv into your input bucket (note: the DAG filters to only ingest CSVs with 'usa_names.csv' as the filepath. Adjust this as needed in the DAG code.)\n",
    "2. Troubleshoot Cloud Function call errors by monitoring the [logs](https://console.cloud.google.com/logs/viewer?). In the below screenshot we filter in Logging for our most recent Dataflow job and are scrolling through to ensure the job is processing and outputting records to BigQuery\n",
    "\n",
    "![Dataflow logging](./img/dataflow_logging.jpg \"Dataflow logging\")\n",
    "\n",
    "3. Troubleshoot Airflow workflow errors by monitoring the **Browse** > **DAG Runs** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! \n",
    "You’ve have completed this advanced lab on triggering a workflow with a Cloud Function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
