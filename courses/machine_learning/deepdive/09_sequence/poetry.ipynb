{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation using tensor2tensor on Cloud ML Engine\n",
    "\n",
    "This notebook illustrates using the <a href=\"https://github.com/tensorflow/tensor2tensor\">tensor2tensor</a> library to do from-scratch, distributed training of a poetry model. Then, the trained model is used to complete new poems.\n",
    "<p/>\n",
    "### Install tensor2tensor, and specify Google Cloud Platform project and bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the necessary packages. tensor2tensor will give us the Transformer model. Project Gutenberg gives us access to historical poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow==1.4.1\n",
      "tensorflow-tensorboard==0.4.0rc3\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "pip freeze | grep tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "pip install --upgrade tensor2tensor gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor2tensor==1.4.2\n",
      "tensorflow==1.4.1\n",
      "tensorflow-tensorboard==0.4.0rc3\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "pip freeze | grep tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'cloud-training-demos' # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = 'cloud-training-demos-ml' # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = 'us-central1' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "\n",
    "# this is what this notebook is demonstrating\n",
    "PROBLEM= 'poetry_line_problem'\n",
    "\n",
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['PROBLEM'] = PROBLEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\r\n",
      "\r\n",
      "\r\n",
      "Updates are available for some Cloud SDK components.  To install them,\r\n",
      "please run:\r\n",
      "  $ gcloud components update\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "We will get some <a href=\"https://www.gutenberg.org/wiki/Poetry_(Bookshelf)\">poetry anthologies</a> from Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf data/poetry\n",
    "mkdir -p data/poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gutenberg.acquire import load_etext\n",
    "from gutenberg.cleanup import strip_headers\n",
    "import re\n",
    "\n",
    "books = [\n",
    "  # bookid, skip N lines\n",
    "  (19221, 223),\n",
    "  (15553, 522) \n",
    "]\n",
    "\n",
    "with open('data/poetry/raw.txt', 'w') as ofp:\n",
    "  for (id_nr, toskip) in books:\n",
    "    text = strip_headers(load_etext(id_nr)).strip()\n",
    "    lines = text.split('\\n')[toskip:]\n",
    "    # any line that is all upper case is a title or author name\n",
    "    for line in lines:\n",
    "      if len(line) > 0 and line.upper() != line:\n",
    "        cleaned = re.sub('[^a-z\\'\\-]+', ' ', line.strip().lower())\n",
    "        ofp.write(cleaned)\n",
    "        ofp.write('\\n')\n",
    "      else:\n",
    "        ofp.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22544 data/poetry/raw.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/poetry/*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training dataset\n",
    "\n",
    "We are going to train a machine learning model to write poetry given a starting point. We'll give it one line, and it is going to tell us the next line.  So, naturally, we will train it on real poetry. Our feature will be a line of a poem and the label will be next line of that poem.\n",
    "<p>\n",
    "Our training dataset will consist of two files.  The first file will consist of the input lines of poetry and the other file will consist of the corresponding output lines, one output line per input line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import random\n",
    "with open('data/poetry/raw.txt', 'r') as rawfp,\\\n",
    "  open('data/poetry/train_input.txt', 'w') as train_infp,\\\n",
    "  open('data/poetry/train_output.txt', 'w') as train_outfp,\\\n",
    "  open('data/poetry/test_input.txt', 'w') as test_infp,\\\n",
    "  open('data/poetry/test_output.txt', 'w') as test_outfp:\n",
    "    \n",
    "    for curr_line in rawfp:\n",
    "        curr_line = curr_line.strip()\n",
    "        # poems break at empty lines, so this ensures we train only\n",
    "        # on lines of the same poem\n",
    "        if len(prev_line) > 0 and len(curr_line) > 0:\n",
    "          if random() < 0.9:        \n",
    "            train_infp.write(prev_line + '\\n')\n",
    "            train_outfp.write(curr_line + '\\n')\n",
    "          else:        \n",
    "            test_infp.write(prev_line + '\\n')\n",
    "            test_outfp.write(curr_line + '\\n')\n",
    "        prev_line = curr_line      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> data/poetry/raw.txt <==\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "spring the sweet spring is the year's pleasant king \r\n",
      "then blooms each thing then maids dance in a ring \r\n",
      "\r\n",
      "==> data/poetry/test_input.txt <==\r\n",
      "phoebus arise\r\n",
      "that she may thy career with roses spread\r\n",
      "your furious chiding stay\r\n",
      "when i have seen the hungry ocean gain\r\n",
      "when i have seen such interchange of state\r\n",
      "\r\n",
      "==> data/poetry/test_output.txt <==\r\n",
      "and paint the sable skies\r\n",
      "the nightingales thy coming eachwhere sing\r\n",
      "let zephyr only breathe\r\n",
      "advantage on the kingdom of the shore\r\n",
      "or state itself confounded to decay\r\n",
      "\r\n",
      "==> data/poetry/train_input.txt <==\r\n",
      "spring the sweet spring is the year's pleasant king\r\n",
      "then blooms each thing then maids dance in a ring\r\n",
      "cold doth not sting the pretty birds do sing\r\n",
      "the palm and may make country houses gay\r\n",
      "lambs frisk and play the shepherds pipe all day\r\n",
      "\r\n",
      "==> data/poetry/train_output.txt <==\r\n",
      "then blooms each thing then maids dance in a ring\r\n",
      "cold doth not sting the pretty birds do sing\r\n",
      "cuckoo jug-jug pu-we to-witta-woo\r\n",
      "lambs frisk and play the shepherds pipe all day\r\n",
      "and we hear aye birds tune their merry lay\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 data/poetry/*.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/poetry/\n",
      "data/poetry/raw.txt\n",
      "data/poetry/test_input.txt\n",
      "data/poetry/test_output.txt\n",
      "data/poetry/train_input.txt\n",
      "data/poetry/train_output.txt\n"
     ]
    }
   ],
   "source": [
    "!tar cvfz poetrydata.tgz data/poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up problem\n",
    "The Problem in tensor2tensor is where you specify parameters like the size of your vocabulary and where to get the training data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf poetry\n",
    "mkdir -p poetry/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting poetry/trainer/problem.py\n"
     ]
    }
   ],
   "source": [
    "%writefile poetry/trainer/problem.py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensor2tensor.data_generators import generator_utils\n",
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "from tensor2tensor.data_generators import translate\n",
    "from tensor2tensor.models import transformer\n",
    "from tensor2tensor.utils import registry\n",
    "\n",
    "TRAIN_DATASETS = [\n",
    "    [\n",
    "        \"{}/poetrydata.tgz\".format(os.getcwd()),\n",
    "        (\"data/poetry/train_input.txt\",\n",
    "         \"data/poetry/train_output.txt\")\n",
    "    ],\n",
    "]\n",
    "TEST_DATASETS = [\n",
    "    [\n",
    "        \"{}/poetrydata.tgz\".format(os.getcwd()),\n",
    "        (\"data/poetry/test_input.txt\",\n",
    "         \"data/poetry/test_output.txt\")\n",
    "    ],\n",
    "]\n",
    "\n",
    "@registry.register_problem\n",
    "class PoetryLineProblem(translate.TranslateProblem):\n",
    "  @property\n",
    "  def targeted_vocab_size(self):\n",
    "    return 2**12  # 4096\n",
    "\n",
    "  @property\n",
    "  def vocab_name(self):\n",
    "    return \"vocab.poetry_anthology\"\n",
    "  \n",
    "  def generator(self, data_dir, tmp_dir, train):\n",
    "    symbolizer_vocab = generator_utils.get_or_generate_vocab(\n",
    "        data_dir, tmp_dir, self.vocab_file, self.targeted_vocab_size, sources=TRAIN_DATASETS)\n",
    "    datasets = TRAIN_DATASETS if train else TEST_DATASETS\n",
    "    tag = \"train\" if train else \"dev\"\n",
    "    data_path = translate.compile_data(tmp_dir, datasets, \"wmt_ende_tok_%s\" % tag)\n",
    "    return translate.token_generator(data_path + \".lang1\", data_path + \".lang2\",\n",
    "                           symbolizer_vocab, text_encoder.EOS_ID)\n",
    "\n",
    "  @property\n",
    "  def input_space_id(self):\n",
    "    return problem.SpaceID.EN_TOK\n",
    "\n",
    "  @property\n",
    "  def target_space_id(self):\n",
    "    return problem.SpaceID.EN_TOK\n",
    "\n",
    "# Smaller than the typical translate model, and with more regularization\n",
    "@registry.register_hparams\n",
    "def transformer_poetry():\n",
    "  hparams = transformer.transformer_base()\n",
    "  hparams.num_hidden_layers = 2\n",
    "  hparams.hidden_size = 128\n",
    "  hparams.filter_size = 512\n",
    "  hparams.num_heads = 4\n",
    "  hparams.attention_dropout = 0.6\n",
    "  hparams.layer_prepostprocess_dropout = 0.6\n",
    "  hparams.learning_rate = 0.05\n",
    "  return hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing poetry/trainer/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile poetry/trainer/__init__.py\n",
    "from . import problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing poetry/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile poetry/setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "  'tensor2tensor'\n",
    "]\n",
    "\n",
    "setup(\n",
    "    name='poetry',\n",
    "    version='0.1',\n",
    "    author = 'Google',\n",
    "    author_email = 'training-feedback@cloud.google.com',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Poetry Line Problem',\n",
    "    requires=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training data \n",
    "\n",
    "Our problem (translation) requires the creation of text sequences from the training dataset.  This is done using t2t-datagen and the Problem defined in the previous section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "DATA_DIR=./t2t_data\n",
    "TMP_DIR=$DATA_DIR/tmp\n",
    "rm -rf $DATA_DIR $TMP_DIR\n",
    "mkdir -p $DATA_DIR $TMP_DIR\n",
    "# Generate data\n",
    "t2t-datagen \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --data_dir=$DATA_DIR \\\n",
    "  --tmp_dir=$TMP_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide Cloud ML Engine access to data\n",
    "\n",
    "Copy the data to Google Cloud Storage, and then provide access to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "DATA_DIR=./t2t_data\n",
    "gsutil -m rm -r gs://${BUCKET}/poetry/\n",
    "gsutil -m cp ${DATA_DIR}/${PROBLEM}* ${DATA_DIR}/vocab* gs://${BUCKET}/poetry/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "PROJECT_ID=$PROJECT\n",
    "AUTH_TOKEN=$(gcloud auth print-access-token)\n",
    "SVC_ACCOUNT=$(curl -X GET -H \"Content-Type: application/json\" \\\n",
    "    -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n",
    "    https://ml.googleapis.com/v1/projects/${PROJECT_ID}:getConfig \\\n",
    "    | python -c \"import json; import sys; response = json.load(sys.stdin); \\\n",
    "    print response['serviceAccount']\")\n",
    "\n",
    "echo \"Authorizing the Cloud ML Service account $SVC_ACCOUNT to access files in $BUCKET\"\n",
    "gsutil -m defacl ch -u $SVC_ACCOUNT:R gs://$BUCKET\n",
    "gsutil -m acl ch -u $SVC_ACCOUNT:R -r gs://$BUCKET  # error message (if bucket is empty) can be ignored\n",
    "gsutil -m acl ch -u $SVC_ACCOUNT:W gs://$BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model as a Python package\n",
    "\n",
    "To submit the training job to Cloud Machine Learning Engine, we need a Python module with a main(). We'll use the t2t-trainer that is distributed with tensor2tensor as the main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "wget https://raw.githubusercontent.com/tensorflow/tensor2tensor/master/tensor2tensor/bin/t2t-trainer\n",
    "mv t2t-trainer poetry/trainer/t2t-trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!touch poetry/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poetry\r\n",
      "poetry/__init__.py\r\n",
      "poetry/setup.py\r\n",
      "poetry/trainer\r\n",
      "poetry/trainer/__init__.py\r\n",
      "poetry/trainer/__init__.pyc\r\n",
      "poetry/trainer/problem.py\r\n",
      "poetry/trainer/problem.pyc\r\n",
      "poetry/trainer/t2t-trainer.py\r\n"
     ]
    }
   ],
   "source": [
    "!find poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test that the Python package works. Since we are running this locally, I'll try it out on a subset of the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "BASE=gs://${BUCKET}/poetry/data\n",
    "OUTDIR=gs://${BUCKET}/poetry/subset\n",
    "gsutil -m rm -r $OUTDIR\n",
    "gsutil -m cp \\\n",
    "    ${BASE}/${PROBLEM}-train-0008* \\\n",
    "    ${BASE}/${PROBLEM}-dev-00000*  \\\n",
    "    ${BASE}/vocab* \\\n",
    "    $OUTDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the following will work only if you are running Datalab on a beefy machine, for example, if you started Datalab  on a machine with a GPU.  Otherwise, don't be alarmed if your process is killed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "OUTDIR=./trained_model\n",
    "rm -rf $OUTDIR\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/poetry\n",
    "python -m trainer.t2t-trainer \\\n",
    "  --data_dir=gs://${BUCKET}/poetry/subset \\\n",
    "  --problems=$PROBLEM \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_poetry \\\n",
    "  --output_dir=$OUTDIR --job-dir=$OUTDIR --train_steps=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on Cloud ML Engine\n",
    "\n",
    "Once we have a working Python package, training on a Cloud ML Engine GPU is straightforward.\n",
    "To run on a single GPU, you would specify \n",
    "```\n",
    "--scale-tier=BASIC_GPU\n",
    "...\n",
    "--train_steps=5000\n",
    "--worker_gpu=1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "OUTDIR=gs://${BUCKET}/poetry/model\n",
    "JOBNAME=poetry_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --scale-tier=BASIC_GPU \\\n",
    "   --module-name=trainer.t2t-trainer \\\n",
    "   --package-path=${PWD}/poetry/trainer \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --runtime-version=1.4 \\\n",
    "   -- \\\n",
    "  --data_dir=gs://${BUCKET}/poetry/data \\\n",
    "  --problems=$PROBLEM \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_poetry \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --train_steps=5000 --worker_gpu=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job took about <b>20 minutes</b> for me and ended with these evaluation metrics:\n",
    "<pre>\n",
    "Saving dict for global step 6000: global_step = 6000, loss = 4.98682, metrics-poetry_line_problem/accuracy = 0.191315, metrics-poetry_line_problem/accuracy_per_sequence = 0.0, metrics-poetry_line_problem/accuracy_top5 = 0.319305, metrics-poetry_line_problem/approx_bleu_score = 0.00794831, metrics-poetry_line_problem/neg_log_perplexity = -5.50358, metrics-poetry_line_problem/rouge_2_fscore = 0.0171307, metrics-poetry_line_problem/rouge_L_fscore = 0.187759\n",
    "</pre>\n",
    "Notice that accuracy_per_sequence is 0, which seems about right, considering that we are asking the NN to be rather creative. Why am I looking at accuracy_per_sequence and not the other metrics? This is because it is more appropriate for problem we are solving; metrics like Bleu score are better for translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil ls gs://${BUCKET}/poetry/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training longer\n",
    "\n",
    "Let's train on 4 GPUs for 75,000 steps. Does the model improve?\n",
    "Note the change from above; I am specifying:\n",
    "```\n",
    "--scale-tier=CUSTOM --config four_gpus.json \n",
    "...\n",
    "--train_steps=75000\n",
    "--worker_gpu=4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing four_gpus.json\n"
     ]
    }
   ],
   "source": [
    "%writefile four_gpus.json\n",
    "{\n",
    "  \"trainingInput\": {\n",
    "    \"scaleTier\": \"CUSTOM\",\n",
    "    \"masterType\": \"complex_model_m_gpu\",\n",
    "  },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "XXX  This takes 12 hours on 4 GPUs. Remove this line if you are sure you want to do this.\n",
    "\n",
    "%bash\n",
    "OUTDIR=gs://${BUCKET}/poetry/model_full\n",
    "JOBNAME=poetry_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --scale-tier=CUSTOM --config four_gpus.json \\\n",
    "   --module-name=trainer.t2t-trainer \\\n",
    "   --package-path=${PWD}/poetry/trainer \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --runtime-version=1.4 \\\n",
    "   -- \\\n",
    "  --data_dir=gs://${BUCKET}/poetry/data \\\n",
    "  --problems=$PROBLEM \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_poetry \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --train_steps=75000 --worker_gpu=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This job took <b>12 hours</b> for me and ended with these metrics:\n",
    "<pre>\n",
    "global_step = 76000, loss = 4.99763, metrics-poetry_line_problem/accuracy = 0.219792, metrics-poetry_line_problem/accuracy_per_sequence = 0.0192308, metrics-poetry_line_problem/accuracy_top5 = 0.37618, metrics-poetry_line_problem/approx_bleu_score = 0.017955, metrics-poetry_line_problem/neg_log_perplexity = -5.38725, metrics-poetry_line_problem/rouge_2_fscore = 0.0325563, metrics-poetry_line_problem/rouge_L_fscore = 0.210618\n",
    "</pre>\n",
    "At least the accuracy per sequence is no longer zero. It is now 0.0192308 ... note that we are using a relatively small dataset (12K lines) and this is *tiny* in the world of natural language problems.\n",
    "<p>\n",
    "In order that you have your expectations set correctly: a high-performing translation model needs 400-million lines of input and takes 1 whole day on a TPU pod!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch-predict\n",
    "\n",
    "How will our poetry model do when faced with Rumi's spiritual couplets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/poetry/rumi.txt\n"
     ]
    }
   ],
   "source": [
    "%writefile data/poetry/rumi.txt\n",
    "Where did the handsome beloved go?\n",
    "I wonder, where did that tall, shapely cypress tree go?\n",
    "He spread his light among us like a candle.\n",
    "Where did he go? So strange, where did he go without me?\n",
    "All day long my heart trembles like a leaf.\n",
    "All alone at midnight, where did that beloved go?\n",
    "Go to the road, and ask any passing traveler — \n",
    "That soul-stirring companion, where did he go?\n",
    "Go to the garden, and ask the gardener — \n",
    "That tall, shapely rose stem, where did he go?\n",
    "Go to the rooftop, and ask the watchman — \n",
    "That unique sultan, where did he go?\n",
    "Like a madman, I search in the meadows!\n",
    "That deer in the meadows, where did he go?\n",
    "My tearful eyes overflow like a river — \n",
    "That pearl in the vast sea, where did he go?\n",
    "All night long, I implore both moon and Venus — \n",
    "That lovely face, like a moon, where did he go?\n",
    "If he is mine, why is he with others?\n",
    "Since he’s not here, to what “there” did he go?\n",
    "If his heart and soul are joined with God,\n",
    "And he left this realm of earth and water, where did he go?\n",
    "Tell me clearly, Shams of Tabriz,\n",
    "Of whom it is said, “The sun never dies” — where did he go?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write out the odd-numbered lines. We'll compare how close our model can get to the beauty of Rumi's second lines given his first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where did the handsome beloved go\n",
      "he spread his light among us like a candle\n",
      "all day long my heart trembles like a leaf\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "awk 'NR % 2 == 1' data/poetry/rumi.txt | tr '[:upper:]' '[:lower:]' | sed \"s/[^a-z\\'-\\ ]//g\" > data/poetry/rumi_leads.txt\n",
    "head -3 data/poetry/rumi_leads.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "# same as the above training job ...\n",
    "TOPDIR=gs://${BUCKET}\n",
    "OUTDIR=${TOPDIR}/poetry/model_full  # or ${TOPDIR}/poetry/model\n",
    "DATADIR=${TOPDIR}/poetry/data\n",
    "MODEL=transformer\n",
    "HPARAMS=transformer_poetry\n",
    "\n",
    "# the file with the input lines\n",
    "DECODE_FILE=data/poetry/rumi_leads.txt\n",
    "\n",
    "BEAM_SIZE=4\n",
    "ALPHA=0.6\n",
    "\n",
    "t2t-decoder \\\n",
    "  --data_dir=$DATADIR \\\n",
    "  --problems=$PROBLEM \\\n",
    "  --model=$MODEL \\\n",
    "  --hparams_set=$HPARAMS \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --decode_hparams=\"beam_size=$BEAM_SIZE,alpha=$ALPHA\" \\\n",
    "  --decode_from_file=$DECODE_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Note </b> if you get an error about \"AttributeError: 'HParams' object has no attribute 'problems'\" please <b>Reset Session</b>, run the cell that defines the PROBLEM and run the above cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and the old familiar faces\n",
      "and gave him low\n",
      "i'll borrow\n",
      "the rapid of the valleys of hall\n",
      "and all the world is gone\n",
      "and let me to thee\n",
      "i'll borrow\n",
      "and thou art thou art gone\n",
      "and nothing more\n",
      "a famous victory\n",
      "and he is marching on his hard heart\n",
      "and many a passage\n"
     ]
    }
   ],
   "source": [
    "%bash  \n",
    "DECODE_FILE=data/poetry/rumi_leads.txt\n",
    "cat ${DECODE_FILE}.*.decodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these are still phrases and not complete sentences. This indicates that we might need to train longer or better somehow. We need to diagnose the model ...\n",
    "<p>\n",
    "### Diagnosing training run\n",
    "<p>\n",
    "Let's diagnose the training run to see what we'd improve the next time around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 18780. Click <a href=\"/_proxy/42653/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "18780"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('gs://{}/poetry/model_full'.format(BUCKET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped TensorBoard\n"
     ]
    }
   ],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().stop(13280)\n",
    "print 'stopped TensorBoard'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"diagrams/poetry_loss.png\"/></td>\n",
    "<td><img src=\"diagrams/poetry_acc.png\"/></td>\n",
    "</table>\n",
    "Looking at the loss curve, it is clear that we are overfitting (note that the orange training curve is well below the blue eval curve). Both loss curves and the accuracy-per-sequence curve, which is our key evaluation measure, plateaus after 40k. (The red curve is a faster way of computing the evaluation metric, and can be ignored). So, how do we improve the model? Well, we need to reduce overfitting and make sure the eval metrics keep going down as long as the loss is also going down.\n",
    "<p>\n",
    "What we really need to do is to get more data, but if that's not an option, we could try to reduce the NN and increase the dropout regularization. We could also do hyperparameter tuning on the dropout and network sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving poetry\n",
    "\n",
    "[TBD]\n",
    "\n",
    "How would you serve these predictions? The easiest way would to be take t2t-decoder and wrap it with a Python Flask web application and run it on a GCE instance with a GPU. It's just Python code, after all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2018 Google Inc. Licensed under the Apache License, Version 2.0 (the \\\"License\\\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
