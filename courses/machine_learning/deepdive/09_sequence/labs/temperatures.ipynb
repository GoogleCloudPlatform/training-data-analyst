{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An RNN model for temperature data\n",
    "This time we will be working with real data: daily (Tmin, Tmax) temperature series from 36 weather stations spanning 50 years. It is to be noted that a pretty good predictor model already exists for temperatures: the average of temperatures on the same day of the year in N previous years. It is not clear if RNNs can do better but we will see how far they can go.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Things to do:<br/>\n",
    "<ol start=\"0\">\n",
    "    <li>Run the notebook as it is. Look at the data visualisations. Then look at the predictions at the end. Not very good...\n",
    "    <li>First play with the data to find good values for RESAMPLE_BY and SEQLEN in hyperparameters ([Assignment #1](#assignment1)).\n",
    "    <li>Now implement the RNN model in the model function ([Assignment #2](#assignment2)).\n",
    "<li>Temperatures are noisy, let's try something new: predicting N data points ahead  instead of only 1 ahead ([Assignment #3](#assignment3)).\n",
    "<li>Now we will adjust more traditional hyperparameters and add regularisations. ([Assignment #4](#assignment4))\n",
    "<li>\n",
    "Look at the save-restore code. The model is saved at the end of the [training loop](#train) and restored when running [validation](#valid). Also see how the restored model is used for [inference](#inference).\n",
    "    <br/><br/>\n",
    "You are ready to run in the cloud on all 1666 weather stations. Use [this bash notebook](../run-on-cloud-ml-engine.ipynb) to convert your code to a regular Python file and invoke the Google Cloud ML Engine command line.\n",
    "When the training is finished on ML Engine, change one line in [validation](#valid) to load the SAVEDMODEL from its cloud bucket and display.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==1.15.3 in /opt/conda/lib/python3.7/site-packages (1.15.3)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.3) (1.12.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.3) (1.1.0)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.3) (1.15.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.3) (0.9.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.3) (3.11.4)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.3) (0.34.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.3) (1.14.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.3) (1.18.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.3) (1.0.8)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.3) (1.15.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.3) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.3) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.3) (1.27.2)\n",
      "Requirement already satisfied: gast==0.2.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.3) (0.2.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.3) (0.8.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.3) (3.2.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.3) (46.1.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.3) (3.2.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.3) (1.0.0)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15.3) (2.10.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==1.15.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
     "Please ignore any compatibility warnings and errors.\n",
     "Make sure to <b>restart</b> your kernel to ensure this change has taken place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, '../temperatures/utils/') #so python can find the utils_ modules\n",
    "import utils_batching\n",
    "import utils_args\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.lib.io import file_io as gfile\n",
    "print(\"Tensorflow version: \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "display"
    ]
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import utils_prettystyle\n",
    "import utils_display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},

   "outputs": [],
   "source": [
    "%%bash\n",
    "DOWNLOAD_DIR=../temperatures/data\n",
    "mkdir $DOWNLOAD_DIR\n",
    "gcloud storage cp gs://cloud-training-demos/courses/machine_learning/deepdive/09_sequence/temperatures/* $DOWNLOAD_DIR"   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"hyperparameters\"></a>\n",
    "<a name=\"assignment1\"></a>\n",
    "## Hyperparameters\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "***Assignment #1*** Temperatures have a periodicity of 365 days. We would need to unroll the RNN over 365 steps (=SEQLEN) to capture that. That is way too much. We will have to work with averages over a handful of days instead of daily temperatures. Bump the unrolling length to SEQLEN=128 and then try averaging over 3 to 5 days (RESAMPLE_BY=3, 4, 5). Look at the data visualisations in [Resampling](#resampling) and [Training sequences](#trainseq). The training sequences should capture a recognizable part of the yearly oscillation.\n",
    "***In the end, use these values: SEQLEN=128, RESAMPLE_BY=5.***\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCHS = 5       # number of times the model sees all the data during training\n",
    "\n",
    "N_FORWARD = 1       # train the network to predict N in advance (traditionnally 1)\n",
    "RESAMPLE_BY = 1     # averaging period in days (training on daily data is too much)\n",
    "RNN_CELLSIZE = 80   # size of the RNN cells\n",
    "N_LAYERS = 1        # number of stacked RNN cells (needed for tensor shapes but code must be changed manually)\n",
    "SEQLEN = 32         # unrolled sequence length\n",
    "BATCHSIZE = 64      # mini-batch size\n",
    "DROPOUT_PKEEP = 0.7 # probability of neurons not being dropped (should be between 0.5 and 1)\n",
    "ACTIVATION = tf.nn.tanh # Activation function for GRU cells (tf.nn.relu or tf.nn.tanh)\n",
    "\n",
    "JOB_DIR  = \"temperature_checkpoints\"\n",
    "DATA_DIR = \"../temperatures/data\"\n",
    "\n",
    "# potentially override some settings from command-line arguments\n",
    "if __name__ == '__main__':\n",
    "    JOB_DIR, DATA_DIR = utils_args.read_args1(JOB_DIR, DATA_DIR)\n",
    "\n",
    "ALL_FILEPATTERN = DATA_DIR + \"/*.csv\" # pattern matches all 1666 files  \n",
    "EVAL_FILEPATTERN = DATA_DIR + \"/USC000*2.csv\" # pattern matches 8 files\n",
    "# pattern USW*.csv -> 298 files, pattern USW*0.csv -> 28 files\n",
    "print('Reading data from \"{}\".\\nWrinting checkpoints to \"{}\".'.format(DATA_DIR, JOB_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature data\n",
    "This is what our temperature datasets looks like: sequences of daily (Tmin, Tmax) from 1960 to 2010. They have been cleaned up and eventual missing values have been filled by interpolation. Interpolated regions of the dataset are marked in red on the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_filenames = gfile.get_matching_files(ALL_FILEPATTERN)\n",
    "eval_filenames = gfile.get_matching_files(EVAL_FILEPATTERN)\n",
    "train_filenames = list(set(all_filenames) - set(eval_filenames))\n",
    "\n",
    "# By default, this utility function loads all the files and places data\n",
    "# from them as-is in an array, one file per line. Later, we will use it\n",
    "# to shape the dataset as needed for training.\n",
    "ite = utils_batching.rnn_multistation_sampling_temperature_sequencer(eval_filenames)\n",
    "evtemps, _, evdates, _, _ = next(ite) # gets everything\n",
    "\n",
    "print('Pattern \"{}\" matches {} files'.format(ALL_FILEPATTERN, len(all_filenames)))\n",
    "print('Pattern \"{}\" matches {} files'.format(EVAL_FILEPATTERN, len(eval_filenames)))\n",
    "print(\"Evaluation files: {}\".format(len(eval_filenames)))\n",
    "print(\"Training files: {}\".format(len(train_filenames)))\n",
    "print(\"Initial shape of the evaluation dataset: \" + str(evtemps.shape))\n",
    "print(\"{} files, {} data points per file, {} values per data point\"\n",
    "      \" (Tmin, Tmax, is_interpolated) \".format(evtemps.shape[0], evtemps.shape[1],evtemps.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "display"
    ]
   },
   "outputs": [],
   "source": [
    "# You can adjust the visualisation range and dataset here.\n",
    "# Interpolated regions of the dataset are marked in red.\n",
    "WEATHER_STATION = 0 # 0 to 7 in default eval dataset\n",
    "START_DATE = 0      # 0 = Jan 2nd 1950\n",
    "END_DATE = 18262    # 18262 = Dec 31st 2009\n",
    "visu_temperatures = evtemps[WEATHER_STATION,START_DATE:END_DATE]\n",
    "visu_dates = evdates[START_DATE:END_DATE]\n",
    "\n",
    "utils_display.picture_this_4(visu_temperatures, visu_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"resampling\"></a>\n",
    "## Resampling\n",
    "Our RNN would need to be unrolled across 365 steps to capture the yearly temperature cycles. That's a bit too much. We will resample the temparatures and work with 5-day averages for example. This is what resampled (Tmin, Tmax) temperatures look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time we ask the utility function to average temperatures over 5-day periods (RESAMPLE_BY=5)\n",
    "ite = utils_batching.rnn_multistation_sampling_temperature_sequencer(eval_filenames, RESAMPLE_BY, tminmax=True)\n",
    "evaltemps, _, evaldates, _, _ = next(ite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "display"
    ]
   },
   "outputs": [],
   "source": [
    "# display five years worth of data\n",
    "WEATHER_STATION = 0              # 0 to 7 in default eval dataset\n",
    "START_DATE = 0                   # 0 = Jan 2nd 1950\n",
    "END_DATE = 365*5//RESAMPLE_BY    # 5 years\n",
    "visu_temperatures = evaltemps[WEATHER_STATION, START_DATE:END_DATE]\n",
    "visu_dates = evaldates[START_DATE:END_DATE]\n",
    "plt.fill_between(visu_dates, visu_temperatures[:,0], visu_temperatures[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"trainseq\"></a>\n",
    "## Visualize training sequences\n",
    "This is what the neural network will see during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "display"
    ]
   },
   "outputs": [],
   "source": [
    "# The function rnn_multistation_sampling_temperature_sequencer puts one weather station per line in\n",
    "# a batch and continues with data from the same station in corresponding lines in the next batch.\n",
    "# Features and labels are returned with shapes [BATCHSIZE, SEQLEN, 2]. The last dimension of size 2\n",
    "# contains (Tmin, Tmax).\n",
    "ite = utils_batching.rnn_multistation_sampling_temperature_sequencer(eval_filenames,\n",
    "                                                                     RESAMPLE_BY,\n",
    "                                                                     BATCHSIZE,\n",
    "                                                                     SEQLEN,\n",
    "                                                                     N_FORWARD,\n",
    "                                                                     nb_epochs=1,\n",
    "                                                                     tminmax=True)\n",
    "\n",
    "# load 6 training sequences (each one contains data for all weather stations)\n",
    "visu_data = [next(ite) for _ in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "display"
    ]
   },
   "outputs": [],
   "source": [
    "# Check that consecutive training sequences from the same weather station are indeed consecutive\n",
    "WEATHER_STATION = 4\n",
    "utils_display.picture_this_5(visu_data, WEATHER_STATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"assignment2\"></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "***Assignement #2*** Temperatures are noisy. If we ask the model to predict the naxt data point, noise might drown the trend and the model will not train. The trend should be clearer if we ask the moder to look further ahead. You can use the [hyperparameter](#hyperparameters) N_FORWARD to shift the target sequences by more than 1. Try values between 4 and 16 and see how [training sequences](#trainseq) look.<br/>\n",
    "<br/>\n",
    "If the model predicts N_FORWARD in advance, you will also need it to output N_FORWARD predicted values instead of 1. Please check that the output of your model is indeed `Yout = Yr[:,-N_FORWARD:,:]`. The inference part has already been adjusted to generate the sequence by blocks of N_FORWARD points. You can have a [look at it](#inference).<br/>\n",
    "<br/>\n",
    "Train and evaluate to see if you are getting better results. ***In the end, use this value: N_FORWARD=8***\n",
    "</div>\n",
    "\n",
    "<a name=\"assignment3\"></a>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "***Assignement #3*** Try adjusting the follwing parameters:<ol><ol>\n",
    "    <li> Use a stacked RNN cell with 2 layers with in the model:<br/>\n",
    "```\n",
    "cells = [tf.nn.rnn_cell.GRUCell(RNN_CELLSIZE) for _ in range(N_LAYERS)]\n",
    "cell = tf.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=False)\n",
    "```\n",
    "        <br/>Do not forget to set N_LAYERS=2 in [hyperparameters](#hyperparameters)\n",
    "    </li>\n",
    "    <li>Regularisation: add dropout between cell layers.<br/>\n",
    "```        \n",
    "cells = [tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob = dropout_pkeep) for cell in cells]\n",
    "```\n",
    "<br/>\n",
    "        Check that you have a good value for DROPOUT_PKEEP in [hyperparameters](#hyperparameters). 0.7 should do. Also check that dropout is deactivated i.e. dropout_pkeep=1.0 during [inference](#inference).\n",
    "    </li>\n",
    "    <li>Increase RNN_CELLSIZE -> 128 to allow the cells to model more complex behaviors.</li>\n",
    "\n",
    "</ol></ol>\n",
    "Play with these options until you get a good fit for at least 1.5 years.\n",
    "</div>\n",
    "\n",
    "![deep RNN schematic](../temperatures/images/RNN2.svg)\n",
    "<div style=\"text-align: right; font-family: monospace\">\n",
    "  X shape [BATCHSIZE, SEQLEN, 2]<br/>\n",
    "  Y shape [BATCHSIZE, SEQLEN, 2]<br/>\n",
    "  H shape [BATCHSIZE, RNN_CELLSIZE*NLAYERS]\n",
    "</div>\n",
    "When executed, this function instantiates the Tensorflow graph for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_rnn_fn(features, Hin, labels, step, dropout_pkeep):\n",
    "    print('features: {}'.format(features))\n",
    "    X = features  # shape [BATCHSIZE, SEQLEN, 2], 2 for (Tmin, Tmax)\n",
    "    batchsize = tf.shape(X)[0]  # allow for variable batch size\n",
    "    seqlen = tf.shape(X)[1]  # allow for variable sequence length\n",
    "    \n",
    "    cell = tf.nn.rnn_cell.GRUCell(RNN_CELLSIZE)\n",
    "    Hr, H = tf.nn.dynamic_rnn(cell,X,initial_state=Hin)\n",
    "    Yn = tf.reshape(Hr, [batchsize*seqlen, RNN_CELLSIZE])\n",
    "    Yr = tf.layers.dense(Yn, 2) # Yr [BATCHSIZE*SEQLEN, 2] predicting vectors of 2 element\n",
    "    Yr = tf.reshape(Yr, [batchsize, seqlen, 2]) # Yr [BATCHSIZE, SEQLEN, 2]\n",
    "    \n",
    "    Yout = Yr[:,-N_FORWARD:,:] # Last N_FORWARD outputs. Yout [BATCHSIZE, N_FORWARD, 2]\n",
    "    \n",
    "    loss = tf.losses.mean_squared_error(Yr, labels) # labels[BATCHSIZE, SEQLEN, 2]\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    \n",
    "    return Yout, H, loss, train_op, Yr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # restart model graph from scratch\n",
    "\n",
    "# placeholder for inputs\n",
    "Hin = tf.placeholder(tf.float32, [None, RNN_CELLSIZE * N_LAYERS])\n",
    "features = tf.placeholder(tf.float32, [None, None, 2]) # [BATCHSIZE, SEQLEN, 2]\n",
    "labels = tf.placeholder(tf.float32, [None, None, 2]) # [BATCHSIZE, SEQLEN, 2]\n",
    "step = tf.placeholder(tf.int32)\n",
    "dropout_pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "# instantiate the model\n",
    "Yout, H, loss, train_op, Yr = model_rnn_fn(features, Hin, labels, step, dropout_pkeep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Tensorflow session\n",
    "This resets all neuron weights and biases to initial random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variable initialization\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run([init])\n",
    "saver = tf.train.Saver(max_to_keep=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"train\"></a>\n",
    "## The training loop\n",
    "You can re-execute this cell to continue training. <br/>\n",
    "<br/>\n",
    "Training data must be batched correctly, one weather station per line, continued on the same line across batches. This way, output states computed from one batch are the correct input states for the next batch. The provided utility function `rnn_multistation_sampling_temperature_sequencer` does the right thing.\n",
    "![batching for RNNs](../temperatures/images/batching.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "indices = []\n",
    "last_epoch = 99999\n",
    "last_fileid = 99999\n",
    "\n",
    "for i, (next_features, next_labels, dates, epoch, fileid) in enumerate(\n",
    "    utils_batching.rnn_multistation_sampling_temperature_sequencer(train_filenames,\n",
    "                                                                   RESAMPLE_BY,\n",
    "                                                                   BATCHSIZE,\n",
    "                                                                   SEQLEN,\n",
    "                                                                   N_FORWARD,\n",
    "                                                                   NB_EPOCHS, tminmax=True)):\n",
    "    \n",
    "    # reinintialize state between epochs or when starting on data from a new weather station\n",
    "    if epoch != last_epoch or fileid != last_fileid:\n",
    "        batchsize = next_features.shape[0]\n",
    "        H_ = np.zeros([batchsize, RNN_CELLSIZE * N_LAYERS])\n",
    "        print(\"State reset\")\n",
    "\n",
    "    #train\n",
    "    feed = {Hin: H_, features: next_features, labels: next_labels, step: i, dropout_pkeep: DROPOUT_PKEEP}\n",
    "    Yout_, H_, loss_, _, Yr_ = sess.run([Yout, H, loss, train_op, Yr], feed_dict=feed)\n",
    "    \n",
    "    # print progress\n",
    "    if i%20 == 0:\n",
    "        print(\"{}: epoch {} loss = {} ({} weather stations this epoch)\".format(i, epoch, np.mean(loss_), fileid+1))\n",
    "        sys.stdout.flush()\n",
    "    if i%10 == 0:\n",
    "        losses.append(np.mean(loss_))\n",
    "        indices.append(i)\n",
    "     # This visualisation can be helpful to see how the model \"locks\" on the shape of the curve\n",
    "#    if i%100 == 0:\n",
    "#        plt.figure(figsize=(10,2))\n",
    "#        plt.fill_between(dates, next_features[0,:,0], next_features[0,:,1]).set_alpha(0.2)\n",
    "#        plt.fill_between(dates, next_labels[0,:,0], next_labels[0,:,1])\n",
    "#        plt.fill_between(dates, Yr_[0,:,0], Yr_[0,:,1]).set_alpha(0.8)\n",
    "#        plt.show()\n",
    "        \n",
    "    last_epoch = epoch\n",
    "    last_fileid = fileid\n",
    "    \n",
    "# save the trained model\n",
    "SAVEDMODEL = JOB_DIR + \"/ckpt\" + str(int(time.time()))\n",
    "tf.saved_model.simple_save(sess, SAVEDMODEL,\n",
    "                           inputs={\"features\":features, \"Hin\":Hin, \"dropout_pkeep\":dropout_pkeep},\n",
    "                           outputs={\"Yout\":Yout, \"H\":H})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "display"
    ]
   },
   "outputs": [],
   "source": [
    "plt.ylim(ymax=np.amax(losses[1:])) # ignore first value for scaling\n",
    "plt.plot(indices, losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"inference\"></a>\n",
    "## Inference\n",
    "This is a generative model: run an trained RNN cell in a loop. This time, with N_FORWARD>1, we generate the sequence by blocks of N_FORWAD data points instead of point by point. The RNN is unrolled across N_FORWARD steps, takes in a the last N_FORWARD data points and predicts the next N_FORWARD data points and so on in a loop. State must be passed around correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction_run(predict_fn, prime_data, run_length):\n",
    "    H = np.zeros([1, RNN_CELLSIZE * N_LAYERS]) # zero state initially\n",
    "    Yout = np.zeros([1, N_FORWARD, 2])\n",
    "    data_len = prime_data.shape[0]-N_FORWARD\n",
    "\n",
    "    # prime the state from data\n",
    "    if data_len > 0:\n",
    "        Yin = np.array(prime_data[:-N_FORWARD])\n",
    "        Yin = np.reshape(Yin, [1, data_len, 2]) # reshape as one sequence of pairs (Tmin, Tmax)\n",
    "        r = predict_fn({'features': Yin, 'Hin':H, 'dropout_pkeep':1.0}) # no dropout during inference\n",
    "        Yout = r[\"Yout\"]\n",
    "        H = r[\"H\"]\n",
    "        \n",
    "        # initaily, put real data on the inputs, not predictions\n",
    "        Yout = np.expand_dims(prime_data[-N_FORWARD:], axis=0)\n",
    "        # Yout shape [1, N_FORWARD, 2]: batch of a single sequence of length N_FORWARD of (Tmin, Tmax) data pointa\n",
    "    \n",
    "    # run prediction\n",
    "    # To generate a sequence, run a trained cell in a loop passing as input and input state\n",
    "    # respectively the output and output state from the previous iteration.\n",
    "    results = []\n",
    "    for i in range(run_length//N_FORWARD+1):\n",
    "        r = predict_fn({'features': Yout, 'Hin':H, 'dropout_pkeep':1.0}) # no dropout during inference\n",
    "        Yout = r[\"Yout\"]\n",
    "        H = r[\"H\"]\n",
    "        results.append(Yout[0]) # shape [N_FORWARD, 2]\n",
    "        \n",
    "    return np.concatenate(results, axis=0)[:run_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"valid\"></a>\n",
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QYEAR = 365//(RESAMPLE_BY*4)\n",
    "YEAR = 365//(RESAMPLE_BY)\n",
    "\n",
    "# Try starting predictions from January / March / July (resp. OFFSET = YEAR or YEAR+QYEAR or YEAR+2*QYEAR)\n",
    "# Some start dates are more challenging for the model than others.\n",
    "OFFSET = 4*YEAR+1*QYEAR\n",
    "\n",
    "PRIMELEN=5*YEAR\n",
    "RUNLEN=3*YEAR\n",
    "RMSELEN=3*365//(RESAMPLE_BY*2) # accuracy of predictions 1.5 years in advance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the model from the last checkpoint saved previously.\n",
    "\n",
    "# Alternative checkpoints:\n",
    "# Once you have trained on all 1666 weather stations on Google Cloud ML Engine, you can load the checkpoint from there.\n",
    "# SAVEDMODEL = \"gs://{BUCKET}/sinejobs/sines_XXXXXX_XXXXXX/ckptXXXXXXXX\"\n",
    "# A sample checkpoint is provided with the lab. You can try loading it for comparison.\n",
    "# You will have to use the following parameters and re-run the entire notebook:\n",
    "# N_FORWARD = 8, RESAMPLE_BY = 5, RNN_CELLSIZE = 128, N_LAYERS = 2\n",
    "# SAVEDMODEL = \"temperatures_best_checkpoint\"\n",
    "\n",
    "predict_fn = tf.contrib.predictor.from_saved_model(SAVEDMODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "display"
    ]
   },
   "outputs": [],
   "source": [
    "for evaldata in evaltemps:\n",
    "    prime_data = evaldata[OFFSET:OFFSET+PRIMELEN]\n",
    "    results = prediction_run(predict_fn, prime_data, RUNLEN)\n",
    "    utils_display.picture_this_6(evaldata, evaldates, prime_data, results, PRIMELEN, RUNLEN, OFFSET, RMSELEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmses = []\n",
    "bad_ones = 0\n",
    "for offset in [YEAR, YEAR+QYEAR, YEAR+2*QYEAR]:\n",
    "    for evaldata in evaltemps:\n",
    "        prime_data = evaldata[offset:offset+PRIMELEN]\n",
    "        results = prediction_run(predict_fn, prime_data, RUNLEN)\n",
    "        rmse = math.sqrt(np.mean((evaldata[offset+PRIMELEN:offset+PRIMELEN+RMSELEN] - results[:RMSELEN])**2))\n",
    "        rmses.append(rmse)\n",
    "        if rmse>7: bad_ones += 1\n",
    "        print(\"RMSE on {} predictions (shaded area): {}\".format(RMSELEN, rmse))\n",
    "print(\"Average RMSE on {} weather stations: {} ({} really bad ones, i.e. >7.0)\".format(len(evaltemps), np.mean(rmses), bad_ones))\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- This notebook is taken from Google's *Tensorflow Without a PhD* series created by Martin Gorner. \n",
    "- Minor modifications were made. See the source notebooks [here](https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd/tree/master/tensorflow-rnn-tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Copyright 2020 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "[http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
