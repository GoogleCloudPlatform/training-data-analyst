{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Creating a custom Word2Vec embedding on your data </h1>\n",
    "\n",
    "This notebook illustrates:\n",
    "<ol>\n",
    "<li> Creating a training dataset\n",
    "<li> Running word2vec\n",
    "<li> Examining the created embedding\n",
    "<li> Export the embedding into a file you can use in other models\n",
    "<li> Training the text classification model of [txtcls2.ipynb](txtcls2.ipynb) with this custom embedding.\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'cloud-training-demos-ml'\n",
    "PROJECT = 'cloud-training-demos'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a training dataset\n",
    "\n",
    "The training dataset simply consists of a bunch of words separated by spaces extracted from your documents. The words are simply in the order that they appear in the documents and words from successive documents are simply appended together. In other words, there is not \"document separator\".\n",
    "<p>\n",
    "The only preprocessing that I do is to replace anything that is not a letter or hyphen by a space.\n",
    "<p>\n",
    "Recall that word2vec is unsupervised. There is no label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import google.datalab.bigquery as bq\n",
    "\n",
    "query=\"\"\"\n",
    "SELECT\n",
    "  CONCAT( LOWER(REGEXP_REPLACE(title, '[^a-zA-Z $-]', ' ')), \n",
    "  \" \", \n",
    "  LOWER(REGEXP_REPLACE(text, '[^a-zA-Z $-]', ' '))) AS text\n",
    "FROM\n",
    "  `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "  LENGTH(title) > 100\n",
    "  AND LENGTH(text) > 100\n",
    "\"\"\"\n",
    "\n",
    "df = bq.Query(query).execute().result().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reddit bookmarklets allow web site owners to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>why not let online ads fight it out in a geome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>smashing the clock  bestbuy s  location and ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ask hn  can google aggregate everything you ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ask yc    think out loud  - like twitter justi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  reddit bookmarklets allow web site owners to c...\n",
       "1  why not let online ads fight it out in a geome...\n",
       "2  smashing the clock  bestbuy s  location and ho...\n",
       "3  ask hn  can google aggregate everything you ve...\n",
       "4  ask yc    think out loud  - like twitter justi..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('word2vec/words.txt', 'w') as ofp:\n",
    "  for txt in df['text']:\n",
    "    ofp.write(txt + \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the resulting file looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit bookmarklets allow web site owners to cheat to get mostly up votes  simple realistic example given   the idea is to associate a positive link and a negative link with your site  you would submit both to reddit  p based on the user s experience  you would switch him her to the positive negative link  p that way  happy users would vote up the positive link while unhappy users would vote down the negative link   your site now has a better chance of making the front page  p as an example  suppose your site has a game puzzle  p when the user visits the site via the positive or negative link  you redirect to the negative link  p if the user plays several levels of the game puzzle  then he she probably likes it and then you can switch him her to the positive link  why not let online ads fight it out in a geometric real-time game played by advertisers and consumers  the advertiser may display his her ad along with all the other ads currently on display    p larger ads have the disadvant\r\n"
     ]
    }
   ],
   "source": [
    "!cut -c-1000 word2vec/words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running word2vec\n",
    "\n",
    "We can run the existing tutorial code as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "cd word2vec\n",
    "TF_INC=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())')\n",
    "TF_LIB=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_lib())')\n",
    "g++ -std=c++11 \\\n",
    "   -I/usr/local/lib/python2.7/dist-packages/tensorflow/include/external/nsync/public \\\n",
    "   -shared word2vec_ops.cc word2vec_kernels.cc \\\n",
    "   -o word2vec_ops.so -fPIC -I $TF_INC -O2 -D_GLIBCXX_USE_CXX11_ABI=0 -L$TF_LIB -ltensorflow_framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual evaluation dataset doesn't matter.  Let's just make sure to have some words in the input also in the eval. The analogy dataset is of the form \n",
    "<pre>\n",
    "Athens Greece Cairo Egypt\n",
    "Baghdad Iraq Beijing China\n",
    "</pre>\n",
    "i.e. four words per line where the model is supposed to predict the fourth given the first three. But we'll just make up a junk file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing word2vec/junk.txt\n"
     ]
    }
   ],
   "source": [
    "%writefile word2vec/junk.txt\n",
    ": analogy-questions-ignored\n",
    "the user plays several levels\n",
    "of the game puzzle\n",
    "vote down the negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "cd word2vec\n",
    "rm -rf trained\n",
    "python word2vec.py \\\n",
    "   --train_data=./words.txt --eval_data=./junk.txt --save_path=./trained \\\n",
    "   --min_count=1 --embedding_size=10 --window_size=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the created embedding\n",
    "\n",
    "Let's load up the embedding file in TensorBoard.  Start up TensorBoard, switch to the \"Projector\" tab and then click on the button to \"Load data\".  Load the vocab.txt that is in the output directory of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('word2vec/trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, for example, is the word \"founders\" in context -- it's near doing, creative, difficult, and fight, which sounds about right ...  The numbers next to the words reflect the count -- we should try to get a large enough vocabulary that we can use --min_count=10 when training word2vec, but that would also take too long for a classroom situation. <img src=\"embeds.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pid in TensorBoard.list()['pid']:\n",
    "    TensorBoard().stop(pid)\n",
    "    print 'Stopped TensorBoard with pid {}'.format(pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the embedding vectors into a text file\n",
    "\n",
    "Let's export the embedding into a text file, so that we can use it the way we used the Glove embeddings in txtcls2.ipynb.\n",
    "\n",
    "Notice that we have written out our vocabulary and vectors into two files.  We just have to merge them now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   890   8900 226934 word2vec/trained/vectors.txt\r\n",
      "   890   1780   8259 word2vec/trained/vocab.txt\r\n",
      "  1780  10680 235193 total\r\n"
     ]
    }
   ],
   "source": [
    "!wc word2vec/trained/*.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> word2vec/trained/vectors.txt <==\r\n",
      "-2.472065091133117676e-01 -3.885798156261444092e-01 -2.226969599723815918e-01 8.574548363685607910e-02 4.453513324260711670e-01 3.030938208103179932e-01 2.762222662568092346e-02 -4.628151655197143555e-01 6.405805051326751709e-02 -4.708295166492462158e-01\r\n",
      "-1.005752161145210266e-01 3.006918132305145264e-01 1.801920384168624878e-01 -3.159367144107818604e-01 -3.252084553241729736e-01 4.999429285526275635e-01 -3.082303404808044434e-01 2.440812736749649048e-01 -4.505534768104553223e-01 -2.321645617485046387e-01\r\n",
      "3.727774024009704590e-01 2.538295388221740723e-01 -9.570891410112380981e-02 -2.781682647764682770e-02 4.326484501361846924e-01 4.568791389465332031e-01 3.149969279766082764e-01 2.019654512405395508e-01 -4.677839279174804688e-01 -1.786493211984634399e-01\r\n",
      "\r\n",
      "==> word2vec/trained/vocab.txt <==\r\n",
      "UNK 0\r\n",
      "to 99\r\n",
      "the 98\r\n"
     ]
    }
   ],
   "source": [
    "!head -3 word2vec/trained/*.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "vocab = pd.read_csv(\"word2vec/trained/vocab.txt\", sep=\"\\s+\", header=None, names=('word', 'count'))\n",
    "vectors = pd.read_csv(\"word2vec/trained/vectors.txt\", sep=\"\\s+\", header=None)\n",
    "vectors = pd.concat([vocab, vectors], axis=1)\n",
    "del vectors['count']\n",
    "vectors.to_csv(\"word2vec/trained/embedding.txt.gz\", sep=\" \", header=False, index=False, index_label=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK -0.247206509113 -0.388579815626 -0.222696959972 0.0857454836369 0.445351332426 0.30309382081 0.0276222266257 -0.46281516552 0.0640580505133 -0.470829516649\r\n",
      "to -0.100575216115 0.300691813231 0.180192038417 -0.315936714411 -0.325208455324 0.499942928553 -0.308230340481 0.244081273675 -0.45055347681 -0.232164561749\r\n",
      "the 0.372777402401 0.253829538822 -0.0957089141011 -0.0278168264776 0.432648450136 0.456879138947 0.314996927977 0.201965451241 -0.467783927917 -0.178649321198\r\n",
      "\r\n",
      "gzip: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!zcat word2vec/trained/embedding.txt.gz | head -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model with custom embedding\n",
    "\n",
    "Now, you can use this embedding file instead of the Glove embedding used in [txtcls2.ipynb](txtcls2.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://word2vec/trained/embedding.txt.gz [Content-Type=text/plain]...\n",
      "/ [0 files][    0.0 B/ 66.1 KiB]                                                \r",
      "/ [1 files][ 66.1 KiB/ 66.1 KiB]                                                \r\n",
      "Operation completed over 1 objects/66.1 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil cp word2vec/trained/embedding.txt.gz gs://${BUCKET}/txtcls2/custom_embedding.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "OUTDIR=gs://${BUCKET}/txtcls2/trained_model\n",
    "JOBNAME=txtcls_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gsutil cp txtcls1/trainer/*.py $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=$(pwd)/txtcls1/trainer \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --scale-tier=BASIC_GPU \\\n",
    "   --runtime-version=1.4 \\\n",
    "   -- \\\n",
    "   --bucket=${BUCKET} \\\n",
    "   --output_dir=${OUTDIR} \\\n",
    "   --glove_embedding=gs://${BUCKET}/txtcls2/custom_embedding.txt.gz \\\n",
    "   --train_steps=36000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
