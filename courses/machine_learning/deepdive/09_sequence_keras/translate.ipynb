{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- add attention (tf.keras.layers.Attention)\n",
    "- see if benchmarks improved\n",
    "- add beam search\n",
    "\n",
    "### Resources\n",
    "\n",
    "- Francois Chollet: https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py (credit when published)\n",
    "- Attention Keras: https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39\n",
    "- TF NMT: https://colab.sandbox.google.com/drive/1R4Hxvzf1a6H95N2sjh5_lVRat_59Zxlx#scrollTo=ddefjBMa3jF0\n",
    "\n",
    "### To research\n",
    "\n",
    "- Better understanding of teacher forcing: https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/\n",
    "\n",
    "#### Optimizations\n",
    "\n",
    "- move everything to translate folder under 09_sequence_keras\n",
    "- package preprocessing code into preprocess.py, and import both here and in deploy, so you don't have two copies of the same source code\n",
    "\n",
    "#### completed\n",
    "\n",
    "- deploy for online prediction (use cmle custom)\n",
    "- make reproducible\n",
    "- prediction works\n",
    "- establish benchmark on: bleu score (do after the fact, then maybe add as keras eval metric)\n",
    "- add way to checkpoint/restore models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab\n",
    "\n",
    "Translation model demonstrating encoder-decoder model, using attention, beam search, bleu score, and deploying via custom AI platform prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import unicodedata\n",
    "import re\n",
    "import io\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "print(tf.__version__) # 2.0.0-beta0\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=0\n",
    "MODEL_PATH = 'translate_models/baseline'\n",
    "LOAD_CHECKPOINT=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "We'll use a language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n",
    "\n",
    "```\n",
    "May I borrow this book?\t¿Puedo tomar prestado este libro?\n",
    "```\n",
    "\n",
    "The dataset is a curated list of 120K translation pairs from http://tatoeba.org/, a platform for community contributed translations by native speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "1. lower case\n",
    "2. add space between puncation and words\n",
    "3. replace tokens that aren't a-z or punctation with space\n",
    "4. add \\<start> and \\<end> tokens\n",
    "5. tokenize \n",
    "6. pad to length of longest sentence (post-pad)\n",
    "7. convert to tf.data dataset\n",
    "8. shuffle and batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
     ]
    }
   ],
   "source": [
    "en, sp = create_dataset(path_to_file, None)\n",
    "print(en[-1])\n",
    "print(sp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang,lang_tokenizer=None):\n",
    "  if lang_tokenizer==None:\n",
    "      lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "          filters='')\n",
    "      lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    # creating cleaned input, output pairs\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit size to 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 30000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 24000, 6000, 6000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(\n",
    "    input_tensor, target_tensor, test_size=0.2, random_state=0)\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang,tensor):\n",
    "    return [lang.index_word[t] if t!=0 else '' for t in tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "[  1 133  14 316   3   2   0   0   0   0   0   0   0   0   0   0]\n",
      "['<start>', 'deja', 'de', 'leer', '.', '<end>', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Target Language; index to word mapping\n",
      "[  1  86 341   3   2   0   0   0   0   0   0]\n",
      "['<start>', 'stop', 'reading', '.', '<end>', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "print(input_tensor_train[0])\n",
    "print(convert(inp_lang, input_tensor_train[0]))\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "print(target_tensor_train[0])\n",
    "print(convert(targ_lang, target_tensor_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0617 15:20:50.376947 140613692798720 deprecation.py:323] From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "def encoder_decoder_dataset(encoder_input, decoder_input):\n",
    "    \"\"\"Converts sequence pairs into a tf.data.Dataset suitable for\n",
    "    encoder-decoder learnign using teacher forcing.\n",
    "    \n",
    "    Arguments:\n",
    "        encoder_input: tensor of shape (num_examples,encoder_ seq_length) fed into the encoder RNN\n",
    "        decoder_input: tensor of shape (num_examples,decoder_seq_length) fed into the decoder RNN during training\n",
    "    Returns\n",
    "        tf.data.Dataset of shape ((encoder_input,decoder_input),target)\n",
    "        target is the decoder_input shifted ahead by 1, with a 0 for padding at the end\n",
    "    \"\"\"\n",
    "    target = tf.roll(decoder_input,-1,1) # shift ahead by 1\n",
    "    target = tf.concat((target[:,:-1],tf.zeros([target.shape[0],1],dtype=tf.int32)),axis=-1) # replace last column with 0s\n",
    "    return tf.data.Dataset.from_tensor_slices(((encoder_input, decoder_input), target))\n",
    "    \n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "train_dataset = encoder_decoder_dataset(input_tensor_train, target_tensor_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "eval_dataset = encoder_decoder_dataset(input_tensor_val, target_tensor_val).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=65, shape=(3, 16), dtype=int32, numpy=\n",
       " array([[   1,    4, 5125,    3,    2,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0],\n",
       "        [   1, 3281,    3,    2,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0],\n",
       "        [   1, 1182,   32,   22,   50,    3,    2,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0]], dtype=int32)>,\n",
       " <tf.Tensor: id=69, shape=(3, 11), dtype=int32, numpy=\n",
       " array([[   1,    5, 1953,    3,    2,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,   28,   38,  230,    3,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,  244,  128,   49,   56,    3,    2,    0,    0,    0,    0]],\n",
       "       dtype=int32)>,\n",
       " <tf.Tensor: id=73, shape=(3, 11), dtype=int32, numpy=\n",
       " array([[   5, 1953,    3,    2,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [  28,   38,  230,    3,    2,    0,    0,    0,    0,    0,    0],\n",
       "        [ 244,  128,   49,   56,    3,    2,    0,    0,    0,    0,    0]],\n",
       "       dtype=int32)>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(example_encoder_input_batch, example_decoder_input_batch), example_target_batch = next(iter(train_dataset))\n",
    "example_encoder_input_batch[:3], example_decoder_input_batch[:3], example_target_batch[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 256)    2409984     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    1263360     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       [(None, None, 1024), 3938304     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     [(None, None, 1024), 3938304     embedding_1[0][0]                \n",
      "                                                                 gru[0][1]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 4935)   5058375     gru_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 16,608,327\n",
      "Trainable params: 16,608,327\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "375/375 [==============================] - 30s 81ms/step - loss: 2.1255 - val_loss: 1.6809\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 27s 72ms/step - loss: 1.4693 - val_loss: 1.3915\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 27s 72ms/step - loss: 1.1230 - val_loss: 1.1904\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 27s 72ms/step - loss: 0.8313 - val_loss: 1.0552\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 27s 72ms/step - loss: 0.5740 - val_loss: 0.9748\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 28s 74ms/step - loss: 0.3727 - val_loss: 0.9291\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 27s 73ms/step - loss: 0.2397 - val_loss: 0.9118\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 27s 72ms/step - loss: 0.1547 - val_loss: 0.8962\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 27s 73ms/step - loss: 0.1033 - val_loss: 0.8959\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 27s 72ms/step - loss: 0.0723 - val_loss: 0.9041\n",
      "CPU times: user 8min 45s, sys: 41.1 s, total: 9min 26s\n",
      "Wall time: 4min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(None,),name=\"encoder_input\")\n",
    "encoder_inputs_embedded = tf.keras.layers.Embedding(input_dim=vocab_inp_size, output_dim=embedding_dim,input_length=max_length_inp)(encoder_inputs)\n",
    "encoder_rnn = tf.keras.layers.GRU(\n",
    "     units = 1024,\n",
    "     return_sequences=True,\n",
    "     return_state=True, # what is recurrent_initializer?\n",
    "     recurrent_initializer='glorot_uniform')\n",
    "encoder_outputs, encoder_state = encoder_rnn(encoder_inputs_embedded)\n",
    "\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(None,),name=\"decoder_input\")\n",
    "decoder_inputs_embedded = tf.keras.layers.Embedding(vocab_tar_size, embedding_dim,input_length=max_length_targ)(decoder_inputs)\n",
    "decoder_rnn = tf.keras.layers.GRU(\n",
    "    units = 1024,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    recurrent_initializer='glorot_uniform')\n",
    "decoder_outputs, decoder_state = decoder_rnn(decoder_inputs_embedded,initial_state=encoder_state)\n",
    "\n",
    "# Classifier (take each intermediate hidden state and predict word)\n",
    "decoder_dense = tf.keras.layers.Dense(vocab_tar_size, activation='softmax')\n",
    "predictions = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Model definition\n",
    "if LOAD_CHECKPOINT:\n",
    "    model = tf.keras.models.load_model(os.path.join(MODEL_PATH,'model.h5'))\n",
    "else:\n",
    "    model = tf.keras.models.Model(inputs=[encoder_inputs,decoder_inputs], outputs=predictions)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy') \n",
    "    model.summary()\n",
    "    model.fit(train_dataset,validation_data=eval_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "We can't just use model.predict(), because we don't know all the inputs we used during training. We only know the encoder_input (source language) but not the decoder_input (target language).\n",
    "\n",
    "We do however know the first token of the decoder input, which is the START character. So using this plus the state of the encoder RNN, we can predict the next token. We will then use that token to be the second token of decoder input, and continue like this until we predict the END token, or we reach some defined max length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0617 17:40:19.542754 140613692798720 hdf5_format.py:171] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "W0617 17:40:19.766307 140613692798720 hdf5_format.py:171] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "if LOAD_CHECKPOINT:\n",
    "    encoder_model = tf.keras.models.load_model(os.path.join(MODEL_PATH,'encoder_model.h5'))\n",
    "    decoder_model = tf.keras.models.load_model(os.path.join(MODEL_PATH,'decoder_model.h5'))\n",
    "    \n",
    "else:\n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_state)\n",
    "\n",
    "    decoder_state_input = tf.keras.layers.Input(shape=(units,),name=\"decoder_state_input\")\n",
    "\n",
    "    decoder_outputs, decoder_state = decoder_rnn(decoder_inputs_embedded, initial_state=decoder_state_input) # reuses layer weights\n",
    "    predictions = decoder_dense(decoder_outputs) # reuses layer weights\n",
    "\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs,decoder_state_input],\n",
    "        [predictions,decoder_state])\n",
    "\n",
    "def decode_sequences(input_seqs, output_tokenizer, max_decode_length=50):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        input_seqs: int tensor of shape (BATCH_SIZE,SEQ_LEN)\n",
    "        \n",
    "    \"\"\"\n",
    "    # Encode the input as state vectors.\n",
    "    batch_size = input_seqs.shape[0]\n",
    "    states_value = encoder_model.predict(input_seqs)\n",
    "\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq = tf.ones([batch_size,1])\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    decoded_sentences = [[] for _ in range(batch_size)]\n",
    "    for i in range(max_decode_length):\n",
    "        output_tokens, decoder_state = decoder_model.predict(\n",
    "            [target_seq,states_value])\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[:, -1, :],axis=-1)\n",
    "        tokens = convert(output_tokenizer,sampled_token_index)\n",
    "        for j in range (batch_size):\n",
    "            decoded_sentences[j].append(tokens[j])\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = tf.expand_dims(tf.constant(sampled_token_index),axis=-1)\n",
    "\n",
    "        # Update states\n",
    "        states_value = decoder_state\n",
    "\n",
    "    return decoded_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we can't feed our natural language sentences in directly. We first have to preprocess them so that they are tokenized according to the same rules as the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentences, tokenizer):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sentences: a python list of of strings\n",
    "        tokenizer: keras_preprocessing.text.Tokenizer for mapping words to integers\n",
    "    \"\"\"\n",
    "    sentences = [preprocess_sentence(sentence) for sentence in sentences]\n",
    "    tokens, _ = tokenize(sentences,tokenizer)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to predict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "INPUT:\n",
      "El soldado actuó valientemente.\n",
      "REFERENCE TRANSLATION:\n",
      "The soldier acted bravely.\n",
      "MACHINE TRANSLATION:\n",
      "['the', 'storm', 'were', '<end>', '.', '<end>', '', '', '', '', '']\n",
      "-\n",
      "INPUT:\n",
      "Su pierna mala le impidió ganar la carrera.\n",
      "REFERENCE TRANSLATION:\n",
      "His bad leg prevented him from winning the race.\n",
      "MACHINE TRANSLATION:\n",
      "['her', 'car', 'inspired', 'me', '.', '<end>', '', '', '', '', '']\n",
      "-\n",
      "INPUT:\n",
      "No estamos comiendo.\n",
      "REFERENCE TRANSLATION:\n",
      "We're not eating.\n",
      "MACHINE TRANSLATION:\n",
      "['we', 're', 'not', 'eating', '.', '<end>', '', '', '', '', '']\n",
      "-\n",
      "INPUT:\n",
      "Está llegando el invierno.\n",
      "REFERENCE TRANSLATION:\n",
      "Winter is coming.\n",
      "MACHINE TRANSLATION:\n",
      "['winter', 'is', 'coming', '.', '<end>', '', '', '', '', '', '']\n",
      "-\n",
      "INPUT:\n",
      "El invierno se acerca.\n",
      "REFERENCE TRANSLATION:\n",
      "Winter is coming.\n",
      "MACHINE TRANSLATION:\n",
      "['winter', 'is', 'coming', 'on', '.', '<end>', '', '', '', '', '']\n",
      "-\n",
      "INPUT:\n",
      "Hay una bolsa sobre el escritorio.\n",
      "REFERENCE TRANSLATION:\n",
      "There is a bag on the desk.\n",
      "MACHINE TRANSLATION:\n",
      "['there', 's', 'a', 'bus', 'here', '.', '<end>', '', '', '', '']\n",
      "-\n",
      "INPUT:\n",
      "¿Qué tal si damos un paseo después del almuerzo?\n",
      "REFERENCE TRANSLATION:\n",
      "How about going for a walk after lunch?\n",
      "MACHINE TRANSLATION:\n",
      "['how', 'about', 'you', 'a', 'sad', '?', '<end>', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"El soldado actuó valientemente.\", \n",
    "    \"Su pierna mala le impidió ganar la carrera.\",\n",
    "    \"No estamos comiendo.\",\n",
    "    \"Está llegando el invierno.\",\n",
    "    \"El invierno se acerca.\",\n",
    "    \"Hay una bolsa sobre el escritorio.\",\n",
    "    \"¿Qué tal si damos un paseo después del almuerzo?\"\n",
    "]\n",
    "\n",
    "reference_translations = [\n",
    "    \"The soldier acted bravely.\",\n",
    "    \"His bad leg prevented him from winning the race.\",\n",
    "    \"We're not eating.\",\n",
    "    \"Winter is coming.\",\n",
    "    \"Winter is coming.\",\n",
    "    \"There is a bag on the desk.\",\n",
    "    \"How about going for a walk after lunch?\"\n",
    "]\n",
    "\n",
    "machine_translations = decode_sequences(\n",
    "    preprocess(sentences,inp_lang),\n",
    "    targ_lang,\n",
    "    max_length_targ\n",
    ")\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print('-')\n",
    "    print('INPUT:')\n",
    "    print(sentences[i])\n",
    "    print('REFERENCE TRANSLATION:')\n",
    "    print(reference_translations[i])\n",
    "    print('MACHINE TRANSLATION:')\n",
    "    print(machine_translations[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Model\n",
    "\n",
    "Save model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_CHECKPOINT:\n",
    "    os.makedirs(MODEL_PATH,exist_ok=True)\n",
    "    model.save(os.path.join(MODEL_PATH,'model.h5'))\n",
    "    encoder_model.save(os.path.join(MODEL_PATH,'encoder_model.h5'))\n",
    "    decoder_model.save(os.path.join(MODEL_PATH,'decoder_model.h5'))\n",
    "    pickle.dump(inp_lang,open(os.path.join(MODEL_PATH,'encoder_tokenizer.pkl'),'wb'))\n",
    "    pickle.dump(targ_lang,open(os.path.join(MODEL_PATH,'decoder_tokenizer.pkl'),'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric (BLEU)\n",
    "\n",
    "Our loss metric, cross entropy log loss, is not the best eval metric for machine translation.\n",
    "\n",
    "Unlike say, image classification, there is no one right answer for a machine translation. However our current loss metric, cross entropy, only gives credit when the machine translation matches the exact same word in the same order as the reference translation.\n",
    "\n",
    "Many attempts have been made to develop a better metric for natural language evaluation. The most popular currently is Bilingual Evaluation Understudy (BLUE).\n",
    "\n",
    "- It is quick and inexpensive to calculate.\n",
    "- It is easy to understand.\n",
    "- It is language independent.\n",
    "- It correlates highly with human evaluation.\n",
    "- It has been widely adopted.\n",
    "\n",
    "It has the advantages that it allows comparison to multiple reference translations, and allows flexibility for the ordering of words and phrases. It still is imperfect, since it gives no credit to synonyms, so human evaluation is still best\n",
    "\n",
    "The score is from 0 to 1, where 1 is an exact match. In practice on about 500 sentences (40 general news stories), a human translator scored 0.3468 against four references and scored 0.2571 against two references.\n",
    "\n",
    "It works by counting matching n-grams between the machine and reference texts, regardless of order. BLUE-4 counts matching n grams from 1-4 (1-gram, 2-gram, 3-gram and 4-gram). It is common to report both BLUE-1 and BLUE-4\n",
    "\n",
    "The NLTK framework has an implementation that we will use.\n",
    "\n",
    "Furthermore we can't run evaluation during training, because at that time the correct decoder input is used. We will run our eval metrics after.\n",
    "\n",
    "For more info: https://machinelearningmastery.com/calculate-bleu-score-for-text-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_1(reference, candidate):\n",
    "    smoothing_function = nltk.translate.bleu_score.SmoothingFunction().method1 \n",
    "    return nltk.translate.bleu_score.sentence_bleu(reference, candidate, (1,),smoothing_function)\n",
    "\n",
    "def bleu_4(reference, candidate):\n",
    "    smoothing_function = nltk.translate.bleu_score.SmoothingFunction().method1 \n",
    "    return nltk.translate.bleu_score.sentence_bleu(reference, candidate, (.25,.25,.25,.25),smoothing_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes ~ 3min to run, the bulk of which is decoding the 6000 sentences in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_examples = len(input_tensor_val)\n",
    "bleu_1_total = 0\n",
    "bleu_4_total = 0\n",
    "\n",
    "for idx in range(num_examples):\n",
    "    reference_sentence = convert(targ_lang, target_tensor_val[idx][1:])\n",
    "    decoded_sentence = decode_sequences(input_tensor_val[idx:idx+1],targ_lang,max_length_targ)[0]\n",
    "    bleu_1_total += bleu_1(reference_sentence,decoded_sentence)\n",
    "    bleu_4_total += bleu_4(reference_sentence,decoded_sentence)\n",
    "print('BLEU 1: {}'.format(bleu_1_total/num_examples))\n",
    "print('BLEU 4: {}'.format(bleu_4_total/num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks\n",
    "\n",
    "- Batch_Size: 64\n",
    "- Optimizer: adam\n",
    "- Embed_dim: 256\n",
    "- GRU Units: 1024\n",
    "- Train Examples: 24,000\n",
    "- Epochs: 10\n",
    "- Hardware: P100 GPU\n",
    "\n",
    "**Baseline**\n",
    "- 5min - loss: 0.0722 - val_loss: 0.9062\n",
    "- BLEU 1: 0.2519574312515255\n",
    "- BLEU 4: 0.04589972764144636\n",
    "- Manuel Inspection:most translations make sense, but are synonyms not exact matches\n",
    "\n",
    "Expect loss to be considerably higher once targets are no longer the same as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy\n",
    "\n",
    "Note that to decode our sequences we're not just calling .predict() on a single Keras model. We're using .predict() on two different models with some python code in between. On top of that we're calling each model multiple times in a for loop.\n",
    "\n",
    "Because of this we can't just export to SavedModel and deploy to AI Platform. Instead we'll take advantage of AI Platforms custom prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
