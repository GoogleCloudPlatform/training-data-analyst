{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras Flowers on TPU (playground).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Yd4z24wngWau",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can try this notebook on a GPU but you will quickly be switching to a TPU. For this model it's worth it."
      ]
    },
    {
      "metadata": {
        "id": "89B27-TGiDNB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "id": "9u3d4Z7uQsmp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "os.environ['GCS_READ_CACHE_MAX_SIZE_MB'] = '1' # Hack around Colab/GCS cache bug\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "# little wrinkle: Kears models do not yet work on TPU if eager mode is enabled\n",
        "# tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "tMy0zz6FXnJY",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title display utilities [RUN ME]\n",
        "\n",
        "def dataset_to_numpy_util(dataset, N):\n",
        "  dataset = dataset.batch(N)\n",
        "  \n",
        "  if tf.executing_eagerly():\n",
        "    # In eager mode, iterate in the Datset directly.\n",
        "    for images, labels, one_hot_labels in dataset:\n",
        "      numpy_images = images.numpy()\n",
        "      numpy_labels = labels.numpy()\n",
        "      numpy_one_hot_labels = one_hot_labels.numpy()\n",
        "      break;\n",
        "      \n",
        "  else: # In non-eager mode, must get the TF note that \n",
        "        # yields the nextitem and run it in a tf.Session.\n",
        "    get_next_item = dataset.make_one_shot_iterator().get_next()\n",
        "    with tf.Session() as ses:\n",
        "      (numpy_images,\n",
        "       numpy_labels,\n",
        "       numpy_one_hot_labels) = ses.run(get_next_item)\n",
        "\n",
        "  return numpy_images, numpy_labels, numpy_one_hot_labels\n",
        "\n",
        "def title_from_label_and_one_hot(label, one_hot_label):\n",
        "  return label.decode(\"utf-8\") + ' ' + str(one_hot_label)\n",
        "def title_from_label_and_target(label, correct_label):\n",
        "  correct = (label == correct_label)\n",
        "  return \"{} [{}{}{}]\".format(label.decode(\"utf-8\"), str(correct), ', shoud be ' if not correct else '',\n",
        "                              correct_label.decode(\"utf-8\") if not correct else ''), correct\n",
        "\n",
        "def display_one_flower(image, title, subplot, red=False):\n",
        "    plt.subplot(subplot)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image)\n",
        "    plt.title(title, fontsize=16, color='red' if red else 'black')\n",
        "    return subplot+1\n",
        "  \n",
        "def display_9_images_from_dataset(dataset):\n",
        "  subplot=331\n",
        "  plt.figure(figsize=(13,13))\n",
        "  images, labels, one_hot_labels = dataset_to_numpy_util(dataset, 9)\n",
        "  for i, image in enumerate(images):\n",
        "    title = title_from_label_and_one_hot(labels[i], one_hot_labels[i])\n",
        "    subplot = display_one_flower(image, title, subplot)\n",
        "    if subplot > 339:\n",
        "      break;\n",
        "              \n",
        "  plt.tight_layout()\n",
        "  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "  plt.show()\n",
        "  \n",
        "def display_9_images_with_predictions(images, predictions, labels):\n",
        "  subplot=331\n",
        "  plt.figure(figsize=(13,13))\n",
        "  classes = np.array(CLASSES)[np.argmax(predictions, axis=-1)]\n",
        "  for i, image in enumerate(images):\n",
        "    title, correct = title_from_label_and_target(classes[i], labels[i])\n",
        "    subplot = display_one_flower(image, title, subplot, not correct)\n",
        "    if subplot > 339:\n",
        "      break;\n",
        "              \n",
        "  plt.tight_layout()\n",
        "  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "  plt.show()\n",
        "  \n",
        "def display_training_curves(training, validation, title, subplot):\n",
        "  if subplot%10==1: # set up the subplots on the first call\n",
        "    plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
        "    plt.tight_layout()\n",
        "  ax = plt.subplot(subplot)\n",
        "  ax.set_facecolor('#F8F8F8')\n",
        "  ax.plot(training)\n",
        "  ax.plot(validation)\n",
        "  ax.set_title('model '+ title)\n",
        "  ax.set_ylabel(title)\n",
        "  ax.set_xlabel('epoch')\n",
        "  ax.legend(['train', 'valid.'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lzd6Qi464PsA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Colab-only auth for this notebook and the TPU"
      ]
    },
    {
      "metadata": {
        "id": "MPx0nvyUnvgT",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# backend identification\n",
        "IS_COLAB_BACKEND = 'COLAB_GPU' in os.environ  # this is always set on Colab, the value is 0 or 1 depending on GPU presence\n",
        "HAS_COLAB_TPU = 'COLAB_TPU_ADDR' in os.environ\n",
        "HAS_CTPU_TPU = 'TPU_NAME' in os.environ\n",
        "HAS_MANUAL_TPU = False\n",
        "# Uncomment the following line to work around the case when TPU_NAME is not set. Please set yout vm name manually.\n",
        "# HAS_MANUAL_TPU, MANUAL_VM_NAME = True, 'MY_VM_NAME'\n",
        "USE_TPU = HAS_COLAB_TPU or HAS_CTPU_TPU or HAS_MANUAL_TPU\n",
        "\n",
        "# Auth on Colab (little wrinkle: without auth, Colab will be extremely slow in accessing data from a GCS bucket, even public).\n",
        "if IS_COLAB_BACKEND:\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "  \n",
        "# Also propagate the Colab Auth to TPU so that it can access your GCS buckets, even private ones.\n",
        "if IS_COLAB_BACKEND and HAS_COLAB_TPU:\n",
        "  with tf.Session('grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])) as sess:    \n",
        "    with open('/content/adc.json', 'r') as f:\n",
        "      auth_info = json.load(f) # Upload the credentials to TPU.\n",
        "    tf.contrib.cloud.configure_gcs(sess, credentials=auth_info)\n",
        "\n",
        "# find the TPU\n",
        "if IS_COLAB_BACKEND and HAS_COLAB_TPU:\n",
        "  TPU_ADDRESS = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "elif HAS_CTPU_TPU:\n",
        "  TPU_ADDRESS = os.environ['TPU_NAME']\n",
        "elif HAS_MANUAL_TPU:\n",
        "  TPU_ADDRESS = MANUAL_VM_NAME\n",
        "      \n",
        "if USE_TPU:\n",
        "  print('Using TPU:', TPU_ADDRESS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w9S3uKC_iXY5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Configuration"
      ]
    },
    {
      "metadata": {
        "id": "M3G-2aUBQJ-H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "GCS_PATTERN = 'gs://flowers-public/tfrecords-jpeg-192x192/*.tfrec'\n",
        "IMAGE_SIZE = [192, 192]\n",
        "\n",
        "if USE_TPU:\n",
        "  BATCH_SIZE = 128  # On TPU in Keras, this is the per-core batch size. The global batch size is 8x this.\n",
        "else:\n",
        "  BATCH_SIZE = 32  # On Colab/GPU, a higher batch size does not help and sometimes does not fit on the GPU (OOM)\n",
        "\n",
        "VALIDATION_SPLIT = 0.19\n",
        "CLASSES = [b'daisy', b'dandelion', b'roses', b'sunflowers', b'tulips'] # do not change, maps to the labels in the data (folder names)\n",
        "\n",
        "# splitting data files between training and validation\n",
        "filenames = tf.gfile.Glob(GCS_PATTERN)\n",
        "split = int(len(filenames) * VALIDATION_SPLIT)\n",
        "training_filenames = filenames[split:]\n",
        "validation_filenames = filenames[:split]\n",
        "print(\"Pattern matches {} data files. Splitting dataset into {} training files and {} validation files\".format(len(filenames), len(training_filenames), len(validation_filenames)))\n",
        "validation_steps = int(3670 // len(filenames) * len(validation_filenames)) // BATCH_SIZE\n",
        "steps_per_epoch = int(3670 // len(filenames) * len(training_filenames)) // BATCH_SIZE\n",
        "print(\"With a batch size of {}, there will be {} batches per training epoch and {} batch(es) per validation run.\".format(BATCH_SIZE, steps_per_epoch, validation_steps))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kvPXiovhi3ZZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Read images and labels from TFRecords"
      ]
    },
    {
      "metadata": {
        "id": "LtAVr-4CP1rp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_tfrecord(example):\n",
        "    features = {\n",
        "        \"image\": tf.FixedLenFeature((), tf.string), # tf.string means byte string\n",
        "        \"label\": tf.FixedLenFeature((), tf.string),\n",
        "        \"one_hot_label\": tf.FixedLenFeature((), tf.string)\n",
        "    }\n",
        "    example = tf.parse_single_example(example, features)\n",
        "    image = tf.image.decode_jpeg(example['image'])\n",
        "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
        "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
        "    one_hot_label = tf.io.decode_raw(example['one_hot_label'], out_type=tf.uint8) # 'decode' byte string into byte list\n",
        "    one_hot_label = tf.cast(one_hot_label, tf.float32)  # convert one hot labels to floats\n",
        "    one_hot_label = tf.reshape(one_hot_label, [5])  # explicit fixed size needed on TPU\n",
        "    label = example['label']  # byte string\n",
        "    return image, label, one_hot_label\n",
        "\n",
        "def load_dataset(filenames):  \n",
        "  # read from tfrecs\n",
        "  records = tf.data.TFRecordDataset(filenames, num_parallel_reads=32)  # this will read from multiple GCS files in parallel\n",
        "  dataset = records.map(read_tfrecord, num_parallel_calls=32)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xb-b4PRz-V6O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "display_9_images_from_dataset(load_dataset(training_filenames))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "22rVDTx8wCqE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## training and validation datasets"
      ]
    },
    {
      "metadata": {
        "id": "7wxKyCklR4Gh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def features_and_targets(image, label, one_hot_label):\n",
        "  feature = image\n",
        "  target = one_hot_label\n",
        "  return feature, target  # for training, a Keras model needs 2 items: features and targets\n",
        "\n",
        "def get_batched_dataset(filenames):\n",
        "  dataset = load_dataset(filenames)\n",
        "  dataset = dataset.map(features_and_targets, num_parallel_calls=32)\n",
        "  dataset = dataset.cache()  # This dataset fits in RAM\n",
        "  dataset = dataset.repeat()\n",
        "  dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) # drop_remainder needed on TPU\n",
        "  dataset = dataset.prefetch(-1) # prefetch next batch while training (-1: autotune prefetch buffer size)\n",
        "  # should shuffle too but this dataset was well shuffled on disk already\n",
        "  return dataset\n",
        "\n",
        "def get_training_dataset():\n",
        "  return get_batched_dataset(training_filenames)\n",
        "\n",
        "def get_validation_dataset():\n",
        "  return get_batched_dataset(validation_filenames)\n",
        "\n",
        "some_flowers, some_labels, some_one_hot_labels = dataset_to_numpy_util(load_dataset(validation_filenames), 8*20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ALtRUlxhw8Vt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model [WORK REQUIRED]\n",
        "1. train the model as it is, with a single convolutional layer\n",
        " * Accuracy 55%... Not great.\n",
        "2. add additional convolutional layers interleaved max-pooling layers. Try also adding a second dense layer. For example:<br/>\n",
        "**`conv 3x3, 16 filters, relu`**<br/>\n",
        "**`conv 3x3, 30 filters, relu`**<br/>\n",
        "**`max pool 2x2`**<br/>\n",
        "**`conv 3x3, 50 filters, relu`**<br/>\n",
        "**`max pool 2x2`**<br/>\n",
        "**`conv 3x3, 70 filters, relu`**<br/>\n",
        "**`flatten`**<br/>\n",
        "**`dense 90 relu, relu`**<br/>\n",
        "**`dense 10 softmax`**<br/>\n",
        " * Accuracy 62%... slightly better. But this model is more than 3M parameters and it overfits dramatically (overfitting = eval loss goes up instead of down).\n",
        "3. Try replacing the Flatten layer by Global average pooling.\n",
        " * Accuracy 68% The model is back to a modest 50K parameters, works better than before and does not overfit anymore\n",
        "4. Try experimenting with 1x1 convolutions too. They typically follow a 3x3 convolution and decrease the filter count. You can also add dropout between the dense layers. For example:\n",
        "**`conv 3x3, 20 filters, relu`**<br/>\n",
        "**`conv 3x3, 50 filters, relu`**<br/>\n",
        "**`max pool 2x2`**<br/>\n",
        "**`conv 3x3, 70 filters, relu`**<br/>\n",
        "**`conv 1x1, 50 filters, relu`**<br/>\n",
        "**`max pool 2x2`**<br/>\n",
        "**`conv 3x3, 100 filters, relu`**<br/>\n",
        "**`conv 1x1, 70 filters, relu`**<br/>\n",
        "**`max pool 2x2`**<br/>\n",
        "**`conv 3x3, 120 filters, relu`**<br/>\n",
        "**`conv 1x1, 80 filters, relu`**<br/>\n",
        "**`max pool 2x2`**<br/>\n",
        "**`global average pooling`**<br/>\n",
        "**`dense 70 relu, relu`**<br/>\n",
        "**`dense 10 softmax`**<br/>\n",
        " * accuracy 73%\n",
        "5. The goal is 80% accuracy ! Good luck. (You might want to train for 20 epochs to get there. Se your traiing curves to see if it is worth training longer.)"
      ]
    },
    {
      "metadata": {
        "id": "XLJNVGwHUDy1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    # little wrinkle: specifying the input shape as a Keras InputLayer does not\n",
        "    # work on TPU yet. Please add an input shape on your first layer instead. \n",
        "    # l.InputLayer(input_shape=[*IMAGE_SIZE, 3]),\n",
        "    \n",
        "    ###\n",
        "    tf.keras.layers.Conv2D(kernel_size=3, filters=20, padding='same', activation='relu', input_shape=[*IMAGE_SIZE, 3]),\n",
        "    #\n",
        "    # YOUR LAYERS HERE\n",
        "    #\n",
        "    # LAYERS TO TRY:\n",
        "    # Conv2D(kernel_size=3, filters=30, padding='same', activation='relu')\n",
        "    # MaxPooling2D(pool_size=2)\n",
        "    # GlobalAveragePooling2D() / Flatten()\n",
        "    # Dense(90, activation='relu')\n",
        "    #\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "    ###\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "  optimizer=tf.train.AdamOptimizer(),\n",
        "  loss= 'categorical_crossentropy',\n",
        "  metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# little wrinkle: in eager mode, only tf.train.*Optimizer optimizers will work in model.compile,\n",
        "# not strings like 'adam' which map to Keras optimizers. Unfortunately the TF optimizers do not\n",
        "# support Keras learning rate schedules (tf.keras.callbacks.LearningRateScheduler).\n",
        "# A fix is in the works."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dMfenMQcxAAb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "H7QwBi6_ri4x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if USE_TPU:\n",
        "  strategy = tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS))\n",
        "  tpu_model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M-ID7vP5mIKs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "if USE_TPU:\n",
        "  history = tpu_model.fit(get_training_dataset, steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
        "                          validation_data=get_validation_dataset, validation_steps=validation_steps)\n",
        "  # Little wrinkle: reading directly from dataset object not yet implemented\n",
        "  # for Keras/TPU. Please use a function that returns a dataset.\n",
        "else:\n",
        "  history = model.fit(get_training_dataset(), steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
        "                      validation_data=get_validation_dataset(), validation_steps=validation_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VngeUBIdyJ1T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(history.history.keys())\n",
        "display_training_curves(history.history['acc'], history.history['val_acc'], 'accuracy', 211)\n",
        "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MKFMWzh0Yxsq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Predictions"
      ]
    },
    {
      "metadata": {
        "id": "yMEsR851VDZb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# randomize the input so that you can execute multiple times to change results\n",
        "permutation = np.random.permutation(8*20)\n",
        "some_flowers, some_labels, some_one_hot_labels = (some_flowers[permutation], some_labels[permutation], some_one_hot_labels[permutation])\n",
        "\n",
        "if USE_TPU:\n",
        "  restored_model = model\n",
        "  restored_model.set_weights(tpu_model.get_weights()) # this copies the weights from TPU to CPU\n",
        "  predictions = restored_model.predict(some_flowers)\n",
        "  evaluations = restored_model.evaluate(some_flowers, some_one_hot_labels)\n",
        "else:\n",
        "  predictions = model.predict(some_flowers, batch_size=16)\n",
        "  evaluations = model.evaluate(some_flowers, some_one_hot_labels, batch_size=16)\n",
        "  \n",
        "print(np.array(CLASSES)[np.argmax(predictions, axis=-1)].tolist())\n",
        "print('[val_loss, val_acc]', evaluations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qzCCDL1CZFx6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "display_9_images_with_predictions(some_flowers, predictions, some_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SVY1pBg5ydH-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## License"
      ]
    },
    {
      "metadata": {
        "id": "hleIN5-pcr0N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "author: Martin Gorner<br>\n",
        "twitter: @martin_gorner\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Copyright 2018 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is not an official Google product but sample code provided for an educational purpose\n"
      ]
    }
  ]
}