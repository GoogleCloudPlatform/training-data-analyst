{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WIT_Smile_Detector.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiNxsd4_q9wq",
        "colab_type": "text"
      },
      "source": [
        "### What-If Tool Image Smile Detection\n",
        "\n",
        "In this demo we demonstrate the use of what-if-tool for image recognition models. Our task is to predict if a person is smiling or not. We provide a CNN that is trained on a subset of [CelebA dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) and visualize the results on a separate test subset.\n",
        "\n",
        "Copyright 2019 Google LLC.\n",
        "SPDX-License-Identifier: Apache-2.0\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqB2tjOMETmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Install the What-If Tool widget if running in colab {display-mode: \"form\"}\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  !pip install --upgrade witwidget -q\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBOHfrOP7Iy5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Download the pretrained keras model files and subset of celeba images\n",
        "\n",
        "!curl -L https://storage.googleapis.com/what-if-tool-resources/smile-demo/smile-colab-model.hdf5 -o ./smile-model.hdf5\n",
        "!curl -L https://storage.googleapis.com/what-if-tool-resources/smile-demo/test_subset.zip -o ./test_subset.zip\n",
        "!unzip -qq -o test_subset.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H2nX-2dEgsR",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Define helper functions for dataset conversion from csv to tf.Examples\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Converts a dataframe into a list of tf.Example protos.\n",
        "# If images_path is specified, it assumes that the dataframe has a special \n",
        "# column \"image_id\" and the path \"images_path/image_id\" points to an image file.\n",
        "# Given this structure, this function loads and processes the images as png byte_lists\n",
        "# into tf.Examples so that they can be shown in WIT. Note that 'image/encoded'\n",
        "# is a reserved field in WIT for encoded image features.\n",
        "def df_to_examples(df, columns=None, images_path=''):\n",
        "  examples = []\n",
        "  if columns == None:\n",
        "    columns = df.columns.values.tolist()\n",
        "  for index, row in df.iterrows():\n",
        "    example = tf.train.Example()\n",
        "    for col in columns:\n",
        "      if df[col].dtype is np.dtype(np.int64):\n",
        "        example.features.feature[col].int64_list.value.append(int(row[col]))\n",
        "      elif df[col].dtype is np.dtype(np.float64):\n",
        "        example.features.feature[col].float_list.value.append(row[col])\n",
        "      elif row[col] == row[col]:\n",
        "        example.features.feature[col].bytes_list.value.append(row[col].encode('utf-8'))\n",
        "    if images_path:\n",
        "      fname = row['image_id']\n",
        "      with open(os.path.join(images_path, fname), 'rb') as f:\n",
        "        im = Image.open(f)\n",
        "        buf = BytesIO()\n",
        "        im.save(buf, format= 'PNG')\n",
        "        im_bytes = buf.getvalue()\n",
        "        example.features.feature['image/encoded'].bytes_list.value.append(im_bytes)\n",
        "    examples.append(example)\n",
        "  return examples\n",
        "\n",
        "# Converts a dataframe column into a column of 0's and 1's based on the provided test.\n",
        "# Used to force label columns to be numeric for binary classification using a TF estimator.\n",
        "def make_label_column_numeric(df, label_column, test):\n",
        "  df[label_column] = np.where(test(df[label_column]), 1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGpLKJI_HY9m",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Load the csv file into pandas dataframe and process it for WIT\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('celeba/data_test_subset.csv')\n",
        "examples = df_to_examples(data, images_path='celeba/img_test_subset_resized/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZR3i6UZlZ96",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Load the keras models\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model1 = load_model('smile-model.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5fYynA9ZpPJ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Define the custom predict function for WIT\n",
        "\n",
        "# This function extracts 'image/encoded' field, which is a reserved key for the \n",
        "# feature that contains encoded image byte list. We read this feature into \n",
        "# BytesIO and decode it back to an image using PIL.\n",
        "# The model expects an array of images that are floats in range 0.0 to 1.0 and \n",
        "# outputs a numpy array of (n_samples, n_labels)\n",
        "def custom_predict(examples_to_infer):\n",
        "  def load_byte_img(im_bytes):\n",
        "    buf = BytesIO(im_bytes)\n",
        "    return np.array(Image.open(buf), dtype=np.float64) / 255.\n",
        "\n",
        "  ims = [load_byte_img(ex.features.feature['image/encoded'].bytes_list.value[0]) \n",
        "         for ex in examples_to_infer]\n",
        "  preds = model1.predict(np.array(ims))\n",
        "  return preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldvP-msGPnIv",
        "colab_type": "text"
      },
      "source": [
        "## Note that this particular model only uses images as input. Therefore, partial dependence plots are flat for all features. These features are provided for slicing and analysis purposes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwiWGrLlSWGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Invoke What-If Tool for the data and model {display-mode: \"form\"}\n",
        "from witwidget.notebook.visualization import WitWidget, WitConfigBuilder\n",
        "\n",
        "num_datapoints = 250  #@param {type: \"number\"}\n",
        "tool_height_in_px = 700  #@param {type: \"number\"}\n",
        "\n",
        "# Decode an image from tf.example bytestring\n",
        "def decode_image(ex):\n",
        "  im_bytes = ex.features.feature['image/encoded'].bytes_list.value[0]\n",
        "  im = Image.open(BytesIO(im_bytes))\n",
        "  return im\n",
        "\n",
        "# Define the custom distance function that compares the average color of images\n",
        "def image_mean_distance(ex, exs, params):\n",
        "  selected_im = decode_image(ex)\n",
        "  mean_color = np.mean(selected_im, axis=(0,1))\n",
        "  image_distances = [np.linalg.norm(mean_color - np.mean(decode_image(e), axis=(0,1))) for e in exs]\n",
        "  return image_distances\n",
        "\n",
        "# Setup the tool with the test examples and the trained classifier\n",
        "config_builder = WitConfigBuilder(examples[:num_datapoints]).set_custom_predict_fn(\n",
        "    custom_predict).set_custom_distance_fn(image_mean_distance)\n",
        "\n",
        "wv = WitWidget(config_builder, height=tool_height_in_px)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1s1_SiOyS0l",
        "colab_type": "text"
      },
      "source": [
        "#### Exploration ideas\n",
        "\n",
        "- In the \"Performance\" tab, set the ground truth feature to \"Smiling\". You can set a scatter axis or binning option to be inference correct and analyze how it varies across other features (i.e. you can make a scatter plot of Young vs inference correct).\n",
        "- Choose an image and click on \"Show nearest counterfactual datapoint\", this will find another example that is closest to the selected image in terms of average color value, but has a different prediction (if selected image is predicted to be \"smiling\" the counterfactual one will have \"not smiling\" prediction).\n",
        "- Define your own custom distance function and set it by calling set_custom_distance_fn on config_builder and explore the counterfactuals. You can even load another neural network to compute distances!\n",
        "- You can slice by any one of the features and analyze the confusion matrix and accuracy for each group.\n",
        "- In the \"Datapoint Editor\" tab, you can upload your own image or download and modify one of the images to see how it affects the inference score.\n"
      ]
    }
  ]
}