{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Scaling up ML using Cloud ML </h1>\n",
    "\n",
    "This notebook is Lab3a of CPB 102, Google's course on Machine Learning using Cloud ML.\n",
    "\n",
    "In this notebook, we take a previously developed TensorFlow model to predict taxifare rides and package it up so that it can be run in Cloud ML. For now, we'll run this on a small dataset. The model that was developed is rather simplistic, and therefore, the accuracy of the model is not great either.  However, this notebook illustrates *how* to package up a TensorFlow model to run it within Cloud ML. \n",
    "\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "Later in the course, we will look at ways to make a more effective machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11.0rc0\n",
      "gs://cloud-ml/sdk/cloudml-0.1.6-alpha.dataflow.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.ml as ml\n",
    "import tensorflow as tf\n",
    "print tf.__version__\n",
    "print ml.sdk_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Environment variables for project and bucket </h2>\n",
    "\n",
    "Change the cell below to reflect your Project ID and bucket name. Note that:\n",
    "<ol>\n",
    "<li> Your project id is the *unique* string that identifies your project (not the project name). You can find this from the GCP Console dashboard's Home page.  My dashboard reads:  <b>Project ID:</b> cloud-training-demos </li>\n",
    "<li> Cloud training often involves saving and restoring model files. Therefore, we should <b>create a single-region bucket</b>. If you don't have a bucket already, I suggest that you create one from the GCP console (because it will dynamically check whether the bucket name you want is available) </li>\n",
    "</ol>\n",
    "\n",
    "The next cell ensures that your bucket is writeable by Cloud ML. You need to do this only once (not once per notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'cloud-training-demos'    # CHANGE THIS\n",
    "BUCKET = 'cloud-training-demos-ml'  # CHANGE THIS\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT # for bash\n",
    "os.environ['BUCKET'] = BUCKET # for bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authorizing Cloud ML service account cloud-ml-service@cml-663413318684.iam.gserviceaccount.com to write to cloud-training-demos-ml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100   127    0   127    0     0    655      0 --:--:-- --:--:-- --:--:--   658\n",
      "No changes to gs://cloud-training-demos-ml/\n",
      "No changes to gs://cloud-training-demos-ml/\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "PROJECT_ID=$PROJECT\n",
    "AUTH_TOKEN=$(gcloud auth print-access-token)\n",
    "SVC_ACCOUNT=$(curl -X GET -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AUTH_TOKEN\" https://ml.googleapis.com/v1beta1/projects/$PROJECT_ID:getConfig | python -c \"import json; import sys; response = json.load(sys.stdin); print response['serviceAccount']\")\n",
    "echo \"Authorizing Cloud ML service account $SVC_ACCOUNT to write to $BUCKET\"\n",
    "gsutil acl ch -u $SVC_ACCOUNT:WRITE gs://$BUCKET/\n",
    "gsutil defacl ch -u $SVC_ACCOUNT:O gs://$BUCKET/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the project and bucket settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project=cloud-training-demos\n",
      "bucket=cloud-training-demos-ml\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "echo \"project=$PROJECT\"\n",
    "echo \"bucket=$BUCKET\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Write code for preprocessing and feature engineering </h2>\n",
    "\n",
    "Realistic ML models involve a fair bit of preprocessing and feature engineering. The standard Cloud ML pipeline expects this. We haven't covered this in class yet, so we'll just pull out the input variables and pass them through untransformed (i.e. we will simply do identity() on the columns).\n",
    "\n",
    "<br/>\n",
    "\n",
    "Datalab can generate the following template code for you.  Just type <b>%mlalpha features</b> into an empty cell, and then fill out the path, headers, target, id.  Running that cell in turn will create Python code to define features. You can then edit it. (try it out by creating a new code block, and starting with %mlalpha features in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import google.cloud.ml.features as features\n",
    "\n",
    "#import google.cloud.ml as ml\n",
    "#print ml.sdk_location\n",
    "\n",
    "class TaxifareFeatures(object):\n",
    "  \"\"\"This class is generated from command line:\n",
    "        %ml features\n",
    "        path: ../lab1a/taxi-train.csv\n",
    "        headers: pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count,fare_amount\n",
    "        target: fare_amount\n",
    "        Please modify it as appropriate!!!\n",
    "  \"\"\"\n",
    "  csv_columns = ('pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count','fare_amount')\n",
    "  fare_amount = features.target('fare_amount').continuous()\n",
    "  inputs = [\n",
    "      features.numeric('pickup_longitude').identity(),\n",
    "      features.numeric('dropoff_longitude').identity(),\n",
    "      features.numeric('passenger_count').identity(),\n",
    "      features.numeric('pickup_latitude').identity(),\n",
    "      features.numeric('dropoff_latitude').identity(),\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Dataflow pipeline for preprocessing </h2>\n",
    "\n",
    "Dataflow pipeline code can also be created using code generation in Datalab.  Type <b>%ml preprocess</b> (or <b>%ml preprocess --cloud</b> to create a template with a DataflowPipelineRunner and gs:// paths) into an empty cell, run it, fill in some params and execute it again. (create a new code cell and try it out!)\n",
    "\n",
    "Note that this code references the features class above (TaxifareFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Direct usage of TextFileSink is deprecated. Please use 'textio.WriteToText()' instead of directly instantiating a TextFileSink object.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.direct_runner.DirectPipelineResult at 0x7fc65c8195d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# header\n",
    "\"\"\"\n",
    "Following code is generated from command line:\n",
    "%%mlalpha preprocess\n",
    "train_data_path: ../lab1a/taxi-train.csv\n",
    "eval_data_path: ../lab1a/taxi-valid.csv\n",
    "data_format: CSV\n",
    "output_dir: ./taxi_preproc\n",
    "feature_set_class_name: TaxifareFeatures\n",
    "\n",
    "Please modify as appropriate!!!\n",
    "\"\"\"\n",
    "\n",
    "# imports\n",
    "import apache_beam as beam\n",
    "import google.cloud.ml as ml\n",
    "import google.cloud.ml.io as io\n",
    "import os\n",
    "\n",
    "# defines\n",
    "feature_set = TaxifareFeatures()\n",
    "OUTPUT_DIR = './taxi_preproc'\n",
    "pipeline = beam.Pipeline('DirectPipelineRunner')\n",
    "\n",
    "# preprocessing\n",
    "training_data = beam.io.TextFileSource(\n",
    "    '../lab1a/taxi-train.csv',\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "train = pipeline | beam.Read('ReadTrainingData', training_data)\n",
    "eval_data = beam.io.TextFileSource(\n",
    "    '../lab1a/taxi-valid.csv',\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "eval = pipeline | beam.Read('ReadEvalData', eval_data)\n",
    "\n",
    "(metadata, train_features, eval_features) = ((train, eval) |\n",
    "   'Preprocess' >> ml.Preprocess(feature_set))\n",
    "\n",
    "(metadata\n",
    "   | 'SaveMetadata'\n",
    "   >> io.SaveMetadata(os.path.join(OUTPUT_DIR, 'metadata.yaml')))\n",
    "(train_features\n",
    "   | 'WriteTraining'\n",
    "   >> io.SaveFeatures(os.path.join(OUTPUT_DIR, 'features_train')))\n",
    "(eval_features\n",
    "   | 'WriteEval'\n",
    "   >> io.SaveFeatures(os.path.join(OUTPUT_DIR, 'features_eval')))\n",
    "\n",
    "# run pipeline\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above preprocessing code creates TFRecords, an efficient compressed format that is suitable for repeated training, distribution, and hyperparameter tuning. This is what our TensorFlow model receives. In addition, the preprocessing pipeline creates metadata.yaml, a set of statistics computed from the input data that is necessary for many of the input transformations covered in the next chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_eval-00000-of-00001.tfrecord.gz   metadata.yaml\r\n",
      "features_train-00000-of-00001.tfrecord.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls taxi_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns:\r\n",
      "  dropoff_latitude:\r\n",
      "    identity:\r\n",
      "      dtype: float\r\n",
      "    max: 41.366138\r\n",
      "    mean: 40.751569090250726\r\n",
      "    min: 40.514429\r\n",
      "    name: dropoff_latitude\r\n",
      "    transform: identity\r\n",
      "    type: numeric\r\n",
      "  dropoff_longitude:\r\n",
      "    identity:\r\n",
      "      dtype: float\r\n",
      "    max: -73.137393\r\n",
      "    mean: -73.97454913255157\r\n",
      "    min: -74.24316\r\n",
      "    name: dropoff_longitude\r\n",
      "    transform: identity\r\n",
      "    type: numeric\r\n",
      "  fare_amount:\r\n"
     ]
    }
   ],
   "source": [
    "!head -20 taxi_preproc/metadata.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Package up TensorFlow model </h2>\n",
    "\n",
    "The TensorFlow model needs to be packaged up into a Python module.  This has a very specific folder structure (you'd typically maintain this exact structure in your source repository). Then, you create an archive of it using the 'tar' command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxifare/\n",
      "taxifare/PKG-INFO\n",
      "taxifare/setup.cfg\n",
      "taxifare/setup.py\n",
      "taxifare/trainer/\n",
      "taxifare/trainer/__init__.py\n",
      "taxifare/trainer/task.py\n",
      "taxifare/trainer/taxifare.py\n",
      "taxifare/trainer.egg-info/\n",
      "taxifare/trainer.egg-info/dependency_links.txt\n",
      "taxifare/trainer.egg-info/PKG-INFO\n",
      "taxifare/trainer.egg-info/SOURCES.txt\n",
      "taxifare/trainer.egg-info/top_level.txt\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "tar cvfz taxifare.tar.gz taxifare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only three of those files are ones that you would actually edit, and one of them (taxifare.py) has meaningful code associated with it.\n",
    "\n",
    "The first is setup.py.  You would change it to reflect your module name, author, author_email and description. You might also add Python packages that you depend upon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "from setuptools import find_packages\r\n",
      "from setuptools import setup\r\n",
      "\r\n",
      "REQUIRED_PACKAGES = [\r\n",
      "]\r\n",
      "\r\n",
      "setup(\r\n",
      "    name='taxifare',\r\n",
      "    version='0.1',\r\n",
      "    author = 'Google',\r\n",
      "    author_email = 'training-feedback@cloud.google.com',\r\n",
      "    install_requires=REQUIRED_PACKAGES,\r\n",
      "    packages=find_packages(),\r\n",
      "    include_package_data=True,\r\n",
      "    description='CPB102 taxifare in Cloud ML',\r\n",
      "    requires=[]\r\n",
      ")\r\n"
     ]
    }
   ],
   "source": [
    "!grep -v \"^#\" taxifare/setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, the second is task.py. This file is canonical code to run your TensorFlow model by reading data in batches, setting up summary statistics, etc.  Most of this code will, in the future, move away from your Python module. For now, simply do a string-replace of 'taxifare' with the name of your Python module and add any hyperparameters you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import taxifare\r\n",
      "  parser.add_argument('--train_data_paths', type=str)\r\n",
      "  parser.add_argument('--eval_data_paths', type=str)\r\n",
      "  parser.add_argument('--metadata_path', type=str)\r\n",
      "  parser.add_argument('--output_path', type=str)\r\n",
      "  parser.add_argument('--max_steps', type=int, default=2000)\r\n",
      "      _, train_examples = taxifare.read_examples(\r\n",
      "          taxifare.create_inputs(metadata, input_data=train_examples))\r\n",
      "      output = taxifare.inference(inputs, metadata, HYPERPARAMS)\r\n",
      "      loss = taxifare.loss(output, targets)\r\n",
      "      train_op, global_step = taxifare.training(loss,\r\n",
      "    placeholder, inputs, _, keys = taxifare.create_inputs(metadata)\r\n",
      "    output = taxifare.inference(inputs, metadata, HYPERPARAMS)\r\n",
      "    _, examples = taxifare.read_examples(\r\n",
      "        taxifare.create_inputs(metadata, input_data=examples))\r\n",
      "    output = taxifare.inference(inputs, metadata, HYPERPARAMS)\r\n",
      "    loss = taxifare.loss(output, targets)\r\n"
     ]
    }
   ],
   "source": [
    "!grep -E \"add_argument|taxifare\" taxifare/trainer/task.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third one is taxifare.py -- this is the real TensorFlow model and the only one for which you have work to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Implementing TensorFlow model </h2>\n",
    "\n",
    "Here are the methods in taxifare.py that get called from task.py. It's your job to implement them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def read_examples(input_files, batch_size, shuffle, num_epochs=None):\r\n",
      "def create_inputs(metadata, input_data=None):\r\n",
      "def inference(inputs, metadata, hyperparams):\r\n",
      "def loss(output, targets):\r\n",
      "def training(loss_op, learning_rate):\r\n"
     ]
    }
   ],
   "source": [
    "!grep def taxifare/trainer/taxifare.py | grep -v \"def _\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the loss function for example.  This should feel familiar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def loss(output, targets):\r\n",
      "  \"\"\"Calculates the loss from the output and the labels.\r\n",
      "  Args:\r\n",
      "    output: output layer tensor, float - [batch_size].\r\n",
      "    targets: Target value tensor, float - [batch_size].\r\n",
      "  Returns:\r\n",
      "    loss_op: Loss tensor of type float.\r\n",
      "  \"\"\"\r\n",
      "  loss = tf.sqrt(tf.reduce_mean(tf.square(output - targets)), name = 'loss') # RMSE\r\n",
      "  return loss\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!grep -A 10 \"def loss\" taxifare/trainer/taxifare.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the cell above to look at the other functions.  Essentially, you'll implement your TensorFlow model in terms of these modules (or refactor an existing monolithic TensorFlow model into these modules) and put the pieces in the right spots:\n",
    "<ol>\n",
    "<li> create_inputs will take the input data and do any input transformations that you want to do. </li>\n",
    "<li> inference will create the TensorFlow ML model i.e. the computational graph. </li>\n",
    "<li> loss will specify what you want to optimize </li>\n",
    "<li> training will implement the training loop. You typically don't have to change this from the sample implementation </li>\n",
    "<li> read_examples can also be left as-is unless you want to change what preprocessing outputs or how batching happens (you probably don't). </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Running training locally </h2>\n",
    "\n",
    "Once you have a packaged TensorFlow model, you can run training by passing in the paths to your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type %mlalpha train into an empty cell, run it, fill in some params and execute it again. (create a new code cell and try it out!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf /content/training-data-analyst/CPB102/lab3a/taxi_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job Running...</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/_nocachecontent/master\" target=\"_blank\">master log</a>&nbsp;&nbsp;"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "master: INFO:root:Eval, step 269: rmse = 9.346<br/>master: <br/>master: INFO:root:Step 300: loss = 9.40 (0.023 sec)<br/>master: <br/>master: INFO:root:Step 400: loss = 11.74 (0.015 sec)<br/>master: <br/>master: INFO:root:Step 500: loss = 9.56 (0.010 sec)<br/>master: INFO:root:Step 600: loss = 10.53 (0.011 sec)<br/>master: <br/>master: INFO:root:Step 700: loss = 11.88 (0.012 sec)<br/>master: INFO:root:Step 800: loss = 9.31 (0.011 sec)<br/>master: <br/>master: INFO:root:Step 900: loss = 9.11 (0.010 sec)<br/>master: INFO:root:Step 1000: loss = 8.49 (0.010 sec)<br/>master: INFO:root:Final rmse after 1000 steps = 9.326<br/>master: INFO:root:Done training.<br/>master: inputs  =  [128, 5]<br/>master: inputs  =  [128, 5]<br/>master: inputs  =  [None, 5]<br/>master: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Job Finished.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%mlalpha train\n",
    "package_uris: /content/training-data-analyst/CPB102/lab3a/taxifare.tar.gz\n",
    "python_module: trainer.task\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  train_data_paths: /content/training-data-analyst/CPB102/lab3a/taxi_preproc/features_train*\n",
    "  eval_data_paths: /content/training-data-analyst/CPB102/lab3a/taxi_preproc/features_eval*\n",
    "  metadata_path: /content/training-data-analyst/CPB102/lab3a/taxi_preproc/metadata.yaml\n",
    "  output_path: /content/training-data-analyst/CPB102/lab3a/taxi_trained\n",
    "  max_steps: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval  logdir  model  summaries\r\n"
     ]
    }
   ],
   "source": [
    "!ls /content/training-data-analyst/CPB102/lab3a/taxi_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"b1b2def4-8a59-4449-b36d-10898182c73b\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"b1b2def4-8a59-4449-b36d-10898182c73b\", [{\"y\": [8.985296249389648, 6.553561210632324, 13.976161003112793, 9.956271171569824, 6.2447052001953125, 11.798726081848145, 8.376893043518066, 8.485426902770996, 9.430144309997559, 12.315016746520996], \"x\": [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], \"type\": \"scatter\", \"name\": \"loss-/content/training-data-analyst/CPB102/lab3a/taxi_trained/summaries\"}, {\"y\": [], \"x\": [], \"type\": \"scatter\", \"name\": \"accuracy-/content/training-data-analyst/CPB102/lab3a/taxi_trained/summaries\"}, {\"y\": [], \"x\": [], \"type\": \"scatter\", \"name\": \"loss-/content/training-data-analyst/CPB102/lab3a/taxi_trained/eval\"}, {\"y\": [], \"x\": [], \"type\": \"scatter\", \"name\": \"accuracy-/content/training-data-analyst/CPB102/lab3a/taxi_trained/eval\"}], {\"title\": \"loss,accuracy\", \"xaxis\": {\"title\": \"step\"}, \"yaxis\": {\"title\": \"loss,accuracy\"}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%mlalpha summary --dir /content/training-data-analyst/CPB102/lab3a/taxi_trained/summaries  /content/training-data-analyst/CPB102/lab3a/taxi_trained/eval  --name loss accuracy --step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is the RMSE on the training dataset; the accuracy is the RMSE on the validation dataset.  The loss is reported frequently since it is computed anyway, but we compute the accuracy only once every 30s of training, so there won't be as many points associated with the error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%tensorboard start --logdir /content/training-data-analyst/CPB102/lab3a/taxi_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%tensorboard stop --pid 6326"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training on cloud </h2>\n",
    "\n",
    "In order to train on the cloud, we have to copy the model and data to our bucket on Google Cloud Storage (GCS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxifare/\n",
      "taxifare/PKG-INFO\n",
      "taxifare/setup.cfg\n",
      "taxifare/setup.py\n",
      "taxifare/trainer/\n",
      "taxifare/trainer/__init__.py\n",
      "taxifare/trainer/task.py\n",
      "taxifare/trainer/taxifare.py\n",
      "taxifare/trainer.egg-info/\n",
      "taxifare/trainer.egg-info/dependency_links.txt\n",
      "taxifare/trainer.egg-info/PKG-INFO\n",
      "taxifare/trainer.egg-info/SOURCES.txt\n",
      "taxifare/trainer.egg-info/top_level.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://taxifare.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [0 files][    0.0 B/  6.3 KiB]                                                \r",
      "/ [1 files][  6.3 KiB/  6.3 KiB]                                                \r\n",
      "Operation completed over 1 objects/6.3 KiB.                                      \n",
      "Copying file://../lab1a/taxi-test.csv [Content-Type=text/csv]...\n",
      "/ [0 files][    0.0 B/ 79.6 KiB]                                                \r",
      "/ [1 files][ 79.6 KiB/ 79.6 KiB]                                                \r",
      "-\r",
      "Copying file://../lab1a/taxi-train.csv [Content-Type=text/csv]...\n",
      "- [1 files][ 79.6 KiB/450.2 KiB]                                                \r",
      "- [2 files][450.2 KiB/450.2 KiB]                                                \r",
      "\\\r",
      "Copying file://../lab1a/taxi-valid.csv [Content-Type=text/csv]...\n",
      "\\ [2 files][450.2 KiB/529.6 KiB]                                                \r",
      "\\ [3 files][529.6 KiB/529.6 KiB]                                                \r",
      "|\r\n",
      "Operation completed over 3 objects/529.6 KiB.                                    \n",
      "CommandException: 1 files/objects could not be removed.\n",
      "CommandException: 1 files/objects could not be removed.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "tar cvfz taxifare.tar.gz taxifare\n",
    "gsutil cp taxifare.tar.gz gs://$BUCKET/taxifare/source/taxifare.tar.gz\n",
    "gsutil cp ../lab1a/*.csv  gs://$BUCKET/taxifare/input/\n",
    "gsutil -m rm -r -f gs://$BUCKET/taxifare/taxi_preproc\n",
    "gsutil -m rm -r -f gs://$BUCKET/taxifare/taxi_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run your preprocessor, you have to change the input and output to be on GCS.  \n",
    "\n",
    "Using DirectPipelineRunner runs Dataflow locally, but the inputs & outputs are on the cloud. Using BlockingDataflowPipelineRunner will use Cloud Dataflow (and take much longer because of the overhead involved for such a small dataset). To see the status of your BlockingDataflowPipelineRunner job, visit https://console.cloud.google.com/dataflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Direct usage of TextFileSink is deprecated. Please use 'textio.WriteToText()' instead of directly instantiating a TextFileSink object.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.direct_runner.DirectPipelineResult at 0x7fc65c652b90>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import apache_beam as beam\n",
    "import google.cloud.ml as ml\n",
    "import google.cloud.ml.dataflow.io.tfrecordio as tfrecordio\n",
    "import google.cloud.ml.io as io\n",
    "import os\n",
    "\n",
    "# Change as needed\n",
    "RUNNER = 'DirectPipelineRunner'  # \n",
    "#RUNNER = 'BlockingDataflowPipelineRunner'\n",
    "\n",
    "# defines\n",
    "feature_set = TaxifareFeatures()\n",
    "OUTPUT_DIR = 'gs://{0}/taxifare/taxi_preproc'.format(BUCKET)\n",
    "\n",
    "pipeline = beam.Pipeline(argv=['--project', PROJECT,\n",
    "                               '--runner', RUNNER,\n",
    "                               '--job_name', 'lab3a',\n",
    "                               '--extra_package', ml.sdk_location,\n",
    "                               '--no_save_main_session', 'True',  # to prevent pickling and uploading Datalab itself!\n",
    "                               '--staging_location', 'gs://{0}/taxifare/staging'.format(BUCKET),\n",
    "                               '--temp_location', 'gs://{0}/taxifare/temp'.format(BUCKET)])\n",
    "\n",
    "\n",
    "# preprocessing\n",
    "training_data = beam.io.TextFileSource(\n",
    "    'gs://{0}/taxifare/input/taxi-train.csv'.format(BUCKET),\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "train = pipeline | beam.Read('ReadTrainingData', training_data)\n",
    "eval_data = beam.io.TextFileSource(\n",
    "    'gs://{0}/taxifare/input/taxi-valid.csv'.format(BUCKET),\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "eval = pipeline | beam.Read('ReadEvalData', eval_data)\n",
    "\n",
    "\n",
    "(metadata, train_features, eval_features) = ((train, eval) |\n",
    "   'Preprocess' >> ml.Preprocess(feature_set))\n",
    "\n",
    "(metadata\n",
    "   | 'SaveMetadata'\n",
    "   >> io.SaveMetadata(os.path.join(OUTPUT_DIR, 'metadata.yaml')))\n",
    "(train_features\n",
    "   | 'WriteTraining'\n",
    "   >> io.SaveFeatures(os.path.join(OUTPUT_DIR, 'features_train')))\n",
    "(eval_features\n",
    "   | 'WriteEval'\n",
    "   >> io.SaveFeatures(os.path.join(OUTPUT_DIR, 'features_eval')))\n",
    "\n",
    "# run pipeline\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-training-demos-ml/taxifare/taxi_preproc/features_eval-00000-of-00001.tfrecord.gz\n",
      "gs://cloud-training-demos-ml/taxifare/taxi_preproc/features_train-00000-of-00001.tfrecord.gz\n",
      "gs://cloud-training-demos-ml/taxifare/taxi_preproc/metadata.yaml\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil ls gs://$BUCKET/taxifare/taxi_preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, submit the training job to the cloud.  Cloud ML jobs usually take hours and are, therefore, queued. It may be a couple of minutes before your job starts being executed. This being a small job, though, the task should complete a few seconds later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up parameters for mlapha command.\n",
    "package_uris = 'gs://' + BUCKET + '/taxifare/source/taxifare.tar.gz'\n",
    "train_data_paths = 'gs://' + BUCKET + '/taxifare/taxi_preproc/features_train*'\n",
    "eval_data_paths = 'gs://' + BUCKET + '/taxifare/taxi_preproc/features_eval*'\n",
    "metadata_path = 'gs://' + BUCKET + '/taxifare/taxi_preproc/metadata.yaml'\n",
    "output_path = 'gs://' + BUCKET + '/taxifare/taxi_trained'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job \"trainer_task_170114_222854\" was submitted successfully.<br/>Run \"%mlalpha jobs --name trainer_task_170114_222854\" to view the status of the job.</p><p>Click <a href=\"https://console.developers.google.com/logs/viewer?project=cloud-training-demos&resource=ml.googleapis.com%2Fjob_id%2Ftrainer_task_170114_222854\" target=\"_blank\">here</a> to view cloud log. <br/>Start TensorBoard by running \"%tensorboard start --logdir=&lt;YourLogDir&gt;\".</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%mlalpha train --cloud\n",
    "package_uris: $package_uris\n",
    "python_module: trainer.task\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  train_data_paths: $train_data_paths\n",
    "  eval_data_paths: $eval_data_paths\n",
    "  metadata_path: $metadata_path\n",
    "  output_path: $output_path\n",
    "  max_steps: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>createTime: '2017-01-14T22:28:58Z'\n",
       "endTime: '2017-01-14T22:36:12Z'\n",
       "jobId: trainer_task_170114_222854\n",
       "startTime: '2017-01-14T22:34:39Z'\n",
       "state: SUCCEEDED\n",
       "trainingInput:\n",
       "  args: [--train_data_paths, 'gs://cloud-training-demos-ml/taxifare/taxi_preproc/features_train*',\n",
       "    --metadata_path, 'gs://cloud-training-demos-ml/taxifare/taxi_preproc/metadata.yaml',\n",
       "    --max_steps, '1000', --output_path, 'gs://cloud-training-demos-ml/taxifare/taxi_trained',\n",
       "    --eval_data_paths, 'gs://cloud-training-demos-ml/taxifare/taxi_preproc/features_eval*']\n",
       "  packageUris: ['gs://cloud-training-demos-ml/taxifare/source/taxifare.tar.gz']\n",
       "  pythonModule: trainer.task\n",
       "  region: us-central1\n",
       "trainingOutput: {consumedMLUnits: 0.17}\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%mlalpha jobs --name trainer_task_170114_222854"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prediction </h2>\n",
    "\n",
    "Make sure that the training job has completed before proceeding to this step (check the log above)\n",
    "\n",
    "To predict the taxifare for new inputs, you first have to deploy the trained model (deleting a previous one if necessary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-training-demos-ml/taxifare/taxi_preproc/metadata.yaml [Content-Type=text/plain]...\n",
      "/ [0 files][    0.0 B/  1.5 KiB]                                                \r",
      "/ [1 files][  1.5 KiB/  1.5 KiB]                                                \r\n",
      "Operation completed over 1 objects/1.5 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "# Work around https://buganizer.corp.google.com/issues/31730085\n",
    "gsutil cp gs://$BUCKET/taxifare/taxi_preproc/metadata.yaml gs://$BUCKET/taxifare/taxi_trained/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%mlalpha delete --name taxifare.v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%mlalpha deploy --name taxifare.v1 --path gs://$BUCKET/taxifare/taxi_trained/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response={u'predictions': [{u'score': [10.59544849395752], u'key': 1.0}]}\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json\n",
    "\n",
    "import google.cloud.ml.features as features\n",
    "from google.cloud.ml import session_bundle\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build('ml', 'v1beta1', credentials=credentials,\n",
    "            discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1beta1_discovery.json')\n",
    "\n",
    "request_data = {'instances':\n",
    "  [\n",
    "    {'examples':\n",
    "      {\n",
    "        'pickup_longitude': -73.885262,\n",
    "        'pickup_latitude': 40.773008,\n",
    "        'dropoff_longitude': -73.987232,\n",
    "        'dropoff_latitude': 40.732403,\n",
    "        'passenger_count': 2,\n",
    "        'fare_amount': -999\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "parent = 'projects/%s/models/%s/versions/%s' % (PROJECT, 'taxifare', 'v1')\n",
    "response = api.projects().predict(body=request_data, name=parent).execute()\n",
    "print \"response={0}\".format(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
