{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Feature Engineering </h1>\n",
    "\n",
    "This notebook is Lab4a of CPB 102, Google's course on Machine Learning using Cloud ML.\n",
    "\n",
    "This notebook demonstrates:\n",
    "<ol>\n",
    "<li> Reading data from BigQuery </li>\n",
    "<li> Carrying out preprocessing using the ML SDK </li>\n",
    "<li> Adding feature crosses in TensorFlow </li>\n",
    "</ol>\n",
    "\n",
    "Table of Contents:\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11.0rc0\n",
      "gs://cloud-ml/sdk/cloudml-0.1.6-alpha.dataflow.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.ml as ml\n",
    "import tensorflow as tf\n",
    "print tf.__version__\n",
    "print ml.sdk_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Environment variables for project and bucket </h2>\n",
    "\n",
    "Change the cell below to reflect your Project ID and bucket name. See Lab 3a for setup instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'cloud-training-demos'    # CHANGE THIS\n",
    "BUCKET = 'cloud-training-demos-ml'  # CHANGE THIS\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT # for bash\n",
    "os.environ['BUCKET'] = BUCKET # for bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Specifying query to pull the data </h2>\n",
    "\n",
    "The full dataset is 1 billion rows. For experimentation, let's sample it to create 10,000 samples.\n",
    "Later, we'll remove the limit and train on the full dataset.\n",
    "We're also using BigQuery sampling to pull out independent training and validation samples.\n",
    "\n",
    "Note that because the test dataset is now different, we can not really compare test statistics between this and the previous .csv methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT\n",
      "  DAYOFWEEK(pickup_datetime)*1.0 AS dayofweek,\n",
      "  HOUR(pickup_datetime)*1.0 AS hourofday,\n",
      "  pickup_longitude, pickup_latitude, \n",
      "  dropoff_longitude, dropoff_latitude,\n",
      "  passenger_count*1.0 AS passenger_count,\n",
      "  (tolls_amount + fare_amount) as fare_amount\n",
      "FROM\n",
      "  [nyc-tlc:yellow.trips]\n",
      "WHERE\n",
      "    trip_distance > 0\n",
      "    AND fare_amount >= 2.5\n",
      "    AND pickup_longitude > -78\n",
      "    AND pickup_longitude < -70\n",
      "    AND dropoff_longitude > -78\n",
      "    AND dropoff_longitude < -70\n",
      "    AND pickup_latitude > 37\n",
      "    AND pickup_latitude < 45\n",
      "    AND dropoff_latitude > 37\n",
      "    AND dropoff_latitude < 45\n",
      "    AND passenger_count > 0 \n",
      "   AND ABS(HASH(pickup_datetime)) % 100000 == 2\n"
     ]
    }
   ],
   "source": [
    "def create_query(phase, EVERY_N):\n",
    "  \"\"\"\n",
    "  phase: 1=train 2=valid\n",
    "  \"\"\"\n",
    "  base_query = \"\"\"\n",
    "SELECT\n",
    "  DAYOFWEEK(pickup_datetime)*1.0 AS dayofweek,\n",
    "  HOUR(pickup_datetime)*1.0 AS hourofday,\n",
    "  pickup_longitude, pickup_latitude, \n",
    "  dropoff_longitude, dropoff_latitude,\n",
    "  passenger_count*1.0 AS passenger_count,\n",
    "  (tolls_amount + fare_amount) as fare_amount\n",
    "FROM\n",
    "  [nyc-tlc:yellow.trips]\n",
    "WHERE\n",
    "    trip_distance > 0\n",
    "    AND fare_amount >= 2.5\n",
    "    AND pickup_longitude > -78\n",
    "    AND pickup_longitude < -70\n",
    "    AND dropoff_longitude > -78\n",
    "    AND dropoff_longitude < -70\n",
    "    AND pickup_latitude > 37\n",
    "    AND pickup_latitude < 45\n",
    "    AND dropoff_latitude > 37\n",
    "    AND dropoff_latitude < 45\n",
    "    AND passenger_count > 0 \n",
    "  \"\"\"\n",
    "\n",
    "  if EVERY_N == None:\n",
    "    if phase < 2:\n",
    "      # training\n",
    "      query = \"{0} AND ABS(HASH(pickup_datetime)) % 4 < 2\".format(base_query)\n",
    "    else:\n",
    "      query = \"{0} AND ABS(HASH(pickup_datetime)) % 4 == {1}\".format(base_query, phase)\n",
    "  else:\n",
    "      query = \"{0} AND ABS(HASH(pickup_datetime)) % {1} == {2}\".format(base_query, EVERY_N, phase)\n",
    "\n",
    "  \n",
    "    \n",
    "  return query\n",
    "    \n",
    "print create_query(2, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the query above in https://bigquery.cloud.google.com/table/nyc-tlc:yellow.trips if you want to see what it does (ADD LIMIT 10 to the query!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Preprocessing features using Cloud ML SDK </h2>\n",
    "\n",
    "We could discretize the lat-lon columns using the SDK, but we'll defer that to TensorFlow to enable it to be a hyper-parameter if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-ml/sdk/cloudml-0.1.6-alpha.dataflow.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.ml.features as features\n",
    "\n",
    "import google.cloud.ml as ml\n",
    "print ml.sdk_location\n",
    "\n",
    "class TaxifareFeatures(object):\n",
    "  csv_columns = ('dayofweek', 'hourofday', 'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count','fare_amount')\n",
    "  fare_amount = features.target('fare_amount').continuous()\n",
    "  pcount = features.numeric('passenger_count').scale()\n",
    "  plat = features.numeric('pickup_latitude').scale()\n",
    "  dlat = features.numeric('dropoff_latitude').scale()\n",
    "  plon = features.numeric('pickup_longitude').scale()\n",
    "  dlon = features.numeric('dropoff_longitude').scale()\n",
    "  dayofweek = features.numeric('dayofweek').identity()\n",
    "  hourofday = features.numeric('hourofday').identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Preprocessing Dataflow job from BigQuery </h2>\n",
    "\n",
    "This code reads from BigQuery and runs the above preprocessing, saving the data on Google Cloud.\n",
    "\n",
    "If you are running on the Cloud, you should go to the GCP Console (https://console.cloud.google.com/dataflow) to look at the status of the job. If you are running locally, you'll get a Running bar and it will take up to 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil -m rm -r -f gs://$BUCKET/taxifare/taxi_preproc4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Direct usage of TextFileSink is deprecated. Please use 'textio.WriteToText()' instead of directly instantiating a TextFileSink object.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.direct_runner.DirectPipelineResult at 0x7f3e14a90b50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import apache_beam as beam\n",
    "import google.cloud.ml as ml\n",
    "import google.cloud.ml.dataflow.io.tfrecordio as tfrecordio\n",
    "import google.cloud.ml.io as io\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Change as needed\n",
    "EVERY_N = 50 * 1000 # Change this to None to preprocess full dataset\n",
    "\n",
    "# Direct runs locally; Dataflow runs on the Cloud.\n",
    "RUNNER = 'DirectPipelineRunner'\n",
    "#RUNNER = 'DataflowPipelineRunner'\n",
    "\n",
    "OUTPUT_DIR = 'gs://{0}/taxifare/taxi_preproc4a/'.format(BUCKET)\n",
    "options = {\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "    'job_name': 'preprocess-taxifeatures' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S'),\n",
    "    'project': PROJECT,\n",
    "    'extra_packages': [ml.sdk_location],\n",
    "    'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "    'no_save_main_session': True\n",
    "}\n",
    "opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "pipeline = beam.Pipeline(RUNNER, options=opts)\n",
    "\n",
    "# defines\n",
    "feature_set = TaxifareFeatures()\n",
    "train_query = create_query(1, EVERY_N)\n",
    "valid_query = create_query(2, EVERY_N)\n",
    "train = pipeline | 'read_train' >> beam.Read(beam.io.BigQuerySource(query=train_query))\n",
    "eval = pipeline | 'read_valid' >> beam.Read(beam.io.BigQuerySource(query=valid_query))\n",
    "\n",
    "(metadata, train_features, eval_features) = ((train, eval) |\n",
    "   'Preprocess' >> ml.Preprocess(feature_set))\n",
    "\n",
    "(metadata\n",
    "   | 'SaveMetadata'\n",
    "   >> io.SaveMetadata(os.path.join(OUTPUT_DIR, 'metadata.yaml')))\n",
    "(train_features\n",
    "   | 'WriteTraining'\n",
    "   >> io.SaveFeatures(os.path.join(OUTPUT_DIR, 'features_train')))\n",
    "(eval_features\n",
    "   | 'WriteEval'\n",
    "   >> io.SaveFeatures(os.path.join(OUTPUT_DIR, 'features_eval')))\n",
    "\n",
    "# run pipeline\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-training-demos-ml/taxifare/taxi_preproc4a/features_eval-00000-of-00001.tfrecord.gz\r\n",
      "gs://cloud-training-demos-ml/taxifare/taxi_preproc4a/features_train-00000-of-00001.tfrecord.gz\r\n",
      "gs://cloud-training-demos-ml/taxifare/taxi_preproc4a/metadata.yaml\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://$BUCKET/taxifare/taxi_preproc4a/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-training-demos-ml/taxifare/taxi_preproc4a/features_eval-00000-of-00001.tfrecord.gz...\n",
      "/ [0 files][    0.0 B/624.3 KiB]                                                \r",
      "-\r",
      "- [1 files][624.3 KiB/624.3 KiB]                                                \r",
      "Copying gs://cloud-training-demos-ml/taxifare/taxi_preproc4a/features_train-00000-of-00001.tfrecord.gz...\n",
      "- [1 files][624.3 KiB/  1.3 MiB]                                                \r",
      "\\\r",
      "\\ [2 files][  1.3 MiB/  1.3 MiB]                                                \r",
      "Copying gs://cloud-training-demos-ml/taxifare/taxi_preproc4a/metadata.yaml...\n",
      "\\ [2 files][  1.3 MiB/  1.3 MiB]                                                \r",
      "\\ [3 files][  1.3 MiB/  1.3 MiB]                                                \r\n",
      "Operation completed over 3 objects/1.3 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil cp -R gs://$BUCKET/taxifare/taxi_preproc4a /content/training-data-analyst/CPB102/lab4a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3. Training </h2>\n",
    "\n",
    "Training requires you to package up your TensorFlow model into a Python package. We've done this in the directory 'taxifare'\n",
    "\n",
    "In that code, the latitude and longitude are discretized, and feature-crossed. The hourofday and dayofweek are divided into buckets that reflect typical traffic patterns.  The whole model is then trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def create_inputs(metadata, input_data=None):\n",
      "  with tf.name_scope('inputs'):\n",
      "    if input_data is None:\n",
      "      input_data = tf.placeholder(tf.string, name='input', shape=(None,))\n",
      "    parsed = features.FeatureMetadata.parse_features(metadata, input_data)\n",
      "\n",
      "    # [batchsize, 1] \n",
      "    plat = parsed['plat']\n",
      "    plon = parsed['plon']\n",
      "    dlat = parsed['dlat']\n",
      "    dlon = parsed['dlon']\n",
      "    dayofweek = parsed['dayofweek']\n",
      "    hourofday = parsed['hourofday']\n",
      "\n",
      "    # pickup is [batchsize, nbuckets**2]\n",
      "    pickup_index, pickup = feature_cross_latlon(plat, plon, 'pickup')\n",
      "    dropoff_index, dropoff = feature_cross_latlon(dlat, dlon, 'dropoff')\n",
      "    # pickupdropoff is [batchsize, nbuckets**4]\n",
      "    pickupdropoff = tf.squeeze(tf.one_hot(tf.mul(pickup_index, dropoff_index), depth=NUMBUCKETS**4, axis=-1), [1])\n",
      "    latdist = tf.abs(tf.sub(plat, dlat))\n",
      "    londist = tf.abs(tf.sub(plon, dlon))\n",
      "\n",
      "    _print_shape(plat, 'plat')\n",
      "    _print_shape(latdist, 'latdist')\n",
      "    _print_shape(pickup, 'pickup')\n",
      "    _print_shape(pickupdropoff, 'pickupdropoff')\n",
      "\n",
      "    # weekend/weekday, rush-hours and low-traffic nighttime\n",
      "    weekend = tf.to_float(tf.logical_or( tf.greater(dayofweek, 6.5), tf.less(dayofweek, 1.5), name='weekend')) #Sa,Su\n",
      "    night = tf.to_float(tf.logical_or( tf.greater(hourofday, 21.5), tf.less(hourofday, 6.5), name='night')) #10-6\n",
      "    morning = tf.to_float(tf.logical_or( tf.greater(hourofday, 6.5), tf.less(hourofday, 10.5), name='morning')) #7-10\n",
      "    evening = tf.to_float(tf.logical_or( tf.greater(hourofday, 15.5), tf.less(hourofday, 20.5), name='evening')) #4-8\n",
      "\n",
      "    # combine all the inputs\n",
      "    inputs = tf.concat(1, [plat, plon, dlat, dlon, latdist, londist, pickup, dropoff, pickupdropoff, weekend, night, morning, evening])\n",
      "    print 'inputs=',inputs.get_shape().as_list()\n",
      "\n",
      "    return (input_data, inputs, tf.squeeze(parsed['fare_amount']),\n",
      "            _create_fakekey(input_data)) # no key tf.identity(parsed['key']))\n",
      "\n",
      "def inference(inputs, metadata, hyperparams):\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "grep -A 40 create_inputs taxifare/trainer/taxifare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxifare/\n",
      "taxifare/PKG-INFO\n",
      "taxifare/setup.cfg\n",
      "taxifare/setup.py\n",
      "taxifare/trainer/\n",
      "taxifare/trainer/__init__.py\n",
      "taxifare/trainer/task.py\n",
      "taxifare/trainer/taxifare.py\n",
      "taxifare/trainer.egg-info/\n",
      "taxifare/trainer.egg-info/dependency_links.txt\n",
      "taxifare/trainer.egg-info/PKG-INFO\n",
      "taxifare/trainer.egg-info/SOURCES.txt\n",
      "taxifare/trainer.egg-info/top_level.txt\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "tar cvfz taxifare.tar.gz taxifare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job Running...</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/_nocachecontent/master\" target=\"_blank\">master log</a>&nbsp;&nbsp;"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "master: INFO:root:Step 2500: loss = 10.69 (0.008 sec)<br/>master: INFO:root:Final rmse after 2500 steps = 9.665<br/>master: <br/>master: INFO:root:Done training.<br/>master: plat  =  [128, 1]<br/>master: latdist  =  [128, 1]<br/>master: pickup  =  [128, 25]<br/>master: pickupdropoff  =  [128, 625]<br/>master: inputs= [128, 685]<br/>master: plat  =  [128, 1]<br/>master: latdist  =  [128, 1]<br/>master: pickup  =  [128, 25]<br/>master: pickupdropoff  =  [128, 625]<br/>master: inputs= [128, 685]<br/>master: plat  =  [None, 1]<br/>master: latdist  =  [None, 1]<br/>master: pickup  =  [None, 25]<br/>master: pickupdropoff  =  [None, 625]<br/>master: inputs= [None, 685]<br/>master: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Job Finished.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%mlalpha train\n",
    "package_uris: /content/training-data-analyst/CPB102/lab4a/taxifare.tar.gz\n",
    "python_module: trainer.task\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  train_data_paths: /content/training-data-analyst/CPB102/lab4a/taxi_preproc4a/features_train*\n",
    "  eval_data_paths: /content/training-data-analyst/CPB102/lab4a/taxi_preproc4a/features_eval*\n",
    "  metadata_path: /content/training-data-analyst/CPB102/lab4a/taxi_preproc4a/metadata.yaml\n",
    "  output_path: /content/training-data-analyst/CPB102/lab4a/taxi_trained\n",
    "  max_steps: 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
