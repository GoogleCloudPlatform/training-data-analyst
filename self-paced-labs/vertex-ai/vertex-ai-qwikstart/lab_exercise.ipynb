{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e68768",
   "metadata": {},
   "source": [
    "# Vertex AI: Qwik Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3be9d1",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "* Train a TensorFlow model locally in a hosted [**Vertex Notebook**](https://cloud.google.com/vertex-ai/docs/general/notebooks?hl=sv).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a746be",
   "metadata": {},
   "source": [
    "## Introduction: customer lifetime value (CLV) prediction with BigQuery and TensorFlow on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bf82e0",
   "metadata": {},
   "source": [
    "In this lab, you use [BigQuery](https://cloud.google.com/bigquery) for data processing and exploratory data analysis and the [Vertex AI](https://cloud.google.com/vertex-ai) platform to train and deploy a custom TensorFlow Regressor model to predict customer lifetime value (CLV). The goal of the lab is to introduce to Vertex AI through a high value real world use case - predictive CLV. You start with a local BigQuery and TensorFlow workflow that you may already be familiar with and progress toward training and deploying your model in the cloud with Vertex AI.\n",
    "\n",
    "![Vertex AI](./images/vertex-ai-overview.png \"Vertex AI Overview\")\n",
    "\n",
    "Vertex AI is Google Cloud's next generation, unified platform for machine learning development and the successor to AI Platform announced at Google I/O in May 2021. By developing machine learning solutions on Vertex AI, you can leverage the latest ML pre-built components and AutoML to significantly enhance development productivity, the ability to scale your workflow and decision making with your data, and accelerate time to value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe3b8c6",
   "metadata": {},
   "source": [
    "### Predictive CLV: how much monetary value existing customers will bring to the business in the future\n",
    "\n",
    "Predictive CLV is a high impact ML business use case. CLV is a customer's past value plus their predicted future value. The goal of predictive CLV is to predict how much monetary value a user will bring to the business in a defined future time range based on historical transactions.\n",
    "\n",
    "By knowing CLV, you can develop positive ROI strategies and make decisions about how much money to invest in acquiring new customers and retaining existing ones to grow revenue and profit.\n",
    "\n",
    "Once your ML model is a success, you can use the results to identify customers more likely to spend money than the others, and make them respond to your offers and discounts with a greater frequency. These customers, with higher lifetime value, are your main marketing target to increase revenue.\n",
    "\n",
    "By using the machine learning approach to predict your customers' value you will use in this lab, you can prioritize your next actions, such as the following:\n",
    "\n",
    "* Decide which customers to target with advertising to increase revenue.\n",
    "* Identify which customer segments are most profitable and plan how to move customers from one segment to another.\n",
    "\n",
    "Your task is to predict the future value for existing customers based on their known transaction history. \n",
    "\n",
    "![CLV](./images/clv-rfm.svg \"Customer Lifetime Value\")  \n",
    "Source: [Cloud Architecture Center - Predicting Customer Lifetime Value with AI Platform: training the models](https://cloud.google.com/architecture/clv-prediction-with-offline-training-train)\n",
    "\n",
    "There is a strong positive correlation between the recency, frequency, and amount of money spent on each purchase each customer makes and their CLV. Consequently, you leverage these features to in your ML model. For this lab, they are defined as:\n",
    "\n",
    "* **Recency**: The time between the last purchase and today, represented by the distance between the rightmost circle and the vertical dotted line that's labeled \"Now\".\n",
    "* **Frequency**: The time between purchases, represented by the distance between the circles on a single line.\n",
    "* **Monetary**: The amount of money spent on each purchase, represented by the size of the circle. This amount could be the average order value or the quantity of products that the customer ordered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46a1982",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc29eb23",
   "metadata": {},
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4c2e53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add installed library dependencies to Python PATH variable.\n",
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ead7a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve and set PROJECT_ID and REGION environment variables.\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "# Replace the value below with your assigned lab region.\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d4df6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a globally unique Google Cloud Storage bucket for artifact storage.\n",
    "GCS_BUCKET = f\"{PROJECT_ID}-bucket\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ab23c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil mb -l $REGION gs://$GCS_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8018cc87",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412ffc51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecf21cb",
   "metadata": {},
   "source": [
    "### Initialize the Vertex Python SDK client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301853d",
   "metadata": {},
   "source": [
    "Import the Vertex SDK for Python into your Python environment and initialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6029df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=f\"gs://{GCS_BUCKET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf880707",
   "metadata": {},
   "source": [
    "## Download and process the lab data into BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742ceefd",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "In this lab, you use the publicly available [Online Retail data set](https://archive.ics.uci.edu/ml/datasets/online+retail) from the UCI Machine Learning Repository. This dataset contains 541,909 transnational customer transactions occuring between (YYYY-MM-DD) 2010-12-01 and 2011-12-09 for a UK-based and registered non-store retailer. The company primarily sells unique all-occasion gifts. Many of the company's customers are wholesalers.\n",
    "\n",
    "**Citation**  \n",
    "Dua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository http://archive.ics.uci.edu/ml. Irvine, CA: University of California, School of Information and Computer Science.\n",
    "\n",
    "This lab is also inspired by the Google Cloud Architect Guide Series [Predicting Customer Lifetime Value with AI Platform: introduction](https://cloud.google.com/architecture/clv-prediction-with-offline-training-intro)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7d9d01",
   "metadata": {},
   "source": [
    "### Data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4efbb9",
   "metadata": {},
   "source": [
    "Execute the command below to ingest the lab data from the UCI Machine Learning repository into `Cloud Storage` and then upload to `BigQuery` for data processing. The data ingestion and processing scripts are available under the `utils` folder in the lab directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7720d05e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BigQuery constants. Please leave these unchanged.\n",
    "BQ_DATASET_NAME=\"online_retail\"\n",
    "BQ_RAW_TABLE_NAME=\"online_retail_clv_raw\"\n",
    "BQ_CLEAN_TABLE_NAME=\"online_retail_clv_clean\"\n",
    "BQ_ML_TABLE_NAME=\"online_retail_clv_ml\"\n",
    "BQ_URI=f\"bq://{PROJECT_ID}.{BQ_DATASET_NAME}.{BQ_ML_TABLE_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557df7b2",
   "metadata": {},
   "source": [
    "**Note**: This Python script will take about 2-3 min to download and process the lab data file. Follow along with logging output in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42e87bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python utils/data_download.py \\\n",
    "  --PROJECT_ID={PROJECT_ID} \\\n",
    "  --GCS_BUCKET={GCS_BUCKET} \\\n",
    "  --BQ_RAW_TABLE_NAME={BQ_RAW_TABLE_NAME} \\\n",
    "  --BQ_CLEAN_TABLE_NAME={BQ_CLEAN_TABLE_NAME} \\\n",
    "  --BQ_ML_TABLE_NAME={BQ_ML_TABLE_NAME} \\\n",
    "  --URL=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online Retail.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca57a9f",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7293fc2",
   "metadata": {},
   "source": [
    "As is the case with many real-world datasets, the lab dataset required some cleanup for you to utilize this historical customer transaction data for predictive CLV.\n",
    "\n",
    "The following changes were applied:\n",
    "\n",
    "* Keep only records that have a Customer ID.\n",
    "* Aggregate transactions by day from Invoices.\n",
    "* Keep only records that have positive order quantities and monetary values.\n",
    "* Aggregate transactions by Customer ID and compute recency, frequency, monetary features as well as the prediction target.\n",
    "\n",
    "**Features**:\n",
    "- `customer_country` (CATEGORICAL): customer purchase country.\n",
    "- `n_purchases` (NUMERIC): number of purchases made in feature window. (frequency)\n",
    "- `avg_purchase_size` (NUMERIC): average unit purchase count in feature window. (monetary)\n",
    "- `avg_purchase_revenue` (NUMERIC): average GBP purchase amount in in feature window. (monetary)\n",
    "- `customer_age` (NUMERIC): days from first purchase in feature window.\n",
    "- `days_since_last_purchase` (NUMERIC): days from the most recent purchase in the feature window. (recency)  \n",
    "\n",
    "**Target**: \n",
    "- `target_monetary_value_3M` (NUMERIC): customer revenue from the entire study window including feature and prediction windows.\n",
    "\n",
    "Note: This lab demonstrates a simple way to use a DNN predict customer 3-month ahead CLV monetary value based solely on the available dataset historical transaction history. Additional factors to consider in practice when using CLV to inform interventions include customer acquisition costs, profit margins, and discount rates to arrive at the present value of future customer cash flows. One of a DNN's benefits over traditional probabilistic modeling approaches is their ability to incorporate additional categorical and unstructured features; this is a great feature engineering opportunity to explore beyond this lab which just explores the RFM numeric features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402abff6",
   "metadata": {},
   "source": [
    "## Exploratory data analysis (EDA) in BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fa4d6c",
   "metadata": {},
   "source": [
    "Below you use BigQuery from this notebook to do exploratory data analysis to get to know this dataset and identify opportunities for data cleanup and feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c50cbe",
   "metadata": {},
   "source": [
    "### Recency: how recently have customers purchased?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50110392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bigquery recency\n",
    "\n",
    "SELECT \n",
    "  days_since_last_purchase\n",
    "FROM \n",
    "  `online_retail.online_retail_clv_ml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75edeba1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recency.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bc69b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recency.hist(bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e857fb43",
   "metadata": {},
   "source": [
    "From the chart, there are clearly a few different customer groups here such as loyal customers that have made purchases in the last few days as well as inactive customers that have not purchased in 250+ days. Using CLV predictions and insights, you can strategize on marketing and promotional interventions to improve customer purchase recency and re-active dormant customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4d8860",
   "metadata": {},
   "source": [
    "### Frequency: how often are customers purchasing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34402015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bigquery frequency\n",
    "\n",
    "SELECT\n",
    "  n_purchases\n",
    "FROM\n",
    "  `online_retail.online_retail_clv_ml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1fd5c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frequency.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbeac7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frequency.hist(bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c933f5",
   "metadata": {},
   "source": [
    "From the chart and quantiles, you can see that half of the customers have less than or equal to only 2 purchases. You can also tell from the average purchases > median purchases and max purchases of 81 that there are customers, likely wholesalers, who have made significantly more purchases. This should have you already thinking about feature engineering opportunities such as bucketizing purchases and removing or clipping outlier customers. You can also explore alternative modeling strategies for CLV on new customers who have only made 1 purchase as the approach demonstrated in this lab will perform better on customers with more relationship transactional history. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c0c043",
   "metadata": {},
   "source": [
    "### Monetary: how much are customers spending?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8d00ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bigquery monetary\n",
    "\n",
    "SELECT\n",
    "  target_monetary_value_3M\n",
    "FROM\n",
    "`online_retail.online_retail_clv_ml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a5010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "monetary.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b651c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "monetary['target_monetary_value_3M'].plot(kind='box', title=\"Target Monetary Value 3M: wide range, long right tail distribution\", grid=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc60b98",
   "metadata": {},
   "source": [
    "From the chart and summary statistics, you can see there is a wide range in customer monetary value ranging from 2.90 to 268,478 GBP. Looking at the quantiles, it is clear there are a few outlier customers whose monetary value is greater than 3 standard deviations from the mean. With this small dataset, it is recommended to remove these outlier customer values to treat separately, change your model's loss function to be more resistant to outliers, log the target feature, or clip their values to a maximum threshold. You should also be revisiting your CLV business requirements to see if binning customer monetary value and reframing this as a ML classification problem would suit your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e553fd",
   "metadata": {},
   "source": [
    "### Establish a simple model performance baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08221502",
   "metadata": {},
   "source": [
    "In order to evaluate the performance of your custom TensorFlow DNN Regressor model you will build in the next steps, it is a ML best practice to establish a simple performance baseline. Below is a simple SQL baseline that multiplies a customer's average purchase spent compounded by their daily purchase rate and computes standard regression metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf088864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "\n",
    "WITH\n",
    "  day_intervals AS (\n",
    "  SELECT\n",
    "      customer_id,\n",
    "      DATE_DIFF(DATE('2011-12-01'), DATE('2011-09-01'), DAY) AS target_days,\n",
    "      DATE_DIFF(DATE('2011-09-01'), MIN(order_date), DAY) AS feature_days,\n",
    "  FROM\n",
    "    `online_retail.online_retail_clv_clean`\n",
    "  GROUP BY\n",
    "      customer_id\n",
    "  ),\n",
    "    \n",
    "  predicted_clv AS (\n",
    "  SELECT\n",
    "      customer_id,\n",
    "      AVG(avg_purchase_revenue) * (COUNT(n_purchases) * (1 + SAFE_DIVIDE(COUNT(target_days),COUNT(feature_days)))) AS predicted_monetary_value_3M,\n",
    "      SUM(target_monetary_value_3M) AS target_monetary_value_3M\n",
    "  FROM\n",
    "    `online_retail.online_retail_clv_ml`\n",
    "  LEFT JOIN day_intervals USING(customer_id)\n",
    "  GROUP BY\n",
    "      customer_id\n",
    "  )\n",
    "\n",
    "# Calculate overall baseline regression metrics.\n",
    "SELECT\n",
    "  ROUND(AVG(ABS(predicted_monetary_value_3M - target_monetary_value_3M)), 2) AS MAE,\n",
    "  ROUND(AVG(POW(predicted_monetary_value_3M - target_monetary_value_3M, 2)), 2) AS MSE,\n",
    "  ROUND(SQRT(AVG(POW(predicted_monetary_value_3M - target_monetary_value_3M, 2))), 2) AS RMSE\n",
    "FROM\n",
    "  predicted_clv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956ac010",
   "metadata": {},
   "source": [
    "These baseline results provide further support for the strong impact of outliers. The extremely high MSE comes from the exponential penalty applied to missed predictions and the magnitude of error on a few predictions.\n",
    "\n",
    "Next, you should look to plot the baseline results to get a sense of opportunity areas for you ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e14ff67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bigquery baseline\n",
    "\n",
    "WITH\n",
    "  day_intervals AS (\n",
    "  SELECT\n",
    "      customer_id,\n",
    "      DATE_DIFF(DATE('2011-12-01'), DATE('2011-09-01'), DAY) AS target_days,\n",
    "      DATE_DIFF(DATE('2011-09-01'), MIN(order_date), DAY) AS feature_days,\n",
    "  FROM\n",
    "    `online_retail.online_retail_clv_clean`\n",
    "  GROUP BY\n",
    "      customer_id\n",
    "  ),\n",
    "    \n",
    "  predicted_clv AS (\n",
    "  SELECT\n",
    "      customer_id,\n",
    "      AVG(avg_purchase_revenue) * (COUNT(n_purchases) * (1 + SAFE_DIVIDE(COUNT(target_days),COUNT(feature_days)))) AS predicted_monetary_value_3M,\n",
    "      SUM(target_monetary_value_3M) AS target_monetary_value_3M\n",
    "  FROM\n",
    "    `online_retail.online_retail_clv_ml`\n",
    "  INNER JOIN day_intervals USING(customer_id)\n",
    "  GROUP BY\n",
    "      customer_id\n",
    "  )\n",
    "\n",
    "SELECT\n",
    " *\n",
    "FROM\n",
    "  predicted_clv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afda73aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "baseline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a543c10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = baseline.plot(kind='scatter',\n",
    "                   x='predicted_monetary_value_3M', \n",
    "                   y='target_monetary_value_3M',\n",
    "                   title='Actual vs. Predicted customer 3-month monetary value',\n",
    "                   figsize=(5,5),\n",
    "                   grid=True)\n",
    "\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "]\n",
    "\n",
    "# now plot both limits against eachother\n",
    "ax.plot(lims, lims, 'k-', alpha=0.5, zorder=0)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlim(lims)\n",
    "ax.set_ylim(lims);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d53ad3a",
   "metadata": {},
   "source": [
    "## Train a TensorFlow model locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3658b32",
   "metadata": {},
   "source": [
    "Now that you have a simple baseline to benchmark your performance against, train a TensorFlow Regressor to predict CLV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45e2feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT data_split, COUNT(*)\n",
    "FROM `online_retail.online_retail_clv_ml`\n",
    "GROUP BY data_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e2994a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bigquery clv\n",
    "\n",
    "SELECT *\n",
    "FROM `online_retail.online_retail_clv_ml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80339852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clv_train = clv.loc[clv.data_split == 'TRAIN', :]\n",
    "clv_dev = clv.loc[clv.data_split == 'VALIDATE', :]\n",
    "clv_test = clv.loc[clv.data_split == 'TEST', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e9683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model training constants.\n",
    "# Virtual epochs design pattern:\n",
    "# https://medium.com/google-cloud/ml-design-pattern-3-virtual-epochs-f842296de730\n",
    "N_TRAIN_EXAMPLES = 2638\n",
    "STOP_POINT = 20.0\n",
    "TOTAL_TRAIN_EXAMPLES = int(STOP_POINT * N_TRAIN_EXAMPLES)\n",
    "BATCH_SIZE = 32\n",
    "N_CHECKPOINTS = 10\n",
    "STEPS_PER_EPOCH = (TOTAL_TRAIN_EXAMPLES // (BATCH_SIZE*N_CHECKPOINTS))\n",
    "\n",
    "NUMERIC_FEATURES = [\n",
    "    \"n_purchases\",\n",
    "    \"avg_purchase_size\",\n",
    "    \"avg_purchase_revenue\",\n",
    "    \"customer_age\",\n",
    "    \"days_since_last_purchase\",\n",
    "]\n",
    "\n",
    "LABEL = \"target_monetary_value_3M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627cc31a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def df_dataset(df):\n",
    "    \"\"\"Transform Pandas Dataframe to TensorFlow Dataset.\"\"\"\n",
    "    return tf.data.Dataset.from_tensor_slices((df[NUMERIC_FEATURES].to_dict('list'), df[LABEL].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0744b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainds = df_dataset(clv_train).prefetch(1).batch(BATCH_SIZE).repeat()\n",
    "devds = df_dataset(clv_dev).prefetch(1).batch(BATCH_SIZE)\n",
    "testds = df_dataset(clv_test).prefetch(1).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9459079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.metrics import RootMeanSquaredError\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"Custom RMSE regression metric.\"\"\"\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"Build and compile a TensorFlow Keras Regressor.\"\"\"\n",
    "\n",
    "    # Define input feature tensors and input layers.\n",
    "    input_layers = {\n",
    "    feature: tf.keras.layers.Input(name=feature, shape=(1,), dtype=tf.float32) \n",
    "    for feature in NUMERIC_FEATURES\n",
    "    }\n",
    "\n",
    "    inputs = tf.keras.layers.concatenate([\n",
    "    tf.keras.layers.Normalization(axis=-1)(input_layers[feature]) \n",
    "    for feature in NUMERIC_FEATURES\n",
    "    ])\n",
    "\n",
    "    d1 = tf.keras.layers.Dense(256, activation=tf.nn.relu, name='d1')(inputs)\n",
    "    d2 = tf.keras.layers.Dropout(0.2, name='d2')(d1)\n",
    "\n",
    "    # Note: the single neuron output for regression.\n",
    "    output = tf.keras.layers.Dense(1, name='output')(d2)\n",
    "\n",
    "    model = tf.keras.Model(input_layers, output, name='online-retail-clv')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "    # Note: MAE loss is more resistant to outliers than MSE.\n",
    "    model.compile(loss=tf.keras.losses.MAE,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['mae', 'mse', RootMeanSquaredError()])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8601ff5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=False, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354206ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir='./local-training/tensorboard',\n",
    "    histogram_freq=1)\n",
    "\n",
    "earlystopping_callback = tf.keras.callbacks.EarlyStopping(patience=1)\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='./local-training/checkpoints/my_model_weights.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730181fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(trainds,\n",
    "                    validation_data=devds,\n",
    "                    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                    epochs=N_CHECKPOINTS,\n",
    "                    callbacks=[[tensorboard_callback,\n",
    "                                earlystopping_callback,\n",
    "                                checkpoint_callback]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2594d084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOSS_COLS = [\"loss\", \"val_loss\"]\n",
    "\n",
    "pd.DataFrame(history.history)[LOSS_COLS].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71775db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_pred = model.predict(df_dataset(clv_train).prefetch(1).batch(BATCH_SIZE))\n",
    "dev_pred = model.predict(devds)\n",
    "test_pred = model.predict(testds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6eceb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_results = pd.DataFrame({'actual': clv_train['target_monetary_value_3M'].to_numpy(), 'predicted': np.squeeze(train_pred)}, columns=['actual', 'predicted'])\n",
    "dev_results = pd.DataFrame({'actual': clv_dev['target_monetary_value_3M'].to_numpy(), 'predicted': np.squeeze(dev_pred)}, columns=['actual', 'predicted'])\n",
    "test_results = pd.DataFrame({'actual': clv_test['target_monetary_value_3M'].to_numpy(), 'predicted': np.squeeze(test_pred)}, columns=['actual', 'predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4659dd09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model prediction calibration plots.\n",
    "fig, (train_ax, dev_ax, test_ax) = plt.subplots(1, 3, figsize=(15,15))\n",
    "\n",
    "train_results.plot(kind='scatter',\n",
    "                  x='predicted',\n",
    "                  y='actual',\n",
    "                  title='Train: act vs. pred customer 3M monetary value',\n",
    "                  grid=True,\n",
    "                  ax=train_ax)\n",
    "\n",
    "train_lims = [\n",
    "    np.min([train_ax.get_xlim(), train_ax.get_ylim()]),  # min of both axes\n",
    "    np.max([train_ax.get_xlim(), train_ax.get_ylim()]),  # max of both axes\n",
    "]\n",
    "\n",
    "train_ax.plot(train_lims, train_lims, 'k-', alpha=0.5, zorder=0)\n",
    "train_ax.set_aspect('equal')\n",
    "train_ax.set_xlim(train_lims)\n",
    "train_ax.set_ylim(train_lims)\n",
    "\n",
    "dev_results.plot(kind='scatter',\n",
    "                  x='predicted',\n",
    "                  y='actual',\n",
    "                  title='Dev: act vs. pred customer 3M monetary value',\n",
    "                  grid=True,\n",
    "                  ax=dev_ax)\n",
    "\n",
    "dev_lims = [\n",
    "    np.min([dev_ax.get_xlim(), dev_ax.get_ylim()]),  # min of both axes\n",
    "    np.max([dev_ax.get_xlim(), dev_ax.get_ylim()]),  # max of both axes\n",
    "]\n",
    "\n",
    "dev_ax.plot(dev_lims, dev_lims, 'k-', alpha=0.5, zorder=0)\n",
    "dev_ax.set_aspect('equal')\n",
    "dev_ax.set_xlim(dev_lims)\n",
    "dev_ax.set_ylim(dev_lims)\n",
    "\n",
    "test_results.plot(kind='scatter',\n",
    "                  x='predicted',\n",
    "                  y='actual',\n",
    "                  title='Test: act vs. pred customer 3M monetary value',\n",
    "                  grid=True,\n",
    "                  ax=test_ax)\n",
    "\n",
    "test_lims = [\n",
    "    np.min([test_ax.get_xlim(), test_ax.get_ylim()]),  # min of both axes\n",
    "    np.max([test_ax.get_xlim(), test_ax.get_ylim()]),  # max of both axes\n",
    "]\n",
    "\n",
    "test_ax.plot(test_lims, test_lims, 'k-', alpha=0.5, zorder=0)\n",
    "test_ax.set_aspect('equal')\n",
    "test_ax.set_xlim(test_lims)\n",
    "test_ax.set_ylim(test_lims);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5f1582",
   "metadata": {},
   "source": [
    "You have trained a model better than your baseline. As indicated in the charts above, there is still additional feature engineering and data cleaning opportunities to improve your model's performance on customers with CLV. Some options include handling these customers as a separate prediction task, applying a log transformation to your target, clipping their value or dropping these customers all together to improve model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc312cf",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ab0ae3",
   "metadata": {},
   "source": [
    "Congratulations! In this lab, you walked through a machine learning experimentation workflow using Google Cloud's BigQuery for data storage and analysis and Vertex AI machine learning services to train and deploy a TensorFlow model to predict customer lifetime value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0749f152",
   "metadata": {},
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2cfd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
