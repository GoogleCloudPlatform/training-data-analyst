{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Question Answering with Generative Models on Vertex AI\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/examples/prompt-design/question_answering.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/examples/prompt-design/question_answering.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/language/examples/prompt-design/question_answering.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Large language models can be used for various natural language processing tasks, including question-answering (Q&A). These models are trained on a vast amount text data and can generate high-quality responses to a wide range of questions. One thing to note here is that most models have cutoff dates regarding their knowledge, and asking anything too recent might yield an incomplete, imaginative or incorrect answer (i.e. a hallucination).\n",
    "\n",
    "This notebook covers the essentials of prompts for answering questions using a generative model. In addition, it showcases the `open domain` (knowledge available on the public internet) and `closed domain` (knowledge that is more private - typically enterprise or personal knowledge).\n",
    "\n",
    "Learn more about prompt design in the [official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/text/text-overview#prompt_structure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objective\n",
    "\n",
    "By the end of the notebook, you should be able to write prompts for the following:\n",
    "\n",
    "* **Open domain** questions:\n",
    "    * Zero-shot prompting\n",
    "    * Few-shot prompting\n",
    "\n",
    "\n",
    "* **Closed domain** questions:\n",
    "    * Providing custom knowledge as context\n",
    "    * Instruction-tune the outputs\n",
    "    * Few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDU0XJ1xRDlL"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a5AEr0lkLKD"
   },
   "source": [
    "### Install Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82ad0c445061"
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud-aiplatform --upgrade --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install additional Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q python-Levenshtein --upgrade --user\n",
    "!pip install -q fuzzywuzzy --upgrade --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell restarts the notebook kernel. For Vertex AI Workbench you can restart from the terminal using the kernel status button on top. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Hsqwn4hkLKE"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after package installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xe7OuYuGkLKF"
   },
   "source": [
    "### Authenticating your notebook environment\n",
    "* If you are using **Colab** to run this notebook, uncomment the cell below and continue.\n",
    "* If you are using **Vertex AI Workbench**, check out the setup instructions [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9Gx2SAZkLKF"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Colab only:** Uncomment the following cell to initialize the Vertex AI SDK. For Vertex AI Workbench, you don't need to run this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import vertexai\n",
    "\n",
    "# PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "# vertexai.init(project=PROJECT_ID, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vertexai.preview.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UP76a2la7O-a"
   },
   "source": [
    "### Import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7isig7e07O-a"
   },
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIPcn5dZ7O-b"
   },
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNNEz7vGFYUP"
   },
   "source": [
    "Question-answering capabilities require providing a prompt or a question that the model can use to generate a response. The prompt can be a few words or a few complete sentences, depending on the complexity of the question.\n",
    "\n",
    "When creating a question-answering prompt, it is essential to be specific and provide as much context as possible. It helps the model understand the intent behind the question and generate a relevant response. For example, if you want to ask:\n",
    "\n",
    "```\n",
    "\"What is the capital of France?\",\n",
    "\n",
    "then a good prompt could be:\n",
    "\n",
    "\"Please tell me the name of the city that serves as the capital of France.\"\n",
    "\n",
    "```\n",
    "\n",
    "In addition to being specific, the prompt should also be grammatically correct and free of spelling errors. It helps the model generate a response that is easy to understand and contains fewer errors or inaccuracies.\n",
    "\n",
    "By providing specific, context-rich prompts, you can help the model understand the intent behind the question and generate accurate and relevant responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5N9ZnlECm-z"
   },
   "source": [
    "Below are some differences between the **open domain** and **closed domain** categories for question-answering prompts.\n",
    "\n",
    "* **Open domain**: All questions whose answers are available online already. They can belong to any category, like history, geography, countries, politics, chemistry, etc. These include trivia or general knowledge questions, like:\n",
    "\n",
    "```\n",
    "Q: Who won the Olympic gold in swimming?\n",
    "Q: Who is the President of [given country]?\n",
    "Q: Who wrote [specific book]\"?\n",
    "```\n",
    "\n",
    "Keep in mind the training cutoff of generative models, as questions involving information more recent than what the model was trained on might give incorrect or imaginative answers.\n",
    "\n",
    "\n",
    "* **Closed domain**: If you have some internal knowledge base not available on the public internet, then those belong to the _closed domain_ category.\n",
    "You can pass that \"private\" knowledge as context to the model. If prompted correctly, the model is more likely to answer from within the context provided and less likely to give answers beyond that from the open internet.\n",
    "\n",
    "Consider the example of building a Q&A bot over your internal product documentation. In this case, you can pass the complete documentation to the model and prompt it only to answer based on that.\n",
    "\n",
    "Typical prompt for **closed domain**:\n",
    "\n",
    "```\n",
    "Prompt: f\"\"\" Answer from the below context: \\n\\n\n",
    "\t\t   context: {your knowledge base} \\n\n",
    "\t\t   question: {question specific to that knowledge base}  \\n\n",
    "\t\t   answer: {to be predicted by model} \\n\n",
    "\t\t\"\"\"\n",
    "```\n",
    "\n",
    "Below are some examples to understand these different types of prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBoN6zixDSiX"
   },
   "source": [
    "### Open Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJnv8XhnDXQm"
   },
   "source": [
    "#### Zero-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PaYoQuRwCm-z"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Create a catchy hashtag based on this house description: \\\"Coastal Elegance Awaits! Immerse yourself in beach living with this stunning 4-bedroom, 3-bathroom home. Recently renovated with over $200,000 in upgrades, this move-in ready haven offers the perfect blend of style and functionality.\n",
    "Step inside to discover a beautifully updated kitchen featuring quartz counters, stainless steel appliances, and a vent hood. The spacious dining area leads to a cozy formal living room with a wood-burning fireplace, creating a welcoming space for gatherings.\n",
    "All three bathrooms have been completely remodeled, showcasing new vanities, quartz counters, and elegant tubs/showers with glass enclosures.\n",
    "The master bedroom addition provides an expansive retreat with a walk-in closet, while a second master bedroom at the front offers versatility.\n",
    "This delightful home boasts bonus features like all-new LVT flooring, smooth ceilings, recessed lighting, dual-pane windows, and a Nest thermostat for modern comfort.\n",
    "Outside, the private rear yard is bordered by block wall fencing, providing a serene space to relax or entertain. A large side yard offers additional opportunities for outdoor enjoyment.\n",
    "Situated in a quiet neighborhood, the property is within walking distance to award-winning schools, a park, golf, shopping, and restaurants. Plus, you\\'re only a 5-minute drive from the beach and the vibrant Lovely Mall.\n",
    "Don\\'t miss out on this coastal gem with luxurious upgrades and a convenient location. Schedule a showing today and experience the beach lifestyle you\\'ve been dreaming of!\\\"\n",
    "\"\"\"\n",
    "\n",
    "print(\n",
    "    generation_model.predict(\n",
    "        prompt,\n",
    "        max_output_tokens=256,\n",
    "        temperature=0.1,\n",
    "    ).text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HShw52X2Dcmx"
   },
   "source": [
    "#### Few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tj_2hHAWE8vh"
   },
   "source": [
    "Let's say you want to a get a short answer from the model (like only a specific name). To do so, you can leverage a few-shot prompt and provide examples to the model to illustrate the expected behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RE5yCAaqDg7m"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Your real estate company aims to establish a strong social media presence and is eager to create a catchy hashtag based on long house descriptions in ads.\n",
    "\n",
    "\n",
    "input: Introducing 1234 Seaside Drive, a beautifully remodeled 2-story home with breathtaking ocean views from both levels. The upper level boasts an inviting open floor plan with vaulted beamed ceilings and a charming fireplace. The kitchen features a stunning oversized quartz island and top-of-the-line appliances. The luxurious primary suite offers ocean vistas, vaulted ceilings, and an en-suite bath with dual sinks and a glass-tiled walk-in shower. Downstairs, two generously sized bedrooms with ocean views and a stylish bathroom await. Step outside to the expansive patio, perfect for entertaining amidst a tasteful succulent garden. Situated in the sought-after Coastal Heights neighborhood, just minutes from shops, dining, beaches, and natural parks. Your dream coastal retreat awaits!\n",
    "\n",
    "\n",
    "output: #CoastalVistaRetreat\n",
    "\n",
    "\n",
    "input: Experience beach resort living in this carefree condo nestled on a hill with panoramic views of the Horizon Bay Resort. Immerse yourself in the serene sounds of waves at Sandy Cove and Sunset Beach, just a stone\\'s throw away across Ocean Breeze Highway.\n",
    "This two-bedroom upper level condo offers an open floor plan, custom lighting, and a breathtaking ocean view from your private deck. Indulge in the remodeled kitchen with sleek stainless steel appliances and stylish granite countertops. The bathroom boasts impeccable quality, adding a touch of luxury to your coastal retreat.\n",
    "Both bedrooms feature ceiling fans and ample natural light, offering tranquil views of the coastal hills. Storage is a breeze with the shaded, over-sized carport area and large attic space with a pull-ladder.\n",
    "Relax at the resort-quality association pool, complete with lounge chairs, umbrellas, and two restrooms with showers, all while enjoying sweeping views over Bluewater Creek and the glistening Pacific.\n",
    "Enjoy an active lifestyle with nearby yoga in the park, golf, hiking trails, shops, and dining. Multiple luxury resorts are mere minutes away.\n",
    "Hop aboard the Seaside Express and Coastal Breeze trolleys, conveniently stopping at the base of the hill, offering easy access to summer concerts, festivals, and vibrant coastal attractions.\n",
    "Don\\'t miss this chance to make your seaside dreams come true! Contact us now for a private showing.\n",
    "output: #SeasideEscape\n",
    "\n",
    "\n",
    "input: Coastal Elegance Awaits! Immerse yourself in beach living with this stunning 4-bedroom, 3-bathroom home. Recently renovated with over $200,000 in upgrades, this move-in ready haven offers the perfect blend of style and functionality.\n",
    "Step inside to discover a beautifully updated kitchen featuring quartz counters, stainless steel appliances, and a vent hood. The spacious dining area leads to a cozy formal living room with a wood-burning fireplace, creating a welcoming space for gatherings.\n",
    "All three bathrooms have been completely remodeled, showcasing new vanities, quartz counters, and elegant tubs/showers with glass enclosures.\n",
    "The master bedroom addition provides an expansive retreat with a walk-in closet, while a second master bedroom at the front offers versatility.\n",
    "This delightful home boasts bonus features like all-new LVT flooring, smooth ceilings, recessed lighting, dual-pane windows, and a Nest thermostat for modern comfort.\n",
    "Outside, the private rear yard is bordered by block wall fencing, providing a serene space to relax or entertain. A large side yard offers additional opportunities for outdoor enjoyment.\n",
    "Situated in a quiet neighborhood, the property is within walking distance to award-winning schools, a park, golf, shopping, and restaurants. Plus, you\\'re only a 5-minute drive from the beach and the vibrant Lovely Mall.\n",
    "Don\\'t miss out on this coastal gem with luxurious upgrades and a convenient location. Schedule a showing today and experience the beach lifestyle you\\'ve been dreaming of!\n",
    "output:\n",
    "\"\"\"\n",
    "\n",
    "print(\n",
    "    generation_model.predict(\n",
    "        prompt,\n",
    "        max_output_tokens=20,\n",
    "        temperature=0.1,\n",
    "    ).text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGvs0jFsUlvM"
   },
   "source": [
    "#### Zero-shot prompting vs Few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yjsAMuMUfZC"
   },
   "source": [
    "Zero-shot prompting can be useful for quickly generating text for new tasks, but the quality of the generated text may be lower than that of a few-shot prompt with well-chosen examples. Few-shot prompting is typically better suited for tasks that require a high degree of specificity or domain-specific knowledge, but requires some additional thought and potentially data to set up the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6UiJTxXEs4t"
   },
   "source": [
    "### Closed Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03ZITm4AGBvP"
   },
   "source": [
    "#### Adding internal knowledge as context in prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkhqjmB6VqPx"
   },
   "source": [
    "Imagine a scenario where you would like to build a question-answering bot that takes in internal documentation and lets users ask questions about it.\n",
    "\n",
    "In the example below, the Google Cloud Storage and content policy documentation is added to the prompt, so that the PaLM API can use that to answer subsequent questions with the provided context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1H2er_lExpW"
   },
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Creating catchy hashtags for a real estate company involves understanding your target audience, being creative, and aligning with your brand identity. Here are some tips to help you create compelling and effective hashtags:\n",
    "Keep them short and sweet. Hashtags should be easy to remember and type, so keep them to 1-2 words or a short phrase.\n",
    "Use relevant keywords. When people are searching for real estate, they'll likely use relevant keywords in their search terms. Make sure to include these keywords in your hashtags so that your listings will show up in their search results.\n",
    "Be creative. Don't be afraid to get creative with your hashtags. Use puns, wordplay, or other creative techniques to make your hashtags stand out.\n",
    "Use trending hashtags. If there are any trending hashtags related to real estate, use them in your posts. This will help your listings get seen by more people.\n",
    "Use branded hashtags. Create your own branded hashtags that you can use across all of your social media channels. This will help people to identify your brand and remember your listings.\n",
    "\"\"\"\n",
    "\n",
    "question = \"What is the best way to create a catchy hashtag based on a real estate description ?\"\n",
    "\n",
    "prompt = f\"\"\"Answer the question given in the context below:\n",
    "Context: {context}?\n",
    "Question: {question} \n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "print(\"[Prompt]\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"[Response]\")\n",
    "print(\n",
    "    generation_model.predict(\n",
    "        prompt,\n",
    "    ).text\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tagWC4VcQIw6"
   },
   "source": [
    "#### Instruction-tuning outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9UkogWHXM6N"
   },
   "source": [
    "Another way to help out language models is to provide additional instructions to frame the output in the prompt. To ensure the model doesn't respond to anything outside the context, the prompt can specify that the response should be \"Information not available in provided context\" if that's the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouq8FfwSQIBT"
   },
   "outputs": [],
   "source": [
    "question = \"Tell me the current market price of a specific property in a particular city?\"\n",
    "prompt = f\"\"\"Answer the question given the context below as {{Context:}}. \\n\n",
    "If the answer is not available in the {{Context:}} and you are not confident about the output,\n",
    "please say \"Information not available in provided context\". \\n\\n\n",
    "Context: {context}?\\n\n",
    "Question: {question} \\n\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "print(\"[Prompt]\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"[Response]\")\n",
    "print(\n",
    "    generation_model.predict(\n",
    "        prompt,\n",
    "        max_output_tokens=256,\n",
    "        temperature=0.3,\n",
    "    ).text\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZJfZShPRGqU"
   },
   "source": [
    "#### Few-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdSEQeQIS6pt"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Context:\n",
    "As the housing market experiences fluctuations in demand, supply, and economic conditions, the concept of \"rent-to-own\" arrangements has gained popularity among both tenants and homeowners. Rent-to-own, also known as lease-to-own or rent-to-buy, is a housing arrangement that offers potential buyers an alternative path to homeownership, particularly for those who may face challenges in qualifying for traditional mortgages or are not ready for an immediate purchase.\n",
    "\n",
    "Question:\n",
    "What is a rent-to-own housing arrangement, and how does it work for both tenants and homeowners?\n",
    "\n",
    "Answer:\n",
    "A rent-to-own housing arrangement is a contract between a tenant and a homeowner where the tenant has the option to buy the property at a predetermined price after a specified rental period. It offers potential buyers a pathway to homeownership while providing homeowners with a secure rental income and a potential future sale.\n",
    "---\n",
    "\n",
    "Context:\n",
    "The concept of \"housing bubbles\" was first widely discussed in the early 2000s after a series of real estate market crashes in various countries. Since then, the phenomenon of housing bubbles has become a significant topic of interest in the housing market, with potential implications for both buyers and sellers.\n",
    "\n",
    "Question:\n",
    "When were housing bubbles first widely discussed?\n",
    "\n",
    "Answer:\n",
    "Housing bubbles were first widely discussed in the early 2000s after a series of real estate market crashes in various countries.\n",
    "\n",
    "---\n",
    "\n",
    "Context:\n",
    "With the rapid growth of social media platforms and their influence on the real estate industry, the use of housing hashtags has become a crucial marketing strategy for real estate agents and property sellers. Crafting the right hashtags can significantly impact the visibility and reach of property listings, helping to attract potential buyers and generate interest in the housing market.\n",
    "\n",
    "Question: Why are housing hashtags so crucial ?\n",
    "\n",
    "Answer:\n",
    "\n",
    "\"\"\"\n",
    "print(\n",
    "    generation_model.predict(\n",
    "        prompt,\n",
    "    ).text\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94d80fb55f48"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b620d23a7634"
   },
   "source": [
    "You can evaluate the outputs of the question and answering task if the ground truth answers of each question are available. In zero-shot prompting, you can only use `open domain` questions. However, with `closed domain` questions, you can add context and evaluate similarly.  To showcase how that will work, start by creating a simple dataframe with questions and ground truth answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8e813a463531"
   },
   "outputs": [],
   "source": [
    "qa_data = {\n",
    "    \"question\": [\n",
    "        \"What is a mortgage?\",\n",
    "        \"What is a condo?\",\n",
    "        \"What is a duplex?\",\n",
    "    ],\n",
    "    \"answer_groundtruth\": [\"a loan secured by a mortgage\", \"a type of housing\", \"a building with two separate units\"],\n",
    "}\n",
    "qa_data_df = pd.DataFrame(qa_data)\n",
    "qa_data_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "951a147dc79d"
   },
   "source": [
    "Now that you have the data with questions and ground truth answers, you can call the PaLM 2 generation model to each review row using the `apply` function. Each row will use the dynamic prompt to predict the answer using the PaLM API. We will save the results in `answer_prediction` column.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffc47e0cb5b9"
   },
   "outputs": [],
   "source": [
    "def get_answer(row):\n",
    "    prompt = f\"\"\"Answer the following question as precise as possible.\\n\\n\n",
    "            question: {row}\n",
    "            answer:\n",
    "              \"\"\"\n",
    "    return generation_model.predict(\n",
    "        prompt=prompt,\n",
    "    ).text\n",
    "\n",
    "\n",
    "qa_data_df[\"answer_prediction\"] = qa_data_df[\"question\"].apply(get_answer)\n",
    "qa_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fe997dbf788"
   },
   "source": [
    "You may want to evaluate the answers predicted by the PaLM API. However, it will be more complex than the text classification since the answers may differ from ground truth and may be presented in slightly more/fewer words. \n",
    "\n",
    "For example, you can observe the question \"What is the name of the Earth's largest ocean?\" and see that model predicted  \"Pacific Ocean\" when a ground truth label is \"The Pacific Ocean\" with the extra \"The.\" Now, if you use the simple classification metrics, then you will consider this as a wrong prediction since original and predicted strings have a difference. However, you can see that the answer is correct since an extra \"The\" is causing the issue. It's a simple string comparison problem.\n",
    "\n",
    "The solution to string comparison where both `ground_thruth` and `predicted` may have some extra or fewer letters, one approach is to use a fuzzy matching algorithm. \n",
    "Fuzzy string matching uses [Levenshtein Distance](https://en.wikipedia.org/wiki/Levenshtein_distance) to calculate the differences between two strings. \n",
    "\n",
    "For example, the Levenshtein distance between \"kitten\" and \"sitting\" is 3, since the following 3 edits change one into the other, and there is no way to do it with fewer than 3 edits:\n",
    "\n",
    "* kitten → sitten (substitution of \"s\" for \"k\"),\n",
    "* sitten → sittin (substitution of \"i\" for \"e\"),\n",
    "* sittin → sitting (insertion of \"g\" at the end).\n",
    "\n",
    "\n",
    "Here's another example, but this time using `fuzzywuzzy`  library, which gives us the same `Levenshtein distance` between two strings but in ratio. The ratio raw score measures the string's similarity as an int in the range [0, 100]. For two strings X and Y, the score is defined by int(round((2.0 * M / T) * 100)) where T is the total number of characters in both strings, and M is the number of matches in the two strings. \n",
    "\n",
    "Read more here about the [ratio formula](https://anhaidgroup.github.io/py_stringmatching/v0.3.x/Ratio.html) : \n",
    "\n",
    "You can see one example to understand this furhter. \n",
    "```\n",
    "String1: \"this is a test\"\n",
    "String2: \"this is a test!\"\n",
    "\n",
    "Fuzz Ratio => 97  #\n",
    "\n",
    "Fuzz Partial Ratio => 100  #Since most characters are the same and in a similar sequence, the algorithm calculates the partial ratio as 100 and ignores simple additions (new characters). \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f048152519f"
   },
   "source": [
    "Now compute a score to perform fuzzy matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "040c1f9a175b"
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "def get_fuzzy_match(df):\n",
    "    return fuzz.partial_ratio(df[\"answer_groundtruth\"], df[\"answer_prediction\"])\n",
    "\n",
    "\n",
    "qa_data_df[\"match_score\"] = qa_data_df.apply(get_fuzzy_match, axis=1)\n",
    "qa_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11e266c49860"
   },
   "source": [
    "Now that you have the individual match score (partial), you can take the mean or average of the whole column to get a sense of overall data. \n",
    "Scores closer to 100 mean PaLM 2 can predict closer to ground truth; if the score is towards 50 or 0, it did not perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dae6a92a7650"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"the average match score of all predicted answer from PaLM 2 is : \",\n",
    "    qa_data_df[\"match_score\"].mean(),\n",
    "    \" %\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9e78972cad1"
   },
   "source": [
    "In this case, you get 100% as the mean score, even though some predictions were missing some words. That means you are very close to the ground truth, and some answers are just missing the exact verboseness of the ground truth. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "question_answering.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
